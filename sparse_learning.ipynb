{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = torch.from_numpy(np.load('./data/elementary_reaction_rates.npy')[::5, :])\n",
    "x_dot = torch.from_numpy(np.load('./data/net_production_rates.npy')[::5, :])\n",
    "mat = torch.from_numpy(np.load('./data/stoichiometric_matrix.npy')).to('cuda')\n",
    "\n",
    "dataset = TensorDataset(rate, x_dot)\n",
    "data_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=65536,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_record(log, epoch):\n",
    "    plt.figure(figsize=(24, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(np.arange(epoch), log[:epoch, 0])\n",
    "    plt.title('Regression Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Regression Loss')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(np.arange(epoch), log[:epoch, 1])\n",
    "    plt.title('Sparse Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Sparse Loss')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(np.arange(epoch), log[:epoch, 2])\n",
    "    plt.title('Total Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Total Loss')\n",
    "\n",
    "    plt.savefig('./figs/loss_record_9.pdf')\n",
    "    plt.savefig('./figs/loss_record_9.svg')\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight(weight):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.bar(np.arange(weight.shape[0]), weight)\n",
    "    plt.xlabel('Reaction Index')\n",
    "    plt.ylabel('Sparse Weight')\n",
    "    plt.ylim([0., 1.])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./figs/weight_9.pdf')\n",
    "    plt.savefig('./figs/weight_9.svg')\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sparse_model(torch.nn.Module):\n",
    "    def __init__(self, mat):\n",
    "        super().__init__()\n",
    "        self.mat = torch.nn.Parameter(mat)\n",
    "        self.sparse_weight = torch.Tensor(mat.shape[0]).uniform_(-1, 1)\n",
    "        self.sparse_weight = torch.nn.Parameter(self.sparse_weight)\n",
    "\n",
    "    def forward(self, rate):\n",
    "        x = rate * F.sigmoid(self.sparse_weight) @ self.mat \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :   0, batch :  1, loss :   53.36107, regression loss :   77.67509, sparse loss :   50.65952, time :    3.43388.\n",
      "Epoch :   1, batch :  1, loss :   53.24986, regression loss :   77.54098, sparse loss :   50.55084, time :    7.00328.\n",
      "Epoch :   2, batch :  1, loss :   53.13866, regression loss :   77.40723, sparse loss :   50.44216, time :   10.42152.\n",
      "Epoch :   3, batch :  1, loss :   53.02751, regression loss :   77.27373, sparse loss :   50.33349, time :   13.73198.\n",
      "Epoch :   4, batch :  1, loss :   52.91641, regression loss :   77.14036, sparse loss :   50.22486, time :   16.93625.\n",
      "Epoch :   5, batch :  1, loss :   52.80532, regression loss :   77.00706, sparse loss :   50.11625, time :   20.14756.\n",
      "Epoch :   6, batch :  1, loss :   52.69430, regression loss :   76.87392, sparse loss :   50.00767, time :   23.41027.\n",
      "Epoch :   7, batch :  1, loss :   52.58330, regression loss :   76.74098, sparse loss :   49.89912, time :   26.73761.\n",
      "Epoch :   8, batch :  1, loss :   52.47234, regression loss :   76.60829, sparse loss :   49.79057, time :   29.90955.\n",
      "Epoch :   9, batch :  1, loss :   52.36144, regression loss :   76.47586, sparse loss :   49.68206, time :   33.18991.\n",
      "Epoch :  10, batch :  1, loss :   52.25058, regression loss :   76.34371, sparse loss :   49.57357, time :   36.45305.\n",
      "Epoch :  11, batch :  1, loss :   52.13976, regression loss :   76.21182, sparse loss :   49.46509, time :   39.70052.\n",
      "Epoch :  12, batch :  1, loss :   52.02900, regression loss :   76.08017, sparse loss :   49.35664, time :   43.02066.\n",
      "Epoch :  13, batch :  1, loss :   51.91828, regression loss :   75.94877, sparse loss :   49.24823, time :   46.22619.\n",
      "Epoch :  14, batch :  1, loss :   51.80762, regression loss :   75.81757, sparse loss :   49.13985, time :   49.46325.\n",
      "Epoch :  15, batch :  1, loss :   51.69700, regression loss :   75.68658, sparse loss :   49.03149, time :   52.83863.\n",
      "Epoch :  16, batch :  1, loss :   51.58645, regression loss :   75.55578, sparse loss :   48.92319, time :   56.10195.\n",
      "Epoch :  17, batch :  1, loss :   51.47595, regression loss :   75.42517, sparse loss :   48.81492, time :   59.39445.\n",
      "Epoch :  18, batch :  1, loss :   51.36550, regression loss :   75.29474, sparse loss :   48.70670, time :   62.65765.\n",
      "Epoch :  19, batch :  1, loss :   51.25512, regression loss :   75.16450, sparse loss :   48.59852, time :   65.86482.\n",
      "Epoch :  20, batch :  1, loss :   51.14479, regression loss :   75.03445, sparse loss :   48.49039, time :   69.10744.\n",
      "Epoch :  21, batch :  1, loss :   51.03453, regression loss :   74.90461, sparse loss :   48.38230, time :   72.41385.\n",
      "Epoch :  22, batch :  1, loss :   50.92433, regression loss :   74.77500, sparse loss :   48.27426, time :   75.67064.\n",
      "Epoch :  23, batch :  1, loss :   50.81419, regression loss :   74.64561, sparse loss :   48.16626, time :   78.94227.\n",
      "Epoch :  24, batch :  1, loss :   50.70412, regression loss :   74.51647, sparse loss :   48.05830, time :   82.21751.\n",
      "Epoch :  25, batch :  1, loss :   50.59412, regression loss :   74.38759, sparse loss :   47.95040, time :   85.53449.\n",
      "Epoch :  26, batch :  1, loss :   50.48418, regression loss :   74.25897, sparse loss :   47.84254, time :   88.82992.\n",
      "Epoch :  27, batch :  1, loss :   50.37431, regression loss :   74.13063, sparse loss :   47.73472, time :   92.12933.\n",
      "Epoch :  28, batch :  1, loss :   50.26452, regression loss :   74.00258, sparse loss :   47.62695, time :   95.34825.\n",
      "Epoch :  29, batch :  1, loss :   50.15479, regression loss :   73.87482, sparse loss :   47.51923, time :   98.57970.\n",
      "Epoch :  30, batch :  1, loss :   50.04513, regression loss :   73.74736, sparse loss :   47.41156, time :  101.76296.\n",
      "Epoch :  31, batch :  1, loss :   49.93556, regression loss :   73.62022, sparse loss :   47.30393, time :  105.06009.\n",
      "Epoch :  32, batch :  1, loss :   49.82606, regression loss :   73.49339, sparse loss :   47.19635, time :  108.23698.\n",
      "Epoch :  33, batch :  1, loss :   49.71663, regression loss :   73.36689, sparse loss :   47.08882, time :  111.39678.\n",
      "Epoch :  34, batch :  1, loss :   49.60727, regression loss :   73.24072, sparse loss :   46.98134, time :  114.71216.\n",
      "Epoch :  35, batch :  1, loss :   49.49800, regression loss :   73.11489, sparse loss :   46.87390, time :  117.90519.\n",
      "Epoch :  36, batch :  1, loss :   49.38881, regression loss :   72.98940, sparse loss :   46.76652, time :  121.19634.\n",
      "Epoch :  37, batch :  1, loss :   49.27969, regression loss :   72.86426, sparse loss :   46.65918, time :  124.55979.\n",
      "Epoch :  38, batch :  1, loss :   49.17065, regression loss :   72.73948, sparse loss :   46.55190, time :  127.82502.\n",
      "Epoch :  39, batch :  1, loss :   49.06170, regression loss :   72.61505, sparse loss :   46.44466, time :  131.09778.\n",
      "Epoch :  40, batch :  1, loss :   48.95283, regression loss :   72.49096, sparse loss :   46.33748, time :  134.29039.\n",
      "Epoch :  41, batch :  1, loss :   48.84404, regression loss :   72.36723, sparse loss :   46.23035, time :  137.76005.\n",
      "Epoch :  42, batch :  1, loss :   48.73534, regression loss :   72.24385, sparse loss :   46.12328, time :  141.07598.\n",
      "Epoch :  43, batch :  1, loss :   48.62672, regression loss :   72.12081, sparse loss :   46.01627, time :  144.50276.\n",
      "Epoch :  44, batch :  1, loss :   48.51819, regression loss :   71.99810, sparse loss :   45.90931, time :  147.82112.\n",
      "Epoch :  45, batch :  1, loss :   48.40974, regression loss :   71.87572, sparse loss :   45.80241, time :  151.00777.\n",
      "Epoch :  46, batch :  1, loss :   48.30139, regression loss :   71.75366, sparse loss :   45.69558, time :  154.40233.\n",
      "Epoch :  47, batch :  1, loss :   48.19313, regression loss :   71.63191, sparse loss :   45.58882, time :  157.67843.\n",
      "Epoch :  48, batch :  1, loss :   48.08495, regression loss :   71.51048, sparse loss :   45.48212, time :  160.98184.\n",
      "Epoch :  49, batch :  1, loss :   47.97687, regression loss :   71.38935, sparse loss :   45.37549, time :  164.25328.\n",
      "Epoch :  50, batch :  1, loss :   47.86888, regression loss :   71.26851, sparse loss :   45.26892, time :  167.53450.\n",
      "Epoch :  51, batch :  1, loss :   47.76099, regression loss :   71.14797, sparse loss :   45.16244, time :  170.74437.\n",
      "Epoch :  52, batch :  1, loss :   47.65320, regression loss :   71.02772, sparse loss :   45.05603, time :  173.95112.\n",
      "Epoch :  53, batch :  1, loss :   47.54550, regression loss :   70.90776, sparse loss :   44.94970, time :  177.18260.\n",
      "Epoch :  54, batch :  1, loss :   47.43790, regression loss :   70.78808, sparse loss :   44.84344, time :  180.42982.\n",
      "Epoch :  55, batch :  1, loss :   47.33040, regression loss :   70.66868, sparse loss :   44.73726, time :  183.78592.\n",
      "Epoch :  56, batch :  1, loss :   47.22301, regression loss :   70.54956, sparse loss :   44.63118, time :  187.05317.\n",
      "Epoch :  57, batch :  1, loss :   47.11572, regression loss :   70.43072, sparse loss :   44.52517, time :  190.38703.\n",
      "Epoch :  58, batch :  1, loss :   47.00853, regression loss :   70.31215, sparse loss :   44.41924, time :  193.73546.\n",
      "Epoch :  59, batch :  1, loss :   46.90144, regression loss :   70.19386, sparse loss :   44.31340, time :  197.00484.\n",
      "Epoch :  60, batch :  1, loss :   46.79447, regression loss :   70.07585, sparse loss :   44.20765, time :  200.28910.\n",
      "Epoch :  61, batch :  1, loss :   46.68760, regression loss :   69.95812, sparse loss :   44.10199, time :  203.51106.\n",
      "Epoch :  62, batch :  1, loss :   46.58084, regression loss :   69.84066, sparse loss :   43.99642, time :  206.68938.\n",
      "Epoch :  63, batch :  1, loss :   46.47420, regression loss :   69.72348, sparse loss :   43.89095, time :  209.99259.\n",
      "Epoch :  64, batch :  1, loss :   46.36766, regression loss :   69.60658, sparse loss :   43.78556, time :  213.33889.\n",
      "Epoch :  65, batch :  1, loss :   46.26124, regression loss :   69.48997, sparse loss :   43.68027, time :  216.68106.\n",
      "Epoch :  66, batch :  1, loss :   46.15494, regression loss :   69.37364, sparse loss :   43.57509, time :  219.91349.\n",
      "Epoch :  67, batch :  1, loss :   46.04875, regression loss :   69.25759, sparse loss :   43.46999, time :  223.20044.\n",
      "Epoch :  68, batch :  1, loss :   45.94268, regression loss :   69.14183, sparse loss :   43.36500, time :  226.46582.\n",
      "Epoch :  69, batch :  1, loss :   45.83673, regression loss :   69.02636, sparse loss :   43.26011, time :  229.68931.\n",
      "Epoch :  70, batch :  1, loss :   45.73089, regression loss :   68.91119, sparse loss :   43.15531, time :  233.06993.\n",
      "Epoch :  71, batch :  1, loss :   45.62519, regression loss :   68.79631, sparse loss :   43.05062, time :  236.34938.\n",
      "Epoch :  72, batch :  1, loss :   45.51959, regression loss :   68.68174, sparse loss :   42.94603, time :  239.70770.\n",
      "Epoch :  73, batch :  1, loss :   45.41413, regression loss :   68.56746, sparse loss :   42.84154, time :  242.98612.\n",
      "Epoch :  74, batch :  1, loss :   45.30879, regression loss :   68.45349, sparse loss :   42.73716, time :  246.14619.\n",
      "Epoch :  75, batch :  1, loss :   45.20358, regression loss :   68.33983, sparse loss :   42.63289, time :  249.55504.\n",
      "Epoch :  76, batch :  1, loss :   45.09850, regression loss :   68.22648, sparse loss :   42.52872, time :  253.30135.\n",
      "Epoch :  77, batch :  1, loss :   44.99354, regression loss :   68.11345, sparse loss :   42.42466, time :  256.88305.\n",
      "Epoch :  78, batch :  1, loss :   44.88872, regression loss :   68.00072, sparse loss :   42.32072, time :  260.28663.\n",
      "Epoch :  79, batch :  1, loss :   44.78402, regression loss :   67.88832, sparse loss :   42.21687, time :  263.70314.\n",
      "Epoch :  80, batch :  1, loss :   44.67946, regression loss :   67.77624, sparse loss :   42.11315, time :  267.17076.\n",
      "Epoch :  81, batch :  1, loss :   44.57502, regression loss :   67.66448, sparse loss :   42.00953, time :  270.52212.\n",
      "Epoch :  82, batch :  1, loss :   44.47072, regression loss :   67.55304, sparse loss :   41.90602, time :  273.77058.\n",
      "Epoch :  83, batch :  1, loss :   44.36656, regression loss :   67.44193, sparse loss :   41.80263, time :  277.12242.\n",
      "Epoch :  84, batch :  1, loss :   44.26253, regression loss :   67.33114, sparse loss :   41.69935, time :  280.34139.\n",
      "Epoch :  85, batch :  1, loss :   44.15865, regression loss :   67.22068, sparse loss :   41.59620, time :  283.81701.\n",
      "Epoch :  86, batch :  1, loss :   44.05490, regression loss :   67.11055, sparse loss :   41.49316, time :  287.19984.\n",
      "Epoch :  87, batch :  1, loss :   43.95129, regression loss :   67.00074, sparse loss :   41.39024, time :  290.74691.\n",
      "Epoch :  88, batch :  1, loss :   43.84781, regression loss :   66.89126, sparse loss :   41.28743, time :  294.17299.\n",
      "Epoch :  89, batch :  1, loss :   43.74448, regression loss :   66.78211, sparse loss :   41.18475, time :  297.48881.\n",
      "Epoch :  90, batch :  1, loss :   43.64129, regression loss :   66.67328, sparse loss :   41.08218, time :  300.77626.\n",
      "Epoch :  91, batch :  1, loss :   43.53825, regression loss :   66.56478, sparse loss :   40.97975, time :  304.06702.\n",
      "Epoch :  92, batch :  1, loss :   43.43535, regression loss :   66.45660, sparse loss :   40.87744, time :  307.31297.\n",
      "Epoch :  93, batch :  1, loss :   43.33260, regression loss :   66.34875, sparse loss :   40.77525, time :  310.60662.\n",
      "Epoch :  94, batch :  1, loss :   43.22999, regression loss :   66.24122, sparse loss :   40.67319, time :  313.93187.\n",
      "Epoch :  95, batch :  1, loss :   43.12754, regression loss :   66.13402, sparse loss :   40.57126, time :  317.52678.\n",
      "Epoch :  96, batch :  1, loss :   43.02522, regression loss :   66.02714, sparse loss :   40.46946, time :  320.73704.\n",
      "Epoch :  97, batch :  1, loss :   42.92306, regression loss :   65.92058, sparse loss :   40.36777, time :  324.12252.\n",
      "Epoch :  98, batch :  1, loss :   42.82105, regression loss :   65.81433, sparse loss :   40.26624, time :  327.40347.\n",
      "Epoch :  99, batch :  1, loss :   42.71918, regression loss :   65.70841, sparse loss :   40.16482, time :  330.82985.\n",
      "Epoch : 100, batch :  1, loss :   42.61747, regression loss :   65.60281, sparse loss :   40.06355, time :  336.86738.\n",
      "Epoch : 101, batch :  1, loss :   42.51592, regression loss :   65.49752, sparse loss :   39.96241, time :  338.94607.\n",
      "Epoch : 102, batch :  1, loss :   42.41452, regression loss :   65.39255, sparse loss :   39.86140, time :  340.92979.\n",
      "Epoch : 103, batch :  1, loss :   42.31326, regression loss :   65.28790, sparse loss :   39.76053, time :  342.85537.\n",
      "Epoch : 104, batch :  1, loss :   42.21218, regression loss :   65.18356, sparse loss :   39.65980, time :  344.81321.\n",
      "Epoch : 105, batch :  1, loss :   42.11124, regression loss :   65.07954, sparse loss :   39.55921, time :  346.88169.\n",
      "Epoch : 106, batch :  1, loss :   42.01047, regression loss :   64.97583, sparse loss :   39.45876, time :  348.85826.\n",
      "Epoch : 107, batch :  1, loss :   41.90984, regression loss :   64.87243, sparse loss :   39.35844, time :  350.83791.\n",
      "Epoch : 108, batch :  1, loss :   41.80938, regression loss :   64.76935, sparse loss :   39.25827, time :  353.04094.\n",
      "Epoch : 109, batch :  1, loss :   41.70908, regression loss :   64.66658, sparse loss :   39.15825, time :  355.12268.\n",
      "Epoch : 110, batch :  1, loss :   41.60894, regression loss :   64.56413, sparse loss :   39.05836, time :  357.16685.\n",
      "Epoch : 111, batch :  1, loss :   41.50897, regression loss :   64.46198, sparse loss :   38.95863, time :  359.14387.\n",
      "Epoch : 112, batch :  1, loss :   41.40915, regression loss :   64.36014, sparse loss :   38.85904, time :  361.22286.\n",
      "Epoch : 113, batch :  1, loss :   41.30949, regression loss :   64.25862, sparse loss :   38.75959, time :  363.42384.\n",
      "Epoch : 114, batch :  1, loss :   41.21001, regression loss :   64.15741, sparse loss :   38.66030, time :  365.45753.\n",
      "Epoch : 115, batch :  1, loss :   41.11068, regression loss :   64.05650, sparse loss :   38.56115, time :  367.41751.\n",
      "Epoch : 116, batch :  1, loss :   41.01153, regression loss :   63.95591, sparse loss :   38.46215, time :  369.39921.\n",
      "Epoch : 117, batch :  1, loss :   40.91254, regression loss :   63.85562, sparse loss :   38.36331, time :  371.40286.\n",
      "Epoch : 118, batch :  1, loss :   40.81372, regression loss :   63.75564, sparse loss :   38.26462, time :  373.36569.\n",
      "Epoch : 119, batch :  1, loss :   40.71507, regression loss :   63.65597, sparse loss :   38.16608, time :  375.38230.\n",
      "Epoch : 120, batch :  1, loss :   40.61658, regression loss :   63.55660, sparse loss :   38.06769, time :  377.37990.\n",
      "Epoch : 121, batch :  1, loss :   40.51827, regression loss :   63.45755, sparse loss :   37.96946, time :  379.36960.\n",
      "Epoch : 122, batch :  1, loss :   40.42012, regression loss :   63.35879, sparse loss :   37.87138, time :  381.55749.\n",
      "Epoch : 123, batch :  1, loss :   40.32214, regression loss :   63.26035, sparse loss :   37.77346, time :  383.67198.\n",
      "Epoch : 124, batch :  1, loss :   40.22435, regression loss :   63.16220, sparse loss :   37.67570, time :  385.72185.\n",
      "Epoch : 125, batch :  1, loss :   40.12672, regression loss :   63.06437, sparse loss :   37.57809, time :  387.85226.\n",
      "Epoch : 126, batch :  1, loss :   40.02927, regression loss :   62.96683, sparse loss :   37.48065, time :  390.03257.\n",
      "Epoch : 127, batch :  1, loss :   39.93199, regression loss :   62.86960, sparse loss :   37.38337, time :  392.12769.\n",
      "Epoch : 128, batch :  1, loss :   39.83488, regression loss :   62.77268, sparse loss :   37.28624, time :  394.10823.\n",
      "Epoch : 129, batch :  1, loss :   39.73795, regression loss :   62.67606, sparse loss :   37.18927, time :  396.31889.\n",
      "Epoch : 130, batch :  1, loss :   39.64120, regression loss :   62.57974, sparse loss :   37.09248, time :  398.41678.\n",
      "Epoch : 131, batch :  1, loss :   39.54462, regression loss :   62.48373, sparse loss :   36.99583, time :  400.48907.\n",
      "Epoch : 132, batch :  1, loss :   39.44823, regression loss :   62.38802, sparse loss :   36.89936, time :  402.46708.\n",
      "Epoch : 133, batch :  1, loss :   39.35201, regression loss :   62.29261, sparse loss :   36.80305, time :  404.43693.\n",
      "Epoch : 134, batch :  1, loss :   39.25597, regression loss :   62.19751, sparse loss :   36.70691, time :  406.40374.\n",
      "Epoch : 135, batch :  1, loss :   39.16012, regression loss :   62.10271, sparse loss :   36.61094, time :  408.47252.\n",
      "Epoch : 136, batch :  1, loss :   39.06443, regression loss :   62.00822, sparse loss :   36.51513, time :  410.58557.\n",
      "Epoch : 137, batch :  1, loss :   38.96894, regression loss :   61.91403, sparse loss :   36.41949, time :  412.72720.\n",
      "Epoch : 138, batch :  1, loss :   38.87362, regression loss :   61.82014, sparse loss :   36.32401, time :  414.70108.\n",
      "Epoch : 139, batch :  1, loss :   38.77849, regression loss :   61.72656, sparse loss :   36.22871, time :  416.76782.\n",
      "Epoch : 140, batch :  1, loss :   38.68354, regression loss :   61.63329, sparse loss :   36.13357, time :  418.87560.\n",
      "Epoch : 141, batch :  1, loss :   38.58878, regression loss :   61.54032, sparse loss :   36.03861, time :  420.95719.\n",
      "Epoch : 142, batch :  1, loss :   38.49420, regression loss :   61.44765, sparse loss :   35.94382, time :  422.93985.\n",
      "Epoch : 143, batch :  1, loss :   38.39980, regression loss :   61.35529, sparse loss :   35.84919, time :  425.17515.\n",
      "Epoch : 144, batch :  1, loss :   38.30560, regression loss :   61.26324, sparse loss :   35.75475, time :  427.21903.\n",
      "Epoch : 145, batch :  1, loss :   38.21157, regression loss :   61.17149, sparse loss :   35.66047, time :  429.42498.\n",
      "Epoch : 146, batch :  1, loss :   38.11774, regression loss :   61.08005, sparse loss :   35.56637, time :  431.53820.\n",
      "Epoch : 147, batch :  1, loss :   38.02409, regression loss :   60.98891, sparse loss :   35.47244, time :  433.62906.\n",
      "Epoch : 148, batch :  1, loss :   37.93063, regression loss :   60.89807, sparse loss :   35.37869, time :  435.72951.\n",
      "Epoch : 149, batch :  1, loss :   37.83736, regression loss :   60.80754, sparse loss :   35.28511, time :  437.89288.\n",
      "Epoch : 150, batch :  1, loss :   37.74428, regression loss :   60.71732, sparse loss :   35.19172, time :  439.99770.\n",
      "Epoch : 151, batch :  1, loss :   37.65139, regression loss :   60.62740, sparse loss :   35.09850, time :  441.98576.\n",
      "Epoch : 152, batch :  1, loss :   37.55869, regression loss :   60.53778, sparse loss :   35.00546, time :  444.06386.\n",
      "Epoch : 153, batch :  1, loss :   37.46617, regression loss :   60.44847, sparse loss :   34.91259, time :  446.06874.\n",
      "Epoch : 154, batch :  1, loss :   37.37386, regression loss :   60.35946, sparse loss :   34.81990, time :  448.23890.\n",
      "Epoch : 155, batch :  1, loss :   37.28173, regression loss :   60.27076, sparse loss :   34.72739, time :  450.39617.\n",
      "Epoch : 156, batch :  1, loss :   37.18980, regression loss :   60.18235, sparse loss :   34.63507, time :  452.48065.\n",
      "Epoch : 157, batch :  1, loss :   37.09806, regression loss :   60.09425, sparse loss :   34.54292, time :  454.58765.\n",
      "Epoch : 158, batch :  1, loss :   37.00650, regression loss :   60.00646, sparse loss :   34.45095, time :  456.61386.\n",
      "Epoch : 159, batch :  1, loss :   36.91515, regression loss :   59.91896, sparse loss :   34.35918, time :  458.61161.\n",
      "Epoch : 160, batch :  1, loss :   36.82400, regression loss :   59.83176, sparse loss :   34.26758, time :  460.79930.\n",
      "Epoch : 161, batch :  1, loss :   36.73303, regression loss :   59.74487, sparse loss :   34.17616, time :  463.14731.\n",
      "Epoch : 162, batch :  1, loss :   36.64226, regression loss :   59.65828, sparse loss :   34.08493, time :  465.14736.\n",
      "Epoch : 163, batch :  1, loss :   36.55169, regression loss :   59.57198, sparse loss :   33.99388, time :  467.12578.\n",
      "Epoch : 164, batch :  1, loss :   36.46131, regression loss :   59.48599, sparse loss :   33.90302, time :  469.18248.\n",
      "Epoch : 165, batch :  1, loss :   36.37113, regression loss :   59.40030, sparse loss :   33.81234, time :  471.38932.\n",
      "Epoch : 166, batch :  1, loss :   36.28115, regression loss :   59.31490, sparse loss :   33.72184, time :  473.42324.\n",
      "Epoch : 167, batch :  1, loss :   36.19136, regression loss :   59.22980, sparse loss :   33.63154, time :  475.62068.\n",
      "Epoch : 168, batch :  1, loss :   36.10178, regression loss :   59.14500, sparse loss :   33.54142, time :  477.77462.\n",
      "Epoch : 169, batch :  1, loss :   36.01239, regression loss :   59.06050, sparse loss :   33.45149, time :  479.80855.\n",
      "Epoch : 170, batch :  1, loss :   35.92320, regression loss :   58.97629, sparse loss :   33.36174, time :  481.92478.\n",
      "Epoch : 171, batch :  1, loss :   35.83421, regression loss :   58.89238, sparse loss :   33.27219, time :  484.03946.\n",
      "Epoch : 172, batch :  1, loss :   35.74542, regression loss :   58.80877, sparse loss :   33.18282, time :  486.05795.\n",
      "Epoch : 173, batch :  1, loss :   35.65683, regression loss :   58.72545, sparse loss :   33.09365, time :  488.28690.\n",
      "Epoch : 174, batch :  1, loss :   35.56843, regression loss :   58.64242, sparse loss :   33.00466, time :  490.41798.\n",
      "Epoch : 175, batch :  1, loss :   35.48025, regression loss :   58.55969, sparse loss :   32.91587, time :  492.43731.\n",
      "Epoch : 176, batch :  1, loss :   35.39226, regression loss :   58.47725, sparse loss :   32.82726, time :  494.66515.\n",
      "Epoch : 177, batch :  1, loss :   35.30447, regression loss :   58.39510, sparse loss :   32.73885, time :  496.65276.\n",
      "Epoch : 178, batch :  1, loss :   35.21689, regression loss :   58.31324, sparse loss :   32.65062, time :  498.65068.\n",
      "Epoch : 179, batch :  1, loss :   35.12950, regression loss :   58.23167, sparse loss :   32.56260, time :  500.64515.\n",
      "Epoch : 180, batch :  1, loss :   35.04232, regression loss :   58.15039, sparse loss :   32.47475, time :  502.74999.\n",
      "Epoch : 181, batch :  1, loss :   34.95534, regression loss :   58.06941, sparse loss :   32.38711, time :  504.74979.\n",
      "Epoch : 182, batch :  1, loss :   34.86856, regression loss :   57.98870, sparse loss :   32.29966, time :  506.80209.\n",
      "Epoch : 183, batch :  1, loss :   34.78199, regression loss :   57.90829, sparse loss :   32.21240, time :  508.81944.\n",
      "Epoch : 184, batch :  1, loss :   34.69562, regression loss :   57.82816, sparse loss :   32.12534, time :  510.87516.\n",
      "Epoch : 185, batch :  1, loss :   34.60946, regression loss :   57.74832, sparse loss :   32.03847, time :  512.87794.\n",
      "Epoch : 186, batch :  1, loss :   34.52349, regression loss :   57.66877, sparse loss :   31.95180, time :  515.05596.\n",
      "Epoch : 187, batch :  1, loss :   34.43774, regression loss :   57.58950, sparse loss :   31.86532, time :  517.15090.\n",
      "Epoch : 188, batch :  1, loss :   34.35219, regression loss :   57.51051, sparse loss :   31.77904, time :  519.27039.\n",
      "Epoch : 189, batch :  1, loss :   34.26684, regression loss :   57.43180, sparse loss :   31.69295, time :  521.41432.\n",
      "Epoch : 190, batch :  1, loss :   34.18169, regression loss :   57.35338, sparse loss :   31.60706, time :  523.46928.\n",
      "Epoch : 191, batch :  1, loss :   34.09676, regression loss :   57.27524, sparse loss :   31.52137, time :  525.55382.\n",
      "Epoch : 192, batch :  1, loss :   34.01202, regression loss :   57.19738, sparse loss :   31.43587, time :  527.63874.\n",
      "Epoch : 193, batch :  1, loss :   33.92750, regression loss :   57.11980, sparse loss :   31.35058, time :  529.68562.\n",
      "Epoch : 194, batch :  1, loss :   33.84317, regression loss :   57.04249, sparse loss :   31.26547, time :  531.89496.\n",
      "Epoch : 195, batch :  1, loss :   33.75906, regression loss :   56.96546, sparse loss :   31.18057, time :  534.00396.\n",
      "Epoch : 196, batch :  1, loss :   33.67515, regression loss :   56.88872, sparse loss :   31.09587, time :  536.37782.\n",
      "Epoch : 197, batch :  1, loss :   33.59145, regression loss :   56.81224, sparse loss :   31.01136, time :  538.45961.\n",
      "Epoch : 198, batch :  1, loss :   33.50796, regression loss :   56.73605, sparse loss :   30.92706, time :  540.46441.\n",
      "Epoch : 199, batch :  1, loss :   33.42467, regression loss :   56.66012, sparse loss :   30.84295, time :  542.57271.\n",
      "Epoch : 200, batch :  1, loss :   33.34158, regression loss :   56.58447, sparse loss :   30.75904, time :  547.36234.\n",
      "Epoch : 201, batch :  1, loss :   33.25871, regression loss :   56.50909, sparse loss :   30.67534, time :  549.21188.\n",
      "Epoch : 202, batch :  1, loss :   33.17604, regression loss :   56.43398, sparse loss :   30.59183, time :  551.02231.\n",
      "Epoch : 203, batch :  1, loss :   33.09358, regression loss :   56.35915, sparse loss :   30.50852, time :  552.85036.\n",
      "Epoch : 204, batch :  1, loss :   33.01133, regression loss :   56.28458, sparse loss :   30.42542, time :  554.74578.\n",
      "Epoch : 205, batch :  1, loss :   32.92929, regression loss :   56.21028, sparse loss :   30.34251, time :  556.76218.\n",
      "Epoch : 206, batch :  1, loss :   32.84745, regression loss :   56.13625, sparse loss :   30.25981, time :  558.87571.\n",
      "Epoch : 207, batch :  1, loss :   32.76582, regression loss :   56.06249, sparse loss :   30.17730, time :  560.74115.\n",
      "Epoch : 208, batch :  1, loss :   32.68440, regression loss :   55.98899, sparse loss :   30.09501, time :  562.73465.\n",
      "Epoch : 209, batch :  1, loss :   32.60319, regression loss :   55.91575, sparse loss :   30.01291, time :  564.64621.\n",
      "Epoch : 210, batch :  1, loss :   32.52218, regression loss :   55.84278, sparse loss :   29.93101, time :  566.59041.\n",
      "Epoch : 211, batch :  1, loss :   32.44139, regression loss :   55.77007, sparse loss :   29.84931, time :  568.58392.\n",
      "Epoch : 212, batch :  1, loss :   32.36080, regression loss :   55.69763, sparse loss :   29.76782, time :  570.68848.\n",
      "Epoch : 213, batch :  1, loss :   32.28042, regression loss :   55.62545, sparse loss :   29.68653, time :  572.62829.\n",
      "Epoch : 214, batch :  1, loss :   32.20025, regression loss :   55.55352, sparse loss :   29.60544, time :  574.48222.\n",
      "Epoch : 215, batch :  1, loss :   32.12029, regression loss :   55.48186, sparse loss :   29.52456, time :  576.28254.\n",
      "Epoch : 216, batch :  1, loss :   32.04053, regression loss :   55.41045, sparse loss :   29.44388, time :  578.36932.\n",
      "Epoch : 217, batch :  1, loss :   31.96099, regression loss :   55.33930, sparse loss :   29.36340, time :  580.41880.\n",
      "Epoch : 218, batch :  1, loss :   31.88165, regression loss :   55.26841, sparse loss :   29.28312, time :  582.34541.\n",
      "Epoch : 219, batch :  1, loss :   31.80252, regression loss :   55.19777, sparse loss :   29.20305, time :  584.32014.\n",
      "Epoch : 220, batch :  1, loss :   31.72360, regression loss :   55.12739, sparse loss :   29.12318, time :  586.24476.\n",
      "Epoch : 221, batch :  1, loss :   31.64489, regression loss :   55.05726, sparse loss :   29.04352, time :  588.12648.\n",
      "Epoch : 222, batch :  1, loss :   31.56639, regression loss :   54.98739, sparse loss :   28.96406, time :  590.11321.\n",
      "Epoch : 223, batch :  1, loss :   31.48810, regression loss :   54.91776, sparse loss :   28.88480, time :  592.09559.\n",
      "Epoch : 224, batch :  1, loss :   31.41002, regression loss :   54.84839, sparse loss :   28.80575, time :  594.02014.\n",
      "Epoch : 225, batch :  1, loss :   31.33214, regression loss :   54.77926, sparse loss :   28.72690, time :  595.94419.\n",
      "Epoch : 226, batch :  1, loss :   31.25447, regression loss :   54.71039, sparse loss :   28.64826, time :  597.77612.\n",
      "Epoch : 227, batch :  1, loss :   31.17701, regression loss :   54.64176, sparse loss :   28.56982, time :  599.64376.\n",
      "Epoch : 228, batch :  1, loss :   31.09977, regression loss :   54.57339, sparse loss :   28.49159, time :  601.58804.\n",
      "Epoch : 229, batch :  1, loss :   31.02273, regression loss :   54.50525, sparse loss :   28.41356, time :  603.59675.\n",
      "Epoch : 230, batch :  1, loss :   30.94589, regression loss :   54.43737, sparse loss :   28.33573, time :  605.48016.\n",
      "Epoch : 231, batch :  1, loss :   30.86927, regression loss :   54.36972, sparse loss :   28.25811, time :  607.30995.\n",
      "Epoch : 232, batch :  1, loss :   30.79286, regression loss :   54.30233, sparse loss :   28.18069, time :  609.34466.\n",
      "Epoch : 233, batch :  1, loss :   30.71665, regression loss :   54.23517, sparse loss :   28.10349, time :  611.29693.\n",
      "Epoch : 234, batch :  1, loss :   30.64066, regression loss :   54.16826, sparse loss :   28.02648, time :  613.18489.\n",
      "Epoch : 235, batch :  1, loss :   30.56487, regression loss :   54.10159, sparse loss :   27.94968, time :  615.06129.\n",
      "Epoch : 236, batch :  1, loss :   30.48929, regression loss :   54.03516, sparse loss :   27.87308, time :  616.85848.\n",
      "Epoch : 237, batch :  1, loss :   30.41391, regression loss :   53.96897, sparse loss :   27.79669, time :  618.79793.\n",
      "Epoch : 238, batch :  1, loss :   30.33875, regression loss :   53.90301, sparse loss :   27.72050, time :  620.84573.\n",
      "Epoch : 239, batch :  1, loss :   30.26380, regression loss :   53.83730, sparse loss :   27.64452, time :  622.90294.\n",
      "Epoch : 240, batch :  1, loss :   30.18905, regression loss :   53.77183, sparse loss :   27.56874, time :  624.78375.\n",
      "Epoch : 241, batch :  1, loss :   30.11451, regression loss :   53.70659, sparse loss :   27.49316, time :  626.83671.\n",
      "Epoch : 242, batch :  1, loss :   30.04017, regression loss :   53.64158, sparse loss :   27.41780, time :  628.95379.\n",
      "Epoch : 243, batch :  1, loss :   29.96605, regression loss :   53.57682, sparse loss :   27.34263, time :  630.87866.\n",
      "Epoch : 244, batch :  1, loss :   29.89213, regression loss :   53.51228, sparse loss :   27.26767, time :  632.81802.\n",
      "Epoch : 245, batch :  1, loss :   29.81842, regression loss :   53.44799, sparse loss :   27.19292, time :  634.74938.\n",
      "Epoch : 246, batch :  1, loss :   29.74492, regression loss :   53.38392, sparse loss :   27.11837, time :  636.56376.\n",
      "Epoch : 247, batch :  1, loss :   29.67163, regression loss :   53.32009, sparse loss :   27.04402, time :  638.42383.\n",
      "Epoch : 248, batch :  1, loss :   29.59854, regression loss :   53.25648, sparse loss :   26.96988, time :  640.31061.\n",
      "Epoch : 249, batch :  1, loss :   29.52566, regression loss :   53.19311, sparse loss :   26.89595, time :  642.44062.\n",
      "Epoch : 250, batch :  1, loss :   29.45299, regression loss :   53.12997, sparse loss :   26.82221, time :  644.30483.\n",
      "Epoch : 251, batch :  1, loss :   29.38052, regression loss :   53.06707, sparse loss :   26.74868, time :  646.34009.\n",
      "Epoch : 252, batch :  1, loss :   29.30826, regression loss :   53.00438, sparse loss :   26.67536, time :  648.42614.\n",
      "Epoch : 253, batch :  1, loss :   29.23621, regression loss :   52.94193, sparse loss :   26.60224, time :  650.38227.\n",
      "Epoch : 254, batch :  1, loss :   29.16436, regression loss :   52.87971, sparse loss :   26.52932, time :  652.41434.\n",
      "Epoch : 255, batch :  1, loss :   29.09272, regression loss :   52.81771, sparse loss :   26.45661, time :  654.52877.\n",
      "Epoch : 256, batch :  1, loss :   29.02128, regression loss :   52.75594, sparse loss :   26.38410, time :  656.52598.\n",
      "Epoch : 257, batch :  1, loss :   28.95006, regression loss :   52.69440, sparse loss :   26.31180, time :  658.48774.\n",
      "Epoch : 258, batch :  1, loss :   28.87903, regression loss :   52.63308, sparse loss :   26.23969, time :  660.32614.\n",
      "Epoch : 259, batch :  1, loss :   28.80821, regression loss :   52.57199, sparse loss :   26.16780, time :  662.23114.\n",
      "Epoch : 260, batch :  1, loss :   28.73760, regression loss :   52.51112, sparse loss :   26.09610, time :  664.04462.\n",
      "Epoch : 261, batch :  1, loss :   28.66719, regression loss :   52.45048, sparse loss :   26.02461, time :  665.84391.\n",
      "Epoch : 262, batch :  1, loss :   28.59699, regression loss :   52.39005, sparse loss :   25.95332, time :  667.92432.\n",
      "Epoch : 263, batch :  1, loss :   28.52699, regression loss :   52.32986, sparse loss :   25.88223, time :  669.88538.\n",
      "Epoch : 264, batch :  1, loss :   28.45720, regression loss :   52.26988, sparse loss :   25.81134, time :  671.72600.\n",
      "Epoch : 265, batch :  1, loss :   28.38761, regression loss :   52.21013, sparse loss :   25.74066, time :  673.75916.\n",
      "Epoch : 266, batch :  1, loss :   28.31822, regression loss :   52.15060, sparse loss :   25.67018, time :  675.90753.\n",
      "Epoch : 267, batch :  1, loss :   28.24904, regression loss :   52.09128, sparse loss :   25.59990, time :  677.88539.\n",
      "Epoch : 268, batch :  1, loss :   28.18006, regression loss :   52.03219, sparse loss :   25.52983, time :  679.73833.\n",
      "Epoch : 269, batch :  1, loss :   28.11129, regression loss :   51.97332, sparse loss :   25.45995, time :  681.61764.\n",
      "Epoch : 270, batch :  1, loss :   28.04272, regression loss :   51.91467, sparse loss :   25.39028, time :  683.48367.\n",
      "Epoch : 271, batch :  1, loss :   27.97435, regression loss :   51.85624, sparse loss :   25.32080, time :  685.42813.\n",
      "Epoch : 272, batch :  1, loss :   27.90618, regression loss :   51.79803, sparse loss :   25.25153, time :  687.42510.\n",
      "Epoch : 273, batch :  1, loss :   27.83822, regression loss :   51.74003, sparse loss :   25.18246, time :  689.40579.\n",
      "Epoch : 274, batch :  1, loss :   27.77046, regression loss :   51.68225, sparse loss :   25.11359, time :  691.20261.\n",
      "Epoch : 275, batch :  1, loss :   27.70290, regression loss :   51.62469, sparse loss :   25.04492, time :  693.17510.\n",
      "Epoch : 276, batch :  1, loss :   27.63554, regression loss :   51.56735, sparse loss :   24.97645, time :  695.19838.\n",
      "Epoch : 277, batch :  1, loss :   27.56838, regression loss :   51.51022, sparse loss :   24.90818, time :  697.20660.\n",
      "Epoch : 278, batch :  1, loss :   27.50143, regression loss :   51.45331, sparse loss :   24.84011, time :  699.16947.\n",
      "Epoch : 279, batch :  1, loss :   27.43467, regression loss :   51.39661, sparse loss :   24.77224, time :  701.07007.\n",
      "Epoch : 280, batch :  1, loss :   27.36812, regression loss :   51.34013, sparse loss :   24.70457, time :  703.08170.\n",
      "Epoch : 281, batch :  1, loss :   27.30177, regression loss :   51.28386, sparse loss :   24.63709, time :  705.02380.\n",
      "Epoch : 282, batch :  1, loss :   27.23562, regression loss :   51.22781, sparse loss :   24.56982, time :  706.83115.\n",
      "Epoch : 283, batch :  1, loss :   27.16966, regression loss :   51.17197, sparse loss :   24.50274, time :  708.78917.\n",
      "Epoch : 284, batch :  1, loss :   27.10391, regression loss :   51.11635, sparse loss :   24.43586, time :  710.70373.\n",
      "Epoch : 285, batch :  1, loss :   27.03836, regression loss :   51.06094, sparse loss :   24.36918, time :  712.78853.\n",
      "Epoch : 286, batch :  1, loss :   26.97300, regression loss :   51.00573, sparse loss :   24.30270, time :  714.60952.\n",
      "Epoch : 287, batch :  1, loss :   26.90785, regression loss :   50.95075, sparse loss :   24.23641, time :  716.50052.\n",
      "Epoch : 288, batch :  1, loss :   26.84289, regression loss :   50.89597, sparse loss :   24.17032, time :  718.42468.\n",
      "Epoch : 289, batch :  1, loss :   26.77813, regression loss :   50.84141, sparse loss :   24.10443, time :  720.46900.\n",
      "Epoch : 290, batch :  1, loss :   26.71357, regression loss :   50.78705, sparse loss :   24.03873, time :  722.32236.\n",
      "Epoch : 291, batch :  1, loss :   26.64920, regression loss :   50.73291, sparse loss :   23.97324, time :  724.44049.\n",
      "Epoch : 292, batch :  1, loss :   26.58503, regression loss :   50.67897, sparse loss :   23.90793, time :  726.34244.\n",
      "Epoch : 293, batch :  1, loss :   26.52106, regression loss :   50.62525, sparse loss :   23.84282, time :  728.34688.\n",
      "Epoch : 294, batch :  1, loss :   26.45729, regression loss :   50.57174, sparse loss :   23.77791, time :  730.24227.\n",
      "Epoch : 295, batch :  1, loss :   26.39371, regression loss :   50.51843, sparse loss :   23.71319, time :  732.14379.\n",
      "Epoch : 296, batch :  1, loss :   26.33033, regression loss :   50.46533, sparse loss :   23.64867, time :  734.06426.\n",
      "Epoch : 297, batch :  1, loss :   26.26715, regression loss :   50.41244, sparse loss :   23.58434, time :  735.96061.\n",
      "Epoch : 298, batch :  1, loss :   26.20416, regression loss :   50.35976, sparse loss :   23.52020, time :  737.95514.\n",
      "Epoch : 299, batch :  1, loss :   26.14136, regression loss :   50.30729, sparse loss :   23.45626, time :  739.89105.\n",
      "Epoch : 300, batch :  1, loss :   26.07876, regression loss :   50.25502, sparse loss :   23.39251, time :  744.67642.\n",
      "Epoch : 301, batch :  1, loss :   26.01636, regression loss :   50.20296, sparse loss :   23.32895, time :  746.71570.\n",
      "Epoch : 302, batch :  1, loss :   25.95414, regression loss :   50.15110, sparse loss :   23.26559, time :  748.69678.\n",
      "Epoch : 303, batch :  1, loss :   25.89212, regression loss :   50.09945, sparse loss :   23.20242, time :  750.67175.\n",
      "Epoch : 304, batch :  1, loss :   25.83030, regression loss :   50.04801, sparse loss :   23.13944, time :  752.56714.\n",
      "Epoch : 305, batch :  1, loss :   25.76866, regression loss :   49.99677, sparse loss :   23.07665, time :  754.44372.\n",
      "Epoch : 306, batch :  1, loss :   25.70722, regression loss :   49.94573, sparse loss :   23.01406, time :  756.33696.\n",
      "Epoch : 307, batch :  1, loss :   25.64598, regression loss :   49.89490, sparse loss :   22.95165, time :  758.23277.\n",
      "Epoch : 308, batch :  1, loss :   25.58492, regression loss :   49.84427, sparse loss :   22.88944, time :  760.09835.\n",
      "Epoch : 309, batch :  1, loss :   25.52406, regression loss :   49.79384, sparse loss :   22.82741, time :  762.18861.\n",
      "Epoch : 310, batch :  1, loss :   25.46338, regression loss :   49.74362, sparse loss :   22.76558, time :  764.34347.\n",
      "Epoch : 311, batch :  1, loss :   25.40290, regression loss :   49.69360, sparse loss :   22.70394, time :  766.35272.\n",
      "Epoch : 312, batch :  1, loss :   25.34261, regression loss :   49.64378, sparse loss :   22.64248, time :  768.22542.\n",
      "Epoch : 313, batch :  1, loss :   25.28251, regression loss :   49.59416, sparse loss :   22.58121, time :  770.35846.\n",
      "Epoch : 314, batch :  1, loss :   25.22260, regression loss :   49.54474, sparse loss :   22.52014, time :  772.41322.\n",
      "Epoch : 315, batch :  1, loss :   25.16287, regression loss :   49.49552, sparse loss :   22.45925, time :  774.44684.\n",
      "Epoch : 316, batch :  1, loss :   25.10334, regression loss :   49.44650, sparse loss :   22.39855, time :  776.34928.\n",
      "Epoch : 317, batch :  1, loss :   25.04400, regression loss :   49.39769, sparse loss :   22.33803, time :  778.42153.\n",
      "Epoch : 318, batch :  1, loss :   24.98484, regression loss :   49.34907, sparse loss :   22.27770, time :  780.39223.\n",
      "Epoch : 319, batch :  1, loss :   24.92587, regression loss :   49.30064, sparse loss :   22.21756, time :  782.38283.\n",
      "Epoch : 320, batch :  1, loss :   24.86709, regression loss :   49.25242, sparse loss :   22.15761, time :  784.23780.\n",
      "Epoch : 321, batch :  1, loss :   24.80850, regression loss :   49.20440, sparse loss :   22.09784, time :  786.06133.\n",
      "Epoch : 322, batch :  1, loss :   24.75009, regression loss :   49.15657, sparse loss :   22.03826, time :  788.14163.\n",
      "Epoch : 323, batch :  1, loss :   24.69187, regression loss :   49.10893, sparse loss :   21.97886, time :  790.16028.\n",
      "Epoch : 324, batch :  1, loss :   24.63383, regression loss :   49.06150, sparse loss :   21.91965, time :  792.15085.\n",
      "Epoch : 325, batch :  1, loss :   24.57598, regression loss :   49.01425, sparse loss :   21.86062, time :  794.06030.\n",
      "Epoch : 326, batch :  1, loss :   24.51832, regression loss :   48.96721, sparse loss :   21.80177, time :  796.13128.\n",
      "Epoch : 327, batch :  1, loss :   24.46084, regression loss :   48.92035, sparse loss :   21.74311, time :  798.08811.\n",
      "Epoch : 328, batch :  1, loss :   24.40354, regression loss :   48.87370, sparse loss :   21.68464, time :  800.12718.\n",
      "Epoch : 329, batch :  1, loss :   24.34643, regression loss :   48.82723, sparse loss :   21.62634, time :  802.00928.\n",
      "Epoch : 330, batch :  1, loss :   24.28950, regression loss :   48.78096, sparse loss :   21.56823, time :  803.97698.\n",
      "Epoch : 331, batch :  1, loss :   24.23276, regression loss :   48.73488, sparse loss :   21.51030, time :  805.83345.\n",
      "Epoch : 332, batch :  1, loss :   24.17620, regression loss :   48.68899, sparse loss :   21.45256, time :  807.72325.\n",
      "Epoch : 333, batch :  1, loss :   24.11982, regression loss :   48.64330, sparse loss :   21.39499, time :  809.80043.\n",
      "Epoch : 334, batch :  1, loss :   24.06362, regression loss :   48.59779, sparse loss :   21.33760, time :  811.65999.\n",
      "Epoch : 335, batch :  1, loss :   24.00761, regression loss :   48.55247, sparse loss :   21.28040, time :  813.61379.\n",
      "Epoch : 336, batch :  1, loss :   23.95177, regression loss :   48.50735, sparse loss :   21.22337, time :  815.53911.\n",
      "Epoch : 337, batch :  1, loss :   23.89612, regression loss :   48.46241, sparse loss :   21.16653, time :  817.48243.\n",
      "Epoch : 338, batch :  1, loss :   23.84064, regression loss :   48.41766, sparse loss :   21.10986, time :  819.67175.\n",
      "Epoch : 339, batch :  1, loss :   23.78535, regression loss :   48.37310, sparse loss :   21.05338, time :  821.66674.\n",
      "Epoch : 340, batch :  1, loss :   23.73023, regression loss :   48.32873, sparse loss :   20.99707, time :  823.52266.\n",
      "Epoch : 341, batch :  1, loss :   23.67530, regression loss :   48.28454, sparse loss :   20.94094, time :  825.43750.\n",
      "Epoch : 342, batch :  1, loss :   23.62054, regression loss :   48.24054, sparse loss :   20.88499, time :  827.38383.\n",
      "Epoch : 343, batch :  1, loss :   23.56596, regression loss :   48.19673, sparse loss :   20.82921, time :  829.48188.\n",
      "Epoch : 344, batch :  1, loss :   23.51156, regression loss :   48.15310, sparse loss :   20.77361, time :  831.32770.\n",
      "Epoch : 345, batch :  1, loss :   23.45734, regression loss :   48.10966, sparse loss :   20.71820, time :  833.28102.\n",
      "Epoch : 346, batch :  1, loss :   23.40330, regression loss :   48.06640, sparse loss :   20.66295, time :  835.37240.\n",
      "Epoch : 347, batch :  1, loss :   23.34943, regression loss :   48.02332, sparse loss :   20.60788, time :  837.38519.\n",
      "Epoch : 348, batch :  1, loss :   23.29573, regression loss :   47.98043, sparse loss :   20.55299, time :  839.33082.\n",
      "Epoch : 349, batch :  1, loss :   23.24222, regression loss :   47.93771, sparse loss :   20.49827, time :  841.36548.\n",
      "Epoch : 350, batch :  1, loss :   23.18888, regression loss :   47.89518, sparse loss :   20.44373, time :  843.31536.\n",
      "Epoch : 351, batch :  1, loss :   23.13570, regression loss :   47.85284, sparse loss :   20.38936, time :  845.29331.\n",
      "Epoch : 352, batch :  1, loss :   23.08271, regression loss :   47.81067, sparse loss :   20.33516, time :  847.21404.\n",
      "Epoch : 353, batch :  1, loss :   23.02989, regression loss :   47.76868, sparse loss :   20.28114, time :  849.17794.\n",
      "Epoch : 354, batch :  1, loss :   22.97725, regression loss :   47.72687, sparse loss :   20.22729, time :  851.09680.\n",
      "Epoch : 355, batch :  1, loss :   22.92477, regression loss :   47.68524, sparse loss :   20.17361, time :  853.00429.\n",
      "Epoch : 356, batch :  1, loss :   22.87248, regression loss :   47.64379, sparse loss :   20.12011, time :  854.99321.\n",
      "Epoch : 357, batch :  1, loss :   22.82035, regression loss :   47.60251, sparse loss :   20.06677, time :  856.88971.\n",
      "Epoch : 358, batch :  1, loss :   22.76839, regression loss :   47.56141, sparse loss :   20.01361, time :  858.94746.\n",
      "Epoch : 359, batch :  1, loss :   22.71661, regression loss :   47.52049, sparse loss :   19.96062, time :  860.95187.\n",
      "Epoch : 360, batch :  1, loss :   22.66499, regression loss :   47.47974, sparse loss :   19.90780, time :  862.95365.\n",
      "Epoch : 361, batch :  1, loss :   22.61355, regression loss :   47.43917, sparse loss :   19.85515, time :  865.09021.\n",
      "Epoch : 362, batch :  1, loss :   22.56228, regression loss :   47.39877, sparse loss :   19.80267, time :  866.98893.\n",
      "Epoch : 363, batch :  1, loss :   22.51117, regression loss :   47.35855, sparse loss :   19.75035, time :  869.14770.\n",
      "Epoch : 364, batch :  1, loss :   22.46024, regression loss :   47.31850, sparse loss :   19.69821, time :  871.07741.\n",
      "Epoch : 365, batch :  1, loss :   22.40948, regression loss :   47.27862, sparse loss :   19.64624, time :  872.94196.\n",
      "Epoch : 366, batch :  1, loss :   22.35888, regression loss :   47.23891, sparse loss :   19.59443, time :  874.82349.\n",
      "Epoch : 367, batch :  1, loss :   22.30845, regression loss :   47.19937, sparse loss :   19.54279, time :  876.78615.\n",
      "Epoch : 368, batch :  1, loss :   22.25819, regression loss :   47.16001, sparse loss :   19.49132, time :  878.64868.\n",
      "Epoch : 369, batch :  1, loss :   22.20809, regression loss :   47.12081, sparse loss :   19.44001, time :  880.58352.\n",
      "Epoch : 370, batch :  1, loss :   22.15816, regression loss :   47.08179, sparse loss :   19.38887, time :  882.45186.\n",
      "Epoch : 371, batch :  1, loss :   22.10840, regression loss :   47.04293, sparse loss :   19.33790, time :  884.33815.\n",
      "Epoch : 372, batch :  1, loss :   22.05881, regression loss :   47.00424, sparse loss :   19.28709, time :  886.22734.\n",
      "Epoch : 373, batch :  1, loss :   22.00938, regression loss :   46.96571, sparse loss :   19.23645, time :  888.32204.\n",
      "Epoch : 374, batch :  1, loss :   21.96011, regression loss :   46.92736, sparse loss :   19.18597, time :  890.26935.\n",
      "Epoch : 375, batch :  1, loss :   21.91101, regression loss :   46.88917, sparse loss :   19.13566, time :  892.15606.\n",
      "Epoch : 376, batch :  1, loss :   21.86207, regression loss :   46.85114, sparse loss :   19.08551, time :  894.26941.\n",
      "Epoch : 377, batch :  1, loss :   21.81330, regression loss :   46.81328, sparse loss :   19.03552, time :  896.36325.\n",
      "Epoch : 378, batch :  1, loss :   21.76468, regression loss :   46.77558, sparse loss :   18.98569, time :  898.35150.\n",
      "Epoch : 379, batch :  1, loss :   21.71623, regression loss :   46.73805, sparse loss :   18.93603, time :  900.26906.\n",
      "Epoch : 380, batch :  1, loss :   21.66795, regression loss :   46.70068, sparse loss :   18.88653, time :  902.13163.\n",
      "Epoch : 381, batch :  1, loss :   21.61982, regression loss :   46.66347, sparse loss :   18.83719, time :  904.08042.\n",
      "Epoch : 382, batch :  1, loss :   21.57186, regression loss :   46.62642, sparse loss :   18.78802, time :  906.19725.\n",
      "Epoch : 383, batch :  1, loss :   21.52405, regression loss :   46.58954, sparse loss :   18.73900, time :  908.16844.\n",
      "Epoch : 384, batch :  1, loss :   21.47641, regression loss :   46.55281, sparse loss :   18.69015, time :  910.13544.\n",
      "Epoch : 385, batch :  1, loss :   21.42893, regression loss :   46.51624, sparse loss :   18.64145, time :  912.00908.\n",
      "Epoch : 386, batch :  1, loss :   21.38160, regression loss :   46.47983, sparse loss :   18.59291, time :  913.92015.\n",
      "Epoch : 387, batch :  1, loss :   21.33444, regression loss :   46.44359, sparse loss :   18.54453, time :  915.90517.\n",
      "Epoch : 388, batch :  1, loss :   21.28743, regression loss :   46.40749, sparse loss :   18.49631, time :  917.83203.\n",
      "Epoch : 389, batch :  1, loss :   21.24058, regression loss :   46.37156, sparse loss :   18.44825, time :  919.82795.\n",
      "Epoch : 390, batch :  1, loss :   21.19389, regression loss :   46.33578, sparse loss :   18.40035, time :  921.90667.\n",
      "Epoch : 391, batch :  1, loss :   21.14736, regression loss :   46.30015, sparse loss :   18.35260, time :  923.78011.\n",
      "Epoch : 392, batch :  1, loss :   21.10098, regression loss :   46.26468, sparse loss :   18.30501, time :  925.83303.\n",
      "Epoch : 393, batch :  1, loss :   21.05476, regression loss :   46.22937, sparse loss :   18.25758, time :  927.85213.\n",
      "Epoch : 394, batch :  1, loss :   21.00870, regression loss :   46.19421, sparse loss :   18.21031, time :  929.72144.\n",
      "Epoch : 395, batch :  1, loss :   20.96279, regression loss :   46.15920, sparse loss :   18.16319, time :  931.60916.\n",
      "Epoch : 396, batch :  1, loss :   20.91703, regression loss :   46.12434, sparse loss :   18.11622, time :  933.43622.\n",
      "Epoch : 397, batch :  1, loss :   20.87143, regression loss :   46.08964, sparse loss :   18.06941, time :  935.50347.\n",
      "Epoch : 398, batch :  1, loss :   20.82598, regression loss :   46.05508, sparse loss :   18.02275, time :  937.62957.\n",
      "Epoch : 399, batch :  1, loss :   20.78069, regression loss :   46.02068, sparse loss :   17.97625, time :  939.54605.\n",
      "Epoch : 400, batch :  1, loss :   20.73555, regression loss :   45.98642, sparse loss :   17.92990, time :  944.22285.\n",
      "Epoch : 401, batch :  1, loss :   20.69057, regression loss :   45.95232, sparse loss :   17.88370, time :  946.23069.\n",
      "Epoch : 402, batch :  1, loss :   20.64573, regression loss :   45.91836, sparse loss :   17.83766, time :  948.14295.\n",
      "Epoch : 403, batch :  1, loss :   20.60105, regression loss :   45.88455, sparse loss :   17.79177, time :  950.41634.\n",
      "Epoch : 404, batch :  1, loss :   20.55652, regression loss :   45.85089, sparse loss :   17.74603, time :  952.39481.\n",
      "Epoch : 405, batch :  1, loss :   20.51214, regression loss :   45.81738, sparse loss :   17.70044, time :  954.51618.\n",
      "Epoch : 406, batch :  1, loss :   20.46790, regression loss :   45.78401, sparse loss :   17.65500, time :  956.51909.\n",
      "Epoch : 407, batch :  1, loss :   20.42382, regression loss :   45.75078, sparse loss :   17.60972, time :  958.53101.\n",
      "Epoch : 408, batch :  1, loss :   20.37989, regression loss :   45.71770, sparse loss :   17.56458, time :  960.47731.\n",
      "Epoch : 409, batch :  1, loss :   20.33611, regression loss :   45.68477, sparse loss :   17.51959, time :  962.39208.\n",
      "Epoch : 410, batch :  1, loss :   20.29248, regression loss :   45.65198, sparse loss :   17.47476, time :  964.48330.\n",
      "Epoch : 411, batch :  1, loss :   20.24899, regression loss :   45.61932, sparse loss :   17.43007, time :  966.50700.\n",
      "Epoch : 412, batch :  1, loss :   20.20566, regression loss :   45.58682, sparse loss :   17.38553, time :  968.61590.\n",
      "Epoch : 413, batch :  1, loss :   20.16247, regression loss :   45.55445, sparse loss :   17.34114, time :  970.59350.\n",
      "Epoch : 414, batch :  1, loss :   20.11942, regression loss :   45.52222, sparse loss :   17.29689, time :  972.52223.\n",
      "Epoch : 415, batch :  1, loss :   20.07653, regression loss :   45.49014, sparse loss :   17.25279, time :  974.47965.\n",
      "Epoch : 416, batch :  1, loss :   20.03378, regression loss :   45.45819, sparse loss :   17.20884, time :  976.52487.\n",
      "Epoch : 417, batch :  1, loss :   19.99117, regression loss :   45.42638, sparse loss :   17.16504, time :  978.39835.\n",
      "Epoch : 418, batch :  1, loss :   19.94871, regression loss :   45.39471, sparse loss :   17.12138, time :  980.45004.\n",
      "Epoch : 419, batch :  1, loss :   19.90639, regression loss :   45.36318, sparse loss :   17.07786, time :  982.46682.\n",
      "Epoch : 420, batch :  1, loss :   19.86422, regression loss :   45.33178, sparse loss :   17.03449, time :  984.56469.\n",
      "Epoch : 421, batch :  1, loss :   19.82220, regression loss :   45.30052, sparse loss :   16.99127, time :  986.59898.\n",
      "Epoch : 422, batch :  1, loss :   19.78031, regression loss :   45.26940, sparse loss :   16.94819, time :  988.62111.\n",
      "Epoch : 423, batch :  1, loss :   19.73857, regression loss :   45.23841, sparse loss :   16.90526, time :  990.69059.\n",
      "Epoch : 424, batch :  1, loss :   19.69697, regression loss :   45.20756, sparse loss :   16.86246, time :  992.64736.\n",
      "Epoch : 425, batch :  1, loss :   19.65551, regression loss :   45.17684, sparse loss :   16.81981, time :  994.89611.\n",
      "Epoch : 426, batch :  1, loss :   19.61420, regression loss :   45.14625, sparse loss :   16.77730, time :  996.97604.\n",
      "Epoch : 427, batch :  1, loss :   19.57302, regression loss :   45.11579, sparse loss :   16.73494, time :  998.98211.\n",
      "Epoch : 428, batch :  1, loss :   19.53199, regression loss :   45.08547, sparse loss :   16.69271, time : 1001.16519.\n",
      "Epoch : 429, batch :  1, loss :   19.49110, regression loss :   45.05528, sparse loss :   16.65063, time : 1003.21653.\n",
      "Epoch : 430, batch :  1, loss :   19.45034, regression loss :   45.02522, sparse loss :   16.60869, time : 1005.25742.\n",
      "Epoch : 431, batch :  1, loss :   19.40973, regression loss :   44.99529, sparse loss :   16.56689, time : 1007.31989.\n",
      "Epoch : 432, batch :  1, loss :   19.36925, regression loss :   44.96549, sparse loss :   16.52523, time : 1009.27399.\n",
      "Epoch : 433, batch :  1, loss :   19.32892, regression loss :   44.93582, sparse loss :   16.48371, time : 1011.27992.\n",
      "Epoch : 434, batch :  1, loss :   19.28872, regression loss :   44.90627, sparse loss :   16.44232, time : 1013.24048.\n",
      "Epoch : 435, batch :  1, loss :   19.24865, regression loss :   44.87686, sparse loss :   16.40108, time : 1015.37205.\n",
      "Epoch : 436, batch :  1, loss :   19.20873, regression loss :   44.84757, sparse loss :   16.35997, time : 1017.39595.\n",
      "Epoch : 437, batch :  1, loss :   19.16894, regression loss :   44.81841, sparse loss :   16.31900, time : 1019.41413.\n",
      "Epoch : 438, batch :  1, loss :   19.12929, regression loss :   44.78937, sparse loss :   16.27817, time : 1021.40227.\n",
      "Epoch : 439, batch :  1, loss :   19.08978, regression loss :   44.76046, sparse loss :   16.23748, time : 1023.40378.\n",
      "Epoch : 440, batch :  1, loss :   19.05040, regression loss :   44.73167, sparse loss :   16.19692, time : 1025.40128.\n",
      "Epoch : 441, batch :  1, loss :   19.01115, regression loss :   44.70301, sparse loss :   16.15650, time : 1027.39736.\n",
      "Epoch : 442, batch :  1, loss :   18.97204, regression loss :   44.67447, sparse loss :   16.11622, time : 1029.47967.\n",
      "Epoch : 443, batch :  1, loss :   18.93307, regression loss :   44.64606, sparse loss :   16.07607, time : 1031.62003.\n",
      "Epoch : 444, batch :  1, loss :   18.89422, regression loss :   44.61776, sparse loss :   16.03605, time : 1033.55596.\n",
      "Epoch : 445, batch :  1, loss :   18.85551, regression loss :   44.58959, sparse loss :   15.99617, time : 1035.64998.\n",
      "Epoch : 446, batch :  1, loss :   18.81694, regression loss :   44.56154, sparse loss :   15.95643, time : 1037.57279.\n",
      "Epoch : 447, batch :  1, loss :   18.77849, regression loss :   44.53361, sparse loss :   15.91681, time : 1039.60973.\n",
      "Epoch : 448, batch :  1, loss :   18.74018, regression loss :   44.50580, sparse loss :   15.87734, time : 1041.64308.\n",
      "Epoch : 449, batch :  1, loss :   18.70200, regression loss :   44.47811, sparse loss :   15.83799, time : 1043.61241.\n",
      "Epoch : 450, batch :  1, loss :   18.66395, regression loss :   44.45054, sparse loss :   15.79878, time : 1045.64645.\n",
      "Epoch : 451, batch :  1, loss :   18.62603, regression loss :   44.42309, sparse loss :   15.75970, time : 1047.77325.\n",
      "Epoch : 452, batch :  1, loss :   18.58825, regression loss :   44.39575, sparse loss :   15.72075, time : 1049.91081.\n",
      "Epoch : 453, batch :  1, loss :   18.55059, regression loss :   44.36853, sparse loss :   15.68193, time : 1051.95182.\n",
      "Epoch : 454, batch :  1, loss :   18.51306, regression loss :   44.34143, sparse loss :   15.64324, time : 1053.93018.\n",
      "Epoch : 455, batch :  1, loss :   18.47566, regression loss :   44.31445, sparse loss :   15.60468, time : 1055.94200.\n",
      "Epoch : 456, batch :  1, loss :   18.43839, regression loss :   44.28758, sparse loss :   15.56626, time : 1057.91129.\n",
      "Epoch : 457, batch :  1, loss :   18.40125, regression loss :   44.26082, sparse loss :   15.52796, time : 1059.83865.\n",
      "Epoch : 458, batch :  1, loss :   18.36423, regression loss :   44.23418, sparse loss :   15.48979, time : 1061.73275.\n",
      "Epoch : 459, batch :  1, loss :   18.32734, regression loss :   44.20766, sparse loss :   15.45175, time : 1063.72379.\n",
      "Epoch : 460, batch :  1, loss :   18.29058, regression loss :   44.18125, sparse loss :   15.41384, time : 1065.91036.\n",
      "Epoch : 461, batch :  1, loss :   18.25395, regression loss :   44.15494, sparse loss :   15.37606, time : 1067.93402.\n",
      "Epoch : 462, batch :  1, loss :   18.21744, regression loss :   44.12876, sparse loss :   15.33841, time : 1069.90654.\n",
      "Epoch : 463, batch :  1, loss :   18.18106, regression loss :   44.10268, sparse loss :   15.30088, time : 1071.77517.\n",
      "Epoch : 464, batch :  1, loss :   18.14480, regression loss :   44.07671, sparse loss :   15.26348, time : 1073.70840.\n",
      "Epoch : 465, batch :  1, loss :   18.10867, regression loss :   44.05086, sparse loss :   15.22620, time : 1075.99722.\n",
      "Epoch : 466, batch :  1, loss :   18.07266, regression loss :   44.02512, sparse loss :   15.18906, time : 1078.07282.\n",
      "Epoch : 467, batch :  1, loss :   18.03678, regression loss :   43.99948, sparse loss :   15.15203, time : 1080.10930.\n",
      "Epoch : 468, batch :  1, loss :   18.00102, regression loss :   43.97395, sparse loss :   15.11514, time : 1082.23150.\n",
      "Epoch : 469, batch :  1, loss :   17.96538, regression loss :   43.94854, sparse loss :   15.07837, time : 1084.27161.\n",
      "Epoch : 470, batch :  1, loss :   17.92987, regression loss :   43.92323, sparse loss :   15.04172, time : 1086.22702.\n",
      "Epoch : 471, batch :  1, loss :   17.89448, regression loss :   43.89803, sparse loss :   15.00519, time : 1088.32451.\n",
      "Epoch : 472, batch :  1, loss :   17.85921, regression loss :   43.87293, sparse loss :   14.96880, time : 1090.43855.\n",
      "Epoch : 473, batch :  1, loss :   17.82406, regression loss :   43.84794, sparse loss :   14.93252, time : 1092.43018.\n",
      "Epoch : 474, batch :  1, loss :   17.78903, regression loss :   43.82306, sparse loss :   14.89637, time : 1094.54615.\n",
      "Epoch : 475, batch :  1, loss :   17.75413, regression loss :   43.79828, sparse loss :   14.86034, time : 1096.57001.\n",
      "Epoch : 476, batch :  1, loss :   17.71935, regression loss :   43.77361, sparse loss :   14.82443, time : 1098.58819.\n",
      "Epoch : 477, batch :  1, loss :   17.68468, regression loss :   43.74904, sparse loss :   14.78864, time : 1100.69519.\n",
      "Epoch : 478, batch :  1, loss :   17.65014, regression loss :   43.72458, sparse loss :   14.75298, time : 1102.72447.\n",
      "Epoch : 479, batch :  1, loss :   17.61571, regression loss :   43.70022, sparse loss :   14.71743, time : 1104.69924.\n",
      "Epoch : 480, batch :  1, loss :   17.58140, regression loss :   43.67596, sparse loss :   14.68200, time : 1106.62101.\n",
      "Epoch : 481, batch :  1, loss :   17.54721, regression loss :   43.65180, sparse loss :   14.64670, time : 1108.58681.\n",
      "Epoch : 482, batch :  1, loss :   17.51314, regression loss :   43.62775, sparse loss :   14.61152, time : 1110.57439.\n",
      "Epoch : 483, batch :  1, loss :   17.47919, regression loss :   43.60379, sparse loss :   14.57646, time : 1112.76749.\n",
      "Epoch : 484, batch :  1, loss :   17.44535, regression loss :   43.57994, sparse loss :   14.54151, time : 1114.75200.\n",
      "Epoch : 485, batch :  1, loss :   17.41163, regression loss :   43.55619, sparse loss :   14.50668, time : 1116.65500.\n",
      "Epoch : 486, batch :  1, loss :   17.37803, regression loss :   43.53254, sparse loss :   14.47198, time : 1118.87139.\n",
      "Epoch : 487, batch :  1, loss :   17.34454, regression loss :   43.50898, sparse loss :   14.43738, time : 1120.95741.\n",
      "Epoch : 488, batch :  1, loss :   17.31117, regression loss :   43.48553, sparse loss :   14.40291, time : 1122.95335.\n",
      "Epoch : 489, batch :  1, loss :   17.27792, regression loss :   43.46217, sparse loss :   14.36856, time : 1124.82230.\n",
      "Epoch : 490, batch :  1, loss :   17.24478, regression loss :   43.43892, sparse loss :   14.33432, time : 1126.80500.\n",
      "Epoch : 491, batch :  1, loss :   17.21175, regression loss :   43.41576, sparse loss :   14.30020, time : 1129.03278.\n",
      "Epoch : 492, batch :  1, loss :   17.17884, regression loss :   43.39269, sparse loss :   14.26619, time : 1131.10257.\n",
      "Epoch : 493, batch :  1, loss :   17.14604, regression loss :   43.36973, sparse loss :   14.23230, time : 1133.17328.\n",
      "Epoch : 494, batch :  1, loss :   17.11336, regression loss :   43.34686, sparse loss :   14.19852, time : 1135.11829.\n",
      "Epoch : 495, batch :  1, loss :   17.08078, regression loss :   43.32408, sparse loss :   14.16486, time : 1137.08162.\n",
      "Epoch : 496, batch :  1, loss :   17.04833, regression loss :   43.30140, sparse loss :   14.13132, time : 1139.19541.\n",
      "Epoch : 497, batch :  1, loss :   17.01598, regression loss :   43.27881, sparse loss :   14.09789, time : 1141.21432.\n",
      "Epoch : 498, batch :  1, loss :   16.98374, regression loss :   43.25632, sparse loss :   14.06457, time : 1143.35543.\n",
      "Epoch : 499, batch :  1, loss :   16.95162, regression loss :   43.23393, sparse loss :   14.03136, time : 1145.38466.\n",
      "Epoch : 500, batch :  1, loss :   16.91961, regression loss :   43.21162, sparse loss :   13.99827, time : 1149.75976.\n",
      "Epoch : 501, batch :  1, loss :   16.88770, regression loss :   43.18941, sparse loss :   13.96529, time : 1151.84295.\n",
      "Epoch : 502, batch :  1, loss :   16.85591, regression loss :   43.16729, sparse loss :   13.93243, time : 1153.76684.\n",
      "Epoch : 503, batch :  1, loss :   16.82423, regression loss :   43.14526, sparse loss :   13.89967, time : 1155.76437.\n",
      "Epoch : 504, batch :  1, loss :   16.79266, regression loss :   43.12333, sparse loss :   13.86703, time : 1157.70402.\n",
      "Epoch : 505, batch :  1, loss :   16.76119, regression loss :   43.10148, sparse loss :   13.83450, time : 1159.65159.\n",
      "Epoch : 506, batch :  1, loss :   16.72984, regression loss :   43.07973, sparse loss :   13.80208, time : 1161.62853.\n",
      "Epoch : 507, batch :  1, loss :   16.69859, regression loss :   43.05806, sparse loss :   13.76976, time : 1163.71707.\n",
      "Epoch : 508, batch :  1, loss :   16.66746, regression loss :   43.03649, sparse loss :   13.73757, time : 1165.72755.\n",
      "Epoch : 509, batch :  1, loss :   16.63643, regression loss :   43.01500, sparse loss :   13.70548, time : 1167.74538.\n",
      "Epoch : 510, batch :  1, loss :   16.60551, regression loss :   42.99360, sparse loss :   13.67349, time : 1169.67093.\n",
      "Epoch : 511, batch :  1, loss :   16.57469, regression loss :   42.97229, sparse loss :   13.64162, time : 1171.63721.\n",
      "Epoch : 512, batch :  1, loss :   16.54398, regression loss :   42.95107, sparse loss :   13.60986, time : 1173.64601.\n",
      "Epoch : 513, batch :  1, loss :   16.51338, regression loss :   42.92994, sparse loss :   13.57820, time : 1175.63635.\n",
      "Epoch : 514, batch :  1, loss :   16.48288, regression loss :   42.90889, sparse loss :   13.54666, time : 1177.72152.\n",
      "Epoch : 515, batch :  1, loss :   16.45249, regression loss :   42.88794, sparse loss :   13.51522, time : 1179.76353.\n",
      "Epoch : 516, batch :  1, loss :   16.42220, regression loss :   42.86706, sparse loss :   13.48388, time : 1181.97471.\n",
      "Epoch : 517, batch :  1, loss :   16.39202, regression loss :   42.84628, sparse loss :   13.45266, time : 1183.94786.\n",
      "Epoch : 518, batch :  1, loss :   16.36194, regression loss :   42.82557, sparse loss :   13.42154, time : 1185.93230.\n",
      "Epoch : 519, batch :  1, loss :   16.33197, regression loss :   42.80496, sparse loss :   13.39053, time : 1187.84935.\n",
      "Epoch : 520, batch :  1, loss :   16.30210, regression loss :   42.78442, sparse loss :   13.35962, time : 1189.88216.\n",
      "Epoch : 521, batch :  1, loss :   16.27233, regression loss :   42.76398, sparse loss :   13.32882, time : 1191.87971.\n",
      "Epoch : 522, batch :  1, loss :   16.24267, regression loss :   42.74361, sparse loss :   13.29812, time : 1193.84639.\n",
      "Epoch : 523, batch :  1, loss :   16.21311, regression loss :   42.72333, sparse loss :   13.26753, time : 1195.82601.\n",
      "Epoch : 524, batch :  1, loss :   16.18365, regression loss :   42.70313, sparse loss :   13.23704, time : 1197.76362.\n",
      "Epoch : 525, batch :  1, loss :   16.15430, regression loss :   42.68302, sparse loss :   13.20666, time : 1199.99763.\n",
      "Epoch : 526, batch :  1, loss :   16.12504, regression loss :   42.66298, sparse loss :   13.17638, time : 1202.08329.\n",
      "Epoch : 527, batch :  1, loss :   16.09589, regression loss :   42.64303, sparse loss :   13.14620, time : 1204.22209.\n",
      "Epoch : 528, batch :  1, loss :   16.06684, regression loss :   42.62316, sparse loss :   13.11613, time : 1206.23700.\n",
      "Epoch : 529, batch :  1, loss :   16.03788, regression loss :   42.60338, sparse loss :   13.08616, time : 1208.16528.\n",
      "Epoch : 530, batch :  1, loss :   16.00903, regression loss :   42.58367, sparse loss :   13.05629, time : 1210.12700.\n",
      "Epoch : 531, batch :  1, loss :   15.98028, regression loss :   42.56404, sparse loss :   13.02653, time : 1212.24135.\n",
      "Epoch : 532, batch :  1, loss :   15.95162, regression loss :   42.54449, sparse loss :   12.99686, time : 1214.31908.\n",
      "Epoch : 533, batch :  1, loss :   15.92307, regression loss :   42.52502, sparse loss :   12.96730, time : 1216.33745.\n",
      "Epoch : 534, batch :  1, loss :   15.89461, regression loss :   42.50563, sparse loss :   12.93783, time : 1218.38840.\n",
      "Epoch : 535, batch :  1, loss :   15.86626, regression loss :   42.48632, sparse loss :   12.90847, time : 1220.44691.\n",
      "Epoch : 536, batch :  1, loss :   15.83800, regression loss :   42.46709, sparse loss :   12.87921, time : 1222.35272.\n",
      "Epoch : 537, batch :  1, loss :   15.80984, regression loss :   42.44793, sparse loss :   12.85005, time : 1224.51193.\n",
      "Epoch : 538, batch :  1, loss :   15.78178, regression loss :   42.42886, sparse loss :   12.82099, time : 1226.41620.\n",
      "Epoch : 539, batch :  1, loss :   15.75381, regression loss :   42.40986, sparse loss :   12.79203, time : 1228.34727.\n",
      "Epoch : 540, batch :  1, loss :   15.72594, regression loss :   42.39094, sparse loss :   12.76316, time : 1230.30980.\n",
      "Epoch : 541, batch :  1, loss :   15.69817, regression loss :   42.37209, sparse loss :   12.73440, time : 1232.23426.\n",
      "Epoch : 542, batch :  1, loss :   15.67049, regression loss :   42.35332, sparse loss :   12.70573, time : 1234.14886.\n",
      "Epoch : 543, batch :  1, loss :   15.64291, regression loss :   42.33463, sparse loss :   12.67716, time : 1236.27145.\n",
      "Epoch : 544, batch :  1, loss :   15.61542, regression loss :   42.31601, sparse loss :   12.64869, time : 1238.39491.\n",
      "Epoch : 545, batch :  1, loss :   15.58803, regression loss :   42.29746, sparse loss :   12.62031, time : 1240.46596.\n",
      "Epoch : 546, batch :  1, loss :   15.56073, regression loss :   42.27899, sparse loss :   12.59204, time : 1242.42275.\n",
      "Epoch : 547, batch :  1, loss :   15.53353, regression loss :   42.26060, sparse loss :   12.56386, time : 1244.65889.\n",
      "Epoch : 548, batch :  1, loss :   15.50642, regression loss :   42.24228, sparse loss :   12.53577, time : 1246.66314.\n",
      "Epoch : 549, batch :  1, loss :   15.47941, regression loss :   42.22404, sparse loss :   12.50778, time : 1248.61976.\n",
      "Epoch : 550, batch :  1, loss :   15.45249, regression loss :   42.20586, sparse loss :   12.47989, time : 1250.61416.\n",
      "Epoch : 551, batch :  1, loss :   15.42566, regression loss :   42.18776, sparse loss :   12.45209, time : 1252.82710.\n",
      "Epoch : 552, batch :  1, loss :   15.39892, regression loss :   42.16974, sparse loss :   12.42439, time : 1254.94704.\n",
      "Epoch : 553, batch :  1, loss :   15.37228, regression loss :   42.15178, sparse loss :   12.39678, time : 1256.95278.\n",
      "Epoch : 554, batch :  1, loss :   15.34573, regression loss :   42.13390, sparse loss :   12.36927, time : 1259.00196.\n",
      "Epoch : 555, batch :  1, loss :   15.31927, regression loss :   42.11608, sparse loss :   12.34185, time : 1260.94072.\n",
      "Epoch : 556, batch :  1, loss :   15.29290, regression loss :   42.09834, sparse loss :   12.31452, time : 1263.09995.\n",
      "Epoch : 557, batch :  1, loss :   15.26663, regression loss :   42.08068, sparse loss :   12.28729, time : 1265.13393.\n",
      "Epoch : 558, batch :  1, loss :   15.24044, regression loss :   42.06308, sparse loss :   12.26015, time : 1267.09073.\n",
      "Epoch : 559, batch :  1, loss :   15.21434, regression loss :   42.04555, sparse loss :   12.23310, time : 1269.10831.\n",
      "Epoch : 560, batch :  1, loss :   15.18834, regression loss :   42.02809, sparse loss :   12.20615, time : 1271.13891.\n",
      "Epoch : 561, batch :  1, loss :   15.16242, regression loss :   42.01070, sparse loss :   12.17928, time : 1273.27780.\n",
      "Epoch : 562, batch :  1, loss :   15.13660, regression loss :   41.99338, sparse loss :   12.15251, time : 1275.28663.\n",
      "Epoch : 563, batch :  1, loss :   15.11086, regression loss :   41.97614, sparse loss :   12.12583, time : 1277.38018.\n",
      "Epoch : 564, batch :  1, loss :   15.08521, regression loss :   41.95895, sparse loss :   12.09924, time : 1279.28086.\n",
      "Epoch : 565, batch :  1, loss :   15.05966, regression loss :   41.94184, sparse loss :   12.07275, time : 1281.80360.\n",
      "Epoch : 566, batch :  1, loss :   15.03418, regression loss :   41.92480, sparse loss :   12.04634, time : 1284.17323.\n",
      "Epoch : 567, batch :  1, loss :   15.00880, regression loss :   41.90782, sparse loss :   12.02002, time : 1286.29521.\n",
      "Epoch : 568, batch :  1, loss :   14.98350, regression loss :   41.89091, sparse loss :   11.99379, time : 1288.34852.\n",
      "Epoch : 569, batch :  1, loss :   14.95830, regression loss :   41.87407, sparse loss :   11.96765, time : 1290.33406.\n",
      "Epoch : 570, batch :  1, loss :   14.93318, regression loss :   41.85729, sparse loss :   11.94161, time : 1292.84353.\n",
      "Epoch : 571, batch :  1, loss :   14.90814, regression loss :   41.84059, sparse loss :   11.91565, time : 1294.84686.\n",
      "Epoch : 572, batch :  1, loss :   14.88319, regression loss :   41.82394, sparse loss :   11.88978, time : 1296.83429.\n",
      "Epoch : 573, batch :  1, loss :   14.85833, regression loss :   41.80737, sparse loss :   11.86399, time : 1298.82476.\n",
      "Epoch : 574, batch :  1, loss :   14.83355, regression loss :   41.79086, sparse loss :   11.83830, time : 1300.87554.\n",
      "Epoch : 575, batch :  1, loss :   14.80886, regression loss :   41.77441, sparse loss :   11.81269, time : 1302.91017.\n",
      "Epoch : 576, batch :  1, loss :   14.78426, regression loss :   41.75803, sparse loss :   11.78717, time : 1304.96209.\n",
      "Epoch : 577, batch :  1, loss :   14.75974, regression loss :   41.74171, sparse loss :   11.76174, time : 1306.89201.\n",
      "Epoch : 578, batch :  1, loss :   14.73530, regression loss :   41.72546, sparse loss :   11.73639, time : 1308.86365.\n",
      "Epoch : 579, batch :  1, loss :   14.71095, regression loss :   41.70928, sparse loss :   11.71113, time : 1311.07350.\n",
      "Epoch : 580, batch :  1, loss :   14.68668, regression loss :   41.69315, sparse loss :   11.68596, time : 1313.05726.\n",
      "Epoch : 581, batch :  1, loss :   14.66250, regression loss :   41.67709, sparse loss :   11.66087, time : 1315.12446.\n",
      "Epoch : 582, batch :  1, loss :   14.63839, regression loss :   41.66110, sparse loss :   11.63587, time : 1317.18797.\n",
      "Epoch : 583, batch :  1, loss :   14.61438, regression loss :   41.64516, sparse loss :   11.61096, time : 1319.24052.\n",
      "Epoch : 584, batch :  1, loss :   14.59044, regression loss :   41.62929, sparse loss :   11.58613, time : 1321.24326.\n",
      "Epoch : 585, batch :  1, loss :   14.56659, regression loss :   41.61348, sparse loss :   11.56138, time : 1323.27265.\n",
      "Epoch : 586, batch :  1, loss :   14.54282, regression loss :   41.59774, sparse loss :   11.53672, time : 1325.31002.\n",
      "Epoch : 587, batch :  1, loss :   14.51913, regression loss :   41.58205, sparse loss :   11.51214, time : 1327.35493.\n",
      "Epoch : 588, batch :  1, loss :   14.49552, regression loss :   41.56643, sparse loss :   11.48765, time : 1329.26798.\n",
      "Epoch : 589, batch :  1, loss :   14.47200, regression loss :   41.55086, sparse loss :   11.46323, time : 1331.43424.\n",
      "Epoch : 590, batch :  1, loss :   14.44855, regression loss :   41.53536, sparse loss :   11.43891, time : 1333.41005.\n",
      "Epoch : 591, batch :  1, loss :   14.42519, regression loss :   41.51992, sparse loss :   11.41466, time : 1335.54936.\n",
      "Epoch : 592, batch :  1, loss :   14.40190, regression loss :   41.50454, sparse loss :   11.39050, time : 1337.62204.\n",
      "Epoch : 593, batch :  1, loss :   14.37870, regression loss :   41.48922, sparse loss :   11.36642, time : 1339.62780.\n",
      "Epoch : 594, batch :  1, loss :   14.35558, regression loss :   41.47396, sparse loss :   11.34242, time : 1341.54987.\n",
      "Epoch : 595, batch :  1, loss :   14.33253, regression loss :   41.45876, sparse loss :   11.31851, time : 1343.52625.\n",
      "Epoch : 596, batch :  1, loss :   14.30957, regression loss :   41.44361, sparse loss :   11.29467, time : 1345.59428.\n",
      "Epoch : 597, batch :  1, loss :   14.28668, regression loss :   41.42853, sparse loss :   11.27092, time : 1347.57196.\n",
      "Epoch : 598, batch :  1, loss :   14.26388, regression loss :   41.41350, sparse loss :   11.24725, time : 1349.64782.\n",
      "Epoch : 599, batch :  1, loss :   14.24115, regression loss :   41.39854, sparse loss :   11.22366, time : 1351.52342.\n",
      "Epoch : 600, batch :  1, loss :   14.21850, regression loss :   41.38363, sparse loss :   11.20015, time : 1355.95522.\n",
      "Epoch : 601, batch :  1, loss :   14.19592, regression loss :   41.36878, sparse loss :   11.17672, time : 1357.49417.\n",
      "Epoch : 602, batch :  1, loss :   14.17343, regression loss :   41.35398, sparse loss :   11.15337, time : 1359.08820.\n",
      "Epoch : 603, batch :  1, loss :   14.15101, regression loss :   41.33924, sparse loss :   11.13010, time : 1360.71746.\n",
      "Epoch : 604, batch :  1, loss :   14.12867, regression loss :   41.32457, sparse loss :   11.10691, time : 1362.36848.\n",
      "Epoch : 605, batch :  1, loss :   14.10641, regression loss :   41.30994, sparse loss :   11.08379, time : 1364.09526.\n",
      "Epoch : 606, batch :  1, loss :   14.08422, regression loss :   41.29538, sparse loss :   11.06076, time : 1365.74991.\n",
      "Epoch : 607, batch :  1, loss :   14.06211, regression loss :   41.28086, sparse loss :   11.03781, time : 1367.44553.\n",
      "Epoch : 608, batch :  1, loss :   14.04008, regression loss :   41.26641, sparse loss :   11.01493, time : 1369.09158.\n",
      "Epoch : 609, batch :  1, loss :   14.01812, regression loss :   41.25201, sparse loss :   10.99213, time : 1370.75099.\n",
      "Epoch : 610, batch :  1, loss :   13.99623, regression loss :   41.23766, sparse loss :   10.96941, time : 1372.34523.\n",
      "Epoch : 611, batch :  1, loss :   13.97442, regression loss :   41.22337, sparse loss :   10.94676, time : 1374.01905.\n",
      "Epoch : 612, batch :  1, loss :   13.95269, regression loss :   41.20914, sparse loss :   10.92420, time : 1375.66099.\n",
      "Epoch : 613, batch :  1, loss :   13.93103, regression loss :   41.19496, sparse loss :   10.90171, time : 1377.47911.\n",
      "Epoch : 614, batch :  1, loss :   13.90945, regression loss :   41.18083, sparse loss :   10.87929, time : 1379.14388.\n",
      "Epoch : 615, batch :  1, loss :   13.88794, regression loss :   41.16676, sparse loss :   10.85696, time : 1380.83748.\n",
      "Epoch : 616, batch :  1, loss :   13.86650, regression loss :   41.15274, sparse loss :   10.83470, time : 1382.43584.\n",
      "Epoch : 617, batch :  1, loss :   13.84514, regression loss :   41.13877, sparse loss :   10.81251, time : 1384.11720.\n",
      "Epoch : 618, batch :  1, loss :   13.82385, regression loss :   41.12486, sparse loss :   10.79040, time : 1385.99671.\n",
      "Epoch : 619, batch :  1, loss :   13.80263, regression loss :   41.11100, sparse loss :   10.76837, time : 1387.63424.\n",
      "Epoch : 620, batch :  1, loss :   13.78149, regression loss :   41.09719, sparse loss :   10.74641, time : 1389.22448.\n",
      "Epoch : 621, batch :  1, loss :   13.76041, regression loss :   41.08344, sparse loss :   10.72452, time : 1390.76361.\n",
      "Epoch : 622, batch :  1, loss :   13.73941, regression loss :   41.06973, sparse loss :   10.70271, time : 1392.35239.\n",
      "Epoch : 623, batch :  1, loss :   13.71849, regression loss :   41.05608, sparse loss :   10.68098, time : 1394.17658.\n",
      "Epoch : 624, batch :  1, loss :   13.69763, regression loss :   41.04248, sparse loss :   10.65932, time : 1395.84564.\n",
      "Epoch : 625, batch :  1, loss :   13.67685, regression loss :   41.02893, sparse loss :   10.63773, time : 1397.44385.\n",
      "Epoch : 626, batch :  1, loss :   13.65613, regression loss :   41.01543, sparse loss :   10.61621, time : 1399.21898.\n",
      "Epoch : 627, batch :  1, loss :   13.63549, regression loss :   41.00198, sparse loss :   10.59477, time : 1400.83586.\n",
      "Epoch : 628, batch :  1, loss :   13.61492, regression loss :   40.98858, sparse loss :   10.57340, time : 1402.47262.\n",
      "Epoch : 629, batch :  1, loss :   13.59442, regression loss :   40.97524, sparse loss :   10.55211, time : 1404.13773.\n",
      "Epoch : 630, batch :  1, loss :   13.57399, regression loss :   40.96194, sparse loss :   10.53088, time : 1405.74571.\n",
      "Epoch : 631, batch :  1, loss :   13.55363, regression loss :   40.94869, sparse loss :   10.50973, time : 1407.39759.\n",
      "Epoch : 632, batch :  1, loss :   13.53334, regression loss :   40.93549, sparse loss :   10.48866, time : 1409.05624.\n",
      "Epoch : 633, batch :  1, loss :   13.51312, regression loss :   40.92234, sparse loss :   10.46765, time : 1410.62029.\n",
      "Epoch : 634, batch :  1, loss :   13.49296, regression loss :   40.90924, sparse loss :   10.44671, time : 1412.19903.\n",
      "Epoch : 635, batch :  1, loss :   13.47288, regression loss :   40.89618, sparse loss :   10.42585, time : 1413.82852.\n",
      "Epoch : 636, batch :  1, loss :   13.45287, regression loss :   40.88318, sparse loss :   10.40505, time : 1415.46689.\n",
      "Epoch : 637, batch :  1, loss :   13.43292, regression loss :   40.87022, sparse loss :   10.38433, time : 1417.01832.\n",
      "Epoch : 638, batch :  1, loss :   13.41304, regression loss :   40.85731, sparse loss :   10.36368, time : 1418.66383.\n",
      "Epoch : 639, batch :  1, loss :   13.39323, regression loss :   40.84445, sparse loss :   10.34310, time : 1420.28288.\n",
      "Epoch : 640, batch :  1, loss :   13.37349, regression loss :   40.83163, sparse loss :   10.32258, time : 1421.88949.\n",
      "Epoch : 641, batch :  1, loss :   13.35381, regression loss :   40.81887, sparse loss :   10.30214, time : 1423.49591.\n",
      "Epoch : 642, batch :  1, loss :   13.33421, regression loss :   40.80614, sparse loss :   10.28177, time : 1425.18543.\n",
      "Epoch : 643, batch :  1, loss :   13.31467, regression loss :   40.79347, sparse loss :   10.26147, time : 1426.85716.\n",
      "Epoch : 644, batch :  1, loss :   13.29519, regression loss :   40.78084, sparse loss :   10.24123, time : 1428.53367.\n",
      "Epoch : 645, batch :  1, loss :   13.27578, regression loss :   40.76825, sparse loss :   10.22106, time : 1430.20716.\n",
      "Epoch : 646, batch :  1, loss :   13.25644, regression loss :   40.75572, sparse loss :   10.20097, time : 1431.80788.\n",
      "Epoch : 647, batch :  1, loss :   13.23717, regression loss :   40.74322, sparse loss :   10.18094, time : 1433.46576.\n",
      "Epoch : 648, batch :  1, loss :   13.21796, regression loss :   40.73078, sparse loss :   10.16098, time : 1435.02753.\n",
      "Epoch : 649, batch :  1, loss :   13.19882, regression loss :   40.71837, sparse loss :   10.14109, time : 1436.89010.\n",
      "Epoch : 650, batch :  1, loss :   13.17974, regression loss :   40.70601, sparse loss :   10.12126, time : 1438.55434.\n",
      "Epoch : 651, batch :  1, loss :   13.16072, regression loss :   40.69370, sparse loss :   10.10150, time : 1440.10570.\n",
      "Epoch : 652, batch :  1, loss :   13.14178, regression loss :   40.68143, sparse loss :   10.08181, time : 1441.65222.\n",
      "Epoch : 653, batch :  1, loss :   13.12289, regression loss :   40.66920, sparse loss :   10.06219, time : 1443.42198.\n",
      "Epoch : 654, batch :  1, loss :   13.10407, regression loss :   40.65702, sparse loss :   10.04263, time : 1445.06296.\n",
      "Epoch : 655, batch :  1, loss :   13.08532, regression loss :   40.64488, sparse loss :   10.02314, time : 1446.74859.\n",
      "Epoch : 656, batch :  1, loss :   13.06662, regression loss :   40.63278, sparse loss :   10.00372, time : 1448.45828.\n",
      "Epoch : 657, batch :  1, loss :   13.04800, regression loss :   40.62073, sparse loss :    9.98436, time : 1450.11260.\n",
      "Epoch : 658, batch :  1, loss :   13.02943, regression loss :   40.60872, sparse loss :    9.96507, time : 1451.72074.\n",
      "Epoch : 659, batch :  1, loss :   13.01093, regression loss :   40.59675, sparse loss :    9.94584, time : 1453.51521.\n",
      "Epoch : 660, batch :  1, loss :   12.99250, regression loss :   40.58482, sparse loss :    9.92668, time : 1455.09779.\n",
      "Epoch : 661, batch :  1, loss :   12.97412, regression loss :   40.57294, sparse loss :    9.90759, time : 1456.82247.\n",
      "Epoch : 662, batch :  1, loss :   12.95581, regression loss :   40.56109, sparse loss :    9.88856, time : 1458.38201.\n",
      "Epoch : 663, batch :  1, loss :   12.93756, regression loss :   40.54929, sparse loss :    9.86959, time : 1459.92880.\n",
      "Epoch : 664, batch :  1, loss :   12.91937, regression loss :   40.53753, sparse loss :    9.85069, time : 1461.58139.\n",
      "Epoch : 665, batch :  1, loss :   12.90125, regression loss :   40.52581, sparse loss :    9.83185, time : 1463.25307.\n",
      "Epoch : 666, batch :  1, loss :   12.88318, regression loss :   40.51413, sparse loss :    9.81308, time : 1464.92126.\n",
      "Epoch : 667, batch :  1, loss :   12.86518, regression loss :   40.50249, sparse loss :    9.79436, time : 1466.72476.\n",
      "Epoch : 668, batch :  1, loss :   12.84724, regression loss :   40.49089, sparse loss :    9.77572, time : 1468.30984.\n",
      "Epoch : 669, batch :  1, loss :   12.82935, regression loss :   40.47933, sparse loss :    9.75714, time : 1470.08927.\n",
      "Epoch : 670, batch :  1, loss :   12.81153, regression loss :   40.46781, sparse loss :    9.73862, time : 1471.78515.\n",
      "Epoch : 671, batch :  1, loss :   12.79378, regression loss :   40.45633, sparse loss :    9.72016, time : 1473.56005.\n",
      "Epoch : 672, batch :  1, loss :   12.77608, regression loss :   40.44489, sparse loss :    9.70177, time : 1475.19975.\n",
      "Epoch : 673, batch :  1, loss :   12.75844, regression loss :   40.43348, sparse loss :    9.68343, time : 1476.80215.\n",
      "Epoch : 674, batch :  1, loss :   12.74086, regression loss :   40.42212, sparse loss :    9.66516, time : 1478.40561.\n",
      "Epoch : 675, batch :  1, loss :   12.72334, regression loss :   40.41079, sparse loss :    9.64696, time : 1479.94586.\n",
      "Epoch : 676, batch :  1, loss :   12.70588, regression loss :   40.39951, sparse loss :    9.62881, time : 1481.65058.\n",
      "Epoch : 677, batch :  1, loss :   12.68848, regression loss :   40.38826, sparse loss :    9.61073, time : 1483.48485.\n",
      "Epoch : 678, batch :  1, loss :   12.67114, regression loss :   40.37705, sparse loss :    9.59270, time : 1485.16693.\n",
      "Epoch : 679, batch :  1, loss :   12.65386, regression loss :   40.36587, sparse loss :    9.57474, time : 1486.80040.\n",
      "Epoch : 680, batch :  1, loss :   12.63663, regression loss :   40.35474, sparse loss :    9.55684, time : 1488.40459.\n",
      "Epoch : 681, batch :  1, loss :   12.61947, regression loss :   40.34364, sparse loss :    9.53900, time : 1489.95566.\n",
      "Epoch : 682, batch :  1, loss :   12.60236, regression loss :   40.33258, sparse loss :    9.52122, time : 1491.79454.\n",
      "Epoch : 683, batch :  1, loss :   12.58531, regression loss :   40.32155, sparse loss :    9.50351, time : 1493.45371.\n",
      "Epoch : 684, batch :  1, loss :   12.56832, regression loss :   40.31056, sparse loss :    9.48585, time : 1495.05672.\n",
      "Epoch : 685, batch :  1, loss :   12.55139, regression loss :   40.29961, sparse loss :    9.46825, time : 1496.91192.\n",
      "Epoch : 686, batch :  1, loss :   12.53451, regression loss :   40.28869, sparse loss :    9.45071, time : 1498.48970.\n",
      "Epoch : 687, batch :  1, loss :   12.51769, regression loss :   40.27782, sparse loss :    9.43323, time : 1500.27131.\n",
      "Epoch : 688, batch :  1, loss :   12.50093, regression loss :   40.26697, sparse loss :    9.41581, time : 1501.95616.\n",
      "Epoch : 689, batch :  1, loss :   12.48422, regression loss :   40.25616, sparse loss :    9.39845, time : 1503.56593.\n",
      "Epoch : 690, batch :  1, loss :   12.46758, regression loss :   40.24539, sparse loss :    9.38115, time : 1505.23195.\n",
      "Epoch : 691, batch :  1, loss :   12.45098, regression loss :   40.23465, sparse loss :    9.36391, time : 1507.04102.\n",
      "Epoch : 692, batch :  1, loss :   12.43445, regression loss :   40.22394, sparse loss :    9.34673, time : 1508.86460.\n",
      "Epoch : 693, batch :  1, loss :   12.41797, regression loss :   40.21328, sparse loss :    9.32960, time : 1510.55460.\n",
      "Epoch : 694, batch :  1, loss :   12.40154, regression loss :   40.20264, sparse loss :    9.31253, time : 1512.17732.\n",
      "Epoch : 695, batch :  1, loss :   12.38517, regression loss :   40.19204, sparse loss :    9.29552, time : 1513.77863.\n",
      "Epoch : 696, batch :  1, loss :   12.36886, regression loss :   40.18148, sparse loss :    9.27857, time : 1515.33859.\n",
      "Epoch : 697, batch :  1, loss :   12.35260, regression loss :   40.17095, sparse loss :    9.26167, time : 1517.11967.\n",
      "Epoch : 698, batch :  1, loss :   12.33640, regression loss :   40.16045, sparse loss :    9.24484, time : 1518.75580.\n",
      "Epoch : 699, batch :  1, loss :   12.32025, regression loss :   40.14998, sparse loss :    9.22806, time : 1520.33081.\n",
      "Epoch : 700, batch :  1, loss :   12.30416, regression loss :   40.13955, sparse loss :    9.21133, time : 1524.83558.\n",
      "Epoch : 701, batch :  1, loss :   12.28812, regression loss :   40.12916, sparse loss :    9.19467, time : 1526.83654.\n",
      "Epoch : 702, batch :  1, loss :   12.27213, regression loss :   40.11879, sparse loss :    9.17806, time : 1528.80572.\n",
      "Epoch : 703, batch :  1, loss :   12.25620, regression loss :   40.10846, sparse loss :    9.16150, time : 1530.84995.\n",
      "Epoch : 704, batch :  1, loss :   12.24032, regression loss :   40.09816, sparse loss :    9.14501, time : 1532.90617.\n",
      "Epoch : 705, batch :  1, loss :   12.22450, regression loss :   40.08790, sparse loss :    9.12856, time : 1534.98135.\n",
      "Epoch : 706, batch :  1, loss :   12.20873, regression loss :   40.07767, sparse loss :    9.11218, time : 1536.93279.\n",
      "Epoch : 707, batch :  1, loss :   12.19301, regression loss :   40.06747, sparse loss :    9.09585, time : 1538.97774.\n",
      "Epoch : 708, batch :  1, loss :   12.17735, regression loss :   40.05730, sparse loss :    9.07957, time : 1540.99546.\n",
      "Epoch : 709, batch :  1, loss :   12.16173, regression loss :   40.04716, sparse loss :    9.06335, time : 1542.96128.\n",
      "Epoch : 710, batch :  1, loss :   12.14617, regression loss :   40.03706, sparse loss :    9.04719, time : 1544.97335.\n",
      "Epoch : 711, batch :  1, loss :   12.13067, regression loss :   40.02699, sparse loss :    9.03108, time : 1546.92849.\n",
      "Epoch : 712, batch :  1, loss :   12.11522, regression loss :   40.01694, sparse loss :    9.01502, time : 1549.11563.\n",
      "Epoch : 713, batch :  1, loss :   12.09981, regression loss :   40.00693, sparse loss :    8.99902, time : 1551.18560.\n",
      "Epoch : 714, batch :  1, loss :   12.08446, regression loss :   39.99696, sparse loss :    8.98308, time : 1553.42842.\n",
      "Epoch : 715, batch :  1, loss :   12.06916, regression loss :   39.98701, sparse loss :    8.96718, time : 1555.49465.\n",
      "Epoch : 716, batch :  1, loss :   12.05392, regression loss :   39.97709, sparse loss :    8.95134, time : 1557.70928.\n",
      "Epoch : 717, batch :  1, loss :   12.03872, regression loss :   39.96721, sparse loss :    8.93556, time : 1559.82381.\n",
      "Epoch : 718, batch :  1, loss :   12.02358, regression loss :   39.95736, sparse loss :    8.91983, time : 1562.04483.\n",
      "Epoch : 719, batch :  1, loss :   12.00849, regression loss :   39.94753, sparse loss :    8.90415, time : 1564.11461.\n",
      "Epoch : 720, batch :  1, loss :   11.99344, regression loss :   39.93774, sparse loss :    8.88852, time : 1566.19184.\n",
      "Epoch : 721, batch :  1, loss :   11.97845, regression loss :   39.92798, sparse loss :    8.87295, time : 1568.18942.\n",
      "Epoch : 722, batch :  1, loss :   11.96351, regression loss :   39.91825, sparse loss :    8.85743, time : 1570.11469.\n",
      "Epoch : 723, batch :  1, loss :   11.94862, regression loss :   39.90854, sparse loss :    8.84196, time : 1572.09899.\n",
      "Epoch : 724, batch :  1, loss :   11.93378, regression loss :   39.89887, sparse loss :    8.82655, time : 1574.12608.\n",
      "Epoch : 725, batch :  1, loss :   11.91899, regression loss :   39.88923, sparse loss :    8.81118, time : 1576.07436.\n",
      "Epoch : 726, batch :  1, loss :   11.90425, regression loss :   39.87962, sparse loss :    8.79587, time : 1578.11917.\n",
      "Epoch : 727, batch :  1, loss :   11.88956, regression loss :   39.87004, sparse loss :    8.78061, time : 1580.36065.\n",
      "Epoch : 728, batch :  1, loss :   11.87491, regression loss :   39.86049, sparse loss :    8.76540, time : 1582.33295.\n",
      "Epoch : 729, batch :  1, loss :   11.86032, regression loss :   39.85097, sparse loss :    8.75025, time : 1584.39984.\n",
      "Epoch : 730, batch :  1, loss :   11.84578, regression loss :   39.84147, sparse loss :    8.73514, time : 1586.41868.\n",
      "Epoch : 731, batch :  1, loss :   11.83128, regression loss :   39.83201, sparse loss :    8.72009, time : 1588.35114.\n",
      "Epoch : 732, batch :  1, loss :   11.81684, regression loss :   39.82258, sparse loss :    8.70509, time : 1590.48628.\n",
      "Epoch : 733, batch :  1, loss :   11.80244, regression loss :   39.81317, sparse loss :    8.69013, time : 1592.58720.\n",
      "Epoch : 734, batch :  1, loss :   11.78809, regression loss :   39.80380, sparse loss :    8.67523, time : 1594.72782.\n",
      "Epoch : 735, batch :  1, loss :   11.77379, regression loss :   39.79445, sparse loss :    8.66038, time : 1596.75167.\n",
      "Epoch : 736, batch :  1, loss :   11.75953, regression loss :   39.78514, sparse loss :    8.64558, time : 1598.83051.\n",
      "Epoch : 737, batch :  1, loss :   11.74533, regression loss :   39.77585, sparse loss :    8.63083, time : 1600.84632.\n",
      "Epoch : 738, batch :  1, loss :   11.73117, regression loss :   39.76659, sparse loss :    8.61612, time : 1603.04276.\n",
      "Epoch : 739, batch :  1, loss :   11.71706, regression loss :   39.75736, sparse loss :    8.60147, time : 1605.14519.\n",
      "Epoch : 740, batch :  1, loss :   11.70300, regression loss :   39.74816, sparse loss :    8.58687, time : 1607.30221.\n",
      "Epoch : 741, batch :  1, loss :   11.68898, regression loss :   39.73898, sparse loss :    8.57231, time : 1609.42335.\n",
      "Epoch : 742, batch :  1, loss :   11.67501, regression loss :   39.72984, sparse loss :    8.55781, time : 1611.52290.\n",
      "Epoch : 743, batch :  1, loss :   11.66109, regression loss :   39.72072, sparse loss :    8.54335, time : 1613.50722.\n",
      "Epoch : 744, batch :  1, loss :   11.64721, regression loss :   39.71164, sparse loss :    8.52894, time : 1615.61280.\n",
      "Epoch : 745, batch :  1, loss :   11.63338, regression loss :   39.70258, sparse loss :    8.51459, time : 1617.73502.\n",
      "Epoch : 746, batch :  1, loss :   11.61960, regression loss :   39.69355, sparse loss :    8.50027, time : 1619.71643.\n",
      "Epoch : 747, batch :  1, loss :   11.60587, regression loss :   39.68455, sparse loss :    8.48601, time : 1621.96151.\n",
      "Epoch : 748, batch :  1, loss :   11.59218, regression loss :   39.67557, sparse loss :    8.47180, time : 1623.98085.\n",
      "Epoch : 749, batch :  1, loss :   11.57853, regression loss :   39.66663, sparse loss :    8.45763, time : 1625.98014.\n",
      "Epoch : 750, batch :  1, loss :   11.56493, regression loss :   39.65771, sparse loss :    8.44351, time : 1627.98897.\n",
      "Epoch : 751, batch :  1, loss :   11.55138, regression loss :   39.64882, sparse loss :    8.42944, time : 1630.24391.\n",
      "Epoch : 752, batch :  1, loss :   11.53787, regression loss :   39.63996, sparse loss :    8.41542, time : 1632.31329.\n",
      "Epoch : 753, batch :  1, loss :   11.52441, regression loss :   39.63113, sparse loss :    8.40144, time : 1634.43922.\n",
      "Epoch : 754, batch :  1, loss :   11.51099, regression loss :   39.62232, sparse loss :    8.38751, time : 1636.48163.\n",
      "Epoch : 755, batch :  1, loss :   11.49762, regression loss :   39.61354, sparse loss :    8.37362, time : 1638.53766.\n",
      "Epoch : 756, batch :  1, loss :   11.48429, regression loss :   39.60479, sparse loss :    8.35979, time : 1640.61951.\n",
      "Epoch : 757, batch :  1, loss :   11.47100, regression loss :   39.59607, sparse loss :    8.34600, time : 1642.71811.\n",
      "Epoch : 758, batch :  1, loss :   11.45777, regression loss :   39.58738, sparse loss :    8.33225, time : 1644.73216.\n",
      "Epoch : 759, batch :  1, loss :   11.44457, regression loss :   39.57871, sparse loss :    8.31855, time : 1646.77800.\n",
      "Epoch : 760, batch :  1, loss :   11.43142, regression loss :   39.57007, sparse loss :    8.30490, time : 1648.75674.\n",
      "Epoch : 761, batch :  1, loss :   11.41831, regression loss :   39.56147, sparse loss :    8.29130, time : 1650.75199.\n",
      "Epoch : 762, batch :  1, loss :   11.40525, regression loss :   39.55288, sparse loss :    8.27773, time : 1652.79456.\n",
      "Epoch : 763, batch :  1, loss :   11.39223, regression loss :   39.54433, sparse loss :    8.26422, time : 1655.13687.\n",
      "Epoch : 764, batch :  1, loss :   11.37925, regression loss :   39.53580, sparse loss :    8.25075, time : 1657.23316.\n",
      "Epoch : 765, batch :  1, loss :   11.36632, regression loss :   39.52730, sparse loss :    8.23732, time : 1659.23245.\n",
      "Epoch : 766, batch :  1, loss :   11.35343, regression loss :   39.51883, sparse loss :    8.22394, time : 1661.41555.\n",
      "Epoch : 767, batch :  1, loss :   11.34059, regression loss :   39.51039, sparse loss :    8.21061, time : 1663.50155.\n",
      "Epoch : 768, batch :  1, loss :   11.32778, regression loss :   39.50197, sparse loss :    8.19732, time : 1665.51823.\n",
      "Epoch : 769, batch :  1, loss :   11.31502, regression loss :   39.49358, sparse loss :    8.18407, time : 1667.51317.\n",
      "Epoch : 770, batch :  1, loss :   11.30230, regression loss :   39.48522, sparse loss :    8.17087, time : 1669.61284.\n",
      "Epoch : 771, batch :  1, loss :   11.28963, regression loss :   39.47688, sparse loss :    8.15771, time : 1671.77513.\n",
      "Epoch : 772, batch :  1, loss :   11.27699, regression loss :   39.46858, sparse loss :    8.14460, time : 1673.82279.\n",
      "Epoch : 773, batch :  1, loss :   11.26440, regression loss :   39.46030, sparse loss :    8.13153, time : 1675.81763.\n",
      "Epoch : 774, batch :  1, loss :   11.25185, regression loss :   39.45205, sparse loss :    8.11850, time : 1677.78598.\n",
      "Epoch : 775, batch :  1, loss :   11.23935, regression loss :   39.44382, sparse loss :    8.10551, time : 1679.79790.\n",
      "Epoch : 776, batch :  1, loss :   11.22688, regression loss :   39.43563, sparse loss :    8.09257, time : 1682.05176.\n",
      "Epoch : 777, batch :  1, loss :   11.21445, regression loss :   39.42746, sparse loss :    8.07968, time : 1684.16012.\n",
      "Epoch : 778, batch :  1, loss :   11.20207, regression loss :   39.41932, sparse loss :    8.06682, time : 1686.16539.\n",
      "Epoch : 779, batch :  1, loss :   11.18973, regression loss :   39.41120, sparse loss :    8.05401, time : 1688.16061.\n",
      "Epoch : 780, batch :  1, loss :   11.17743, regression loss :   39.40311, sparse loss :    8.04124, time : 1690.16253.\n",
      "Epoch : 781, batch :  1, loss :   11.16517, regression loss :   39.39506, sparse loss :    8.02851, time : 1692.13001.\n",
      "Epoch : 782, batch :  1, loss :   11.15295, regression loss :   39.38702, sparse loss :    8.01583, time : 1694.10179.\n",
      "Epoch : 783, batch :  1, loss :   11.14077, regression loss :   39.37902, sparse loss :    8.00319, time : 1696.30014.\n",
      "Epoch : 784, batch :  1, loss :   11.12863, regression loss :   39.37104, sparse loss :    7.99059, time : 1698.30890.\n",
      "Epoch : 785, batch :  1, loss :   11.11654, regression loss :   39.36309, sparse loss :    7.97803, time : 1700.35024.\n",
      "Epoch : 786, batch :  1, loss :   11.10448, regression loss :   39.35517, sparse loss :    7.96551, time : 1702.41783.\n",
      "Epoch : 787, batch :  1, loss :   11.09246, regression loss :   39.34727, sparse loss :    7.95304, time : 1704.48459.\n",
      "Epoch : 788, batch :  1, loss :   11.08049, regression loss :   39.33941, sparse loss :    7.94061, time : 1706.54555.\n",
      "Epoch : 789, batch :  1, loss :   11.06855, regression loss :   39.33157, sparse loss :    7.92821, time : 1708.59420.\n",
      "Epoch : 790, batch :  1, loss :   11.05665, regression loss :   39.32375, sparse loss :    7.91586, time : 1710.66904.\n",
      "Epoch : 791, batch :  1, loss :   11.04479, regression loss :   39.31597, sparse loss :    7.90355, time : 1712.75465.\n",
      "Epoch : 792, batch :  1, loss :   11.03298, regression loss :   39.30821, sparse loss :    7.89128, time : 1714.85148.\n",
      "Epoch : 793, batch :  1, loss :   11.02120, regression loss :   39.30048, sparse loss :    7.87905, time : 1716.90149.\n",
      "Epoch : 794, batch :  1, loss :   11.00946, regression loss :   39.29277, sparse loss :    7.86686, time : 1718.84054.\n",
      "Epoch : 795, batch :  1, loss :   10.99775, regression loss :   39.28510, sparse loss :    7.85472, time : 1720.85254.\n",
      "Epoch : 796, batch :  1, loss :   10.98609, regression loss :   39.27745, sparse loss :    7.84261, time : 1722.81196.\n",
      "Epoch : 797, batch :  1, loss :   10.97447, regression loss :   39.26983, sparse loss :    7.83054, time : 1725.07107.\n",
      "Epoch : 798, batch :  1, loss :   10.96288, regression loss :   39.26223, sparse loss :    7.81851, time : 1727.23037.\n",
      "Epoch : 799, batch :  1, loss :   10.95134, regression loss :   39.25466, sparse loss :    7.80652, time : 1729.41592.\n",
      "Epoch : 800, batch :  1, loss :   10.93983, regression loss :   39.24712, sparse loss :    7.79457, time : 1734.28560.\n",
      "Epoch : 801, batch :  1, loss :   10.92836, regression loss :   39.23961, sparse loss :    7.78266, time : 1735.97695.\n",
      "Epoch : 802, batch :  1, loss :   10.91693, regression loss :   39.23213, sparse loss :    7.77080, time : 1737.97284.\n",
      "Epoch : 803, batch :  1, loss :   10.90553, regression loss :   39.22467, sparse loss :    7.75896, time : 1739.64412.\n",
      "Epoch : 804, batch :  1, loss :   10.89418, regression loss :   39.21724, sparse loss :    7.74717, time : 1741.35263.\n",
      "Epoch : 805, batch :  1, loss :   10.88286, regression loss :   39.20983, sparse loss :    7.73542, time : 1742.91788.\n",
      "Epoch : 806, batch :  1, loss :   10.87158, regression loss :   39.20246, sparse loss :    7.72370, time : 1744.49669.\n",
      "Epoch : 807, batch :  1, loss :   10.86034, regression loss :   39.19511, sparse loss :    7.71203, time : 1746.17058.\n",
      "Epoch : 808, batch :  1, loss :   10.84913, regression loss :   39.18778, sparse loss :    7.70039, time : 1747.86841.\n",
      "Epoch : 809, batch :  1, loss :   10.83796, regression loss :   39.18049, sparse loss :    7.68879, time : 1749.50570.\n",
      "Epoch : 810, batch :  1, loss :   10.82683, regression loss :   39.17322, sparse loss :    7.67723, time : 1751.21620.\n",
      "Epoch : 811, batch :  1, loss :   10.81573, regression loss :   39.16598, sparse loss :    7.66571, time : 1752.81622.\n",
      "Epoch : 812, batch :  1, loss :   10.80467, regression loss :   39.15877, sparse loss :    7.65422, time : 1754.64092.\n",
      "Epoch : 813, batch :  1, loss :   10.79365, regression loss :   39.15158, sparse loss :    7.64277, time : 1756.25898.\n",
      "Epoch : 814, batch :  1, loss :   10.78267, regression loss :   39.14442, sparse loss :    7.63136, time : 1757.81164.\n",
      "Epoch : 815, batch :  1, loss :   10.77172, regression loss :   39.13729, sparse loss :    7.61999, time : 1759.61236.\n",
      "Epoch : 816, batch :  1, loss :   10.76081, regression loss :   39.13018, sparse loss :    7.60865, time : 1761.34937.\n",
      "Epoch : 817, batch :  1, loss :   10.74993, regression loss :   39.12311, sparse loss :    7.59736, time : 1763.00585.\n",
      "Epoch : 818, batch :  1, loss :   10.73909, regression loss :   39.11605, sparse loss :    7.58609, time : 1764.84335.\n",
      "Epoch : 819, batch :  1, loss :   10.72829, regression loss :   39.10903, sparse loss :    7.57487, time : 1766.57172.\n",
      "Epoch : 820, batch :  1, loss :   10.71752, regression loss :   39.10204, sparse loss :    7.56368, time : 1768.31012.\n",
      "Epoch : 821, batch :  1, loss :   10.70678, regression loss :   39.09507, sparse loss :    7.55253, time : 1769.99219.\n",
      "Epoch : 822, batch :  1, loss :   10.69609, regression loss :   39.08812, sparse loss :    7.54142, time : 1771.68126.\n",
      "Epoch : 823, batch :  1, loss :   10.68542, regression loss :   39.08121, sparse loss :    7.53034, time : 1773.38640.\n",
      "Epoch : 824, batch :  1, loss :   10.67480, regression loss :   39.07432, sparse loss :    7.51930, time : 1775.08871.\n",
      "Epoch : 825, batch :  1, loss :   10.66421, regression loss :   39.06746, sparse loss :    7.50829, time : 1776.77333.\n",
      "Epoch : 826, batch :  1, loss :   10.65365, regression loss :   39.06062, sparse loss :    7.49732, time : 1778.38726.\n",
      "Epoch : 827, batch :  1, loss :   10.64313, regression loss :   39.05382, sparse loss :    7.48638, time : 1779.95730.\n",
      "Epoch : 828, batch :  1, loss :   10.63264, regression loss :   39.04704, sparse loss :    7.47548, time : 1781.55637.\n",
      "Epoch : 829, batch :  1, loss :   10.62219, regression loss :   39.04028, sparse loss :    7.46462, time : 1783.19616.\n",
      "Epoch : 830, batch :  1, loss :   10.61177, regression loss :   39.03356, sparse loss :    7.45379, time : 1784.80618.\n",
      "Epoch : 831, batch :  1, loss :   10.60139, regression loss :   39.02686, sparse loss :    7.44300, time : 1786.39957.\n",
      "Epoch : 832, batch :  1, loss :   10.59104, regression loss :   39.02018, sparse loss :    7.43224, time : 1788.01675.\n",
      "Epoch : 833, batch :  1, loss :   10.58072, regression loss :   39.01354, sparse loss :    7.42152, time : 1789.68258.\n",
      "Epoch : 834, batch :  1, loss :   10.57044, regression loss :   39.00692, sparse loss :    7.41083, time : 1791.25890.\n",
      "Epoch : 835, batch :  1, loss :   10.56019, regression loss :   39.00033, sparse loss :    7.40018, time : 1792.86418.\n",
      "Epoch : 836, batch :  1, loss :   10.54998, regression loss :   38.99376, sparse loss :    7.38956, time : 1794.54430.\n",
      "Epoch : 837, batch :  1, loss :   10.53980, regression loss :   38.98722, sparse loss :    7.37898, time : 1796.38769.\n",
      "Epoch : 838, batch :  1, loss :   10.52966, regression loss :   38.98071, sparse loss :    7.36843, time : 1798.04986.\n",
      "Epoch : 839, batch :  1, loss :   10.51954, regression loss :   38.97422, sparse loss :    7.35791, time : 1799.66584.\n",
      "Epoch : 840, batch :  1, loss :   10.50947, regression loss :   38.96777, sparse loss :    7.34743, time : 1801.32694.\n",
      "Epoch : 841, batch :  1, loss :   10.49942, regression loss :   38.96134, sparse loss :    7.33699, time : 1802.99702.\n",
      "Epoch : 842, batch :  1, loss :   10.48941, regression loss :   38.95493, sparse loss :    7.32657, time : 1804.81708.\n",
      "Epoch : 843, batch :  1, loss :   10.47943, regression loss :   38.94855, sparse loss :    7.31619, time : 1806.36613.\n",
      "Epoch : 844, batch :  1, loss :   10.46948, regression loss :   38.94220, sparse loss :    7.30585, time : 1807.94624.\n",
      "Epoch : 845, batch :  1, loss :   10.45957, regression loss :   38.93587, sparse loss :    7.29554, time : 1809.55274.\n",
      "Epoch : 846, batch :  1, loss :   10.44969, regression loss :   38.92957, sparse loss :    7.28526, time : 1811.16741.\n",
      "Epoch : 847, batch :  1, loss :   10.43984, regression loss :   38.92330, sparse loss :    7.27501, time : 1812.97532.\n",
      "Epoch : 848, batch :  1, loss :   10.43002, regression loss :   38.91705, sparse loss :    7.26480, time : 1814.65424.\n",
      "Epoch : 849, batch :  1, loss :   10.42024, regression loss :   38.91083, sparse loss :    7.25462, time : 1816.23519.\n",
      "Epoch : 850, batch :  1, loss :   10.41049, regression loss :   38.90464, sparse loss :    7.24447, time : 1817.89960.\n",
      "Epoch : 851, batch :  1, loss :   10.40077, regression loss :   38.89847, sparse loss :    7.23436, time : 1819.50540.\n",
      "Epoch : 852, batch :  1, loss :   10.39108, regression loss :   38.89233, sparse loss :    7.22428, time : 1821.14765.\n",
      "Epoch : 853, batch :  1, loss :   10.38143, regression loss :   38.88622, sparse loss :    7.21423, time : 1822.75252.\n",
      "Epoch : 854, batch :  1, loss :   10.37180, regression loss :   38.88013, sparse loss :    7.20421, time : 1824.32194.\n",
      "Epoch : 855, batch :  1, loss :   10.36221, regression loss :   38.87407, sparse loss :    7.19423, time : 1825.99102.\n",
      "Epoch : 856, batch :  1, loss :   10.35265, regression loss :   38.86803, sparse loss :    7.18428, time : 1827.55974.\n",
      "Epoch : 857, batch :  1, loss :   10.34312, regression loss :   38.86202, sparse loss :    7.17436, time : 1829.30201.\n",
      "Epoch : 858, batch :  1, loss :   10.33363, regression loss :   38.85604, sparse loss :    7.16447, time : 1831.04297.\n",
      "Epoch : 859, batch :  1, loss :   10.32416, regression loss :   38.85008, sparse loss :    7.15461, time : 1832.75652.\n",
      "Epoch : 860, batch :  1, loss :   10.31472, regression loss :   38.84415, sparse loss :    7.14479, time : 1834.51279.\n",
      "Epoch : 861, batch :  1, loss :   10.30532, regression loss :   38.83824, sparse loss :    7.13500, time : 1836.21597.\n",
      "Epoch : 862, batch :  1, loss :   10.29595, regression loss :   38.83236, sparse loss :    7.12524, time : 1837.96013.\n",
      "Epoch : 863, batch :  1, loss :   10.28661, regression loss :   38.82651, sparse loss :    7.11551, time : 1839.60138.\n",
      "Epoch : 864, batch :  1, loss :   10.27730, regression loss :   38.82068, sparse loss :    7.10581, time : 1841.19469.\n",
      "Epoch : 865, batch :  1, loss :   10.26801, regression loss :   38.81487, sparse loss :    7.09614, time : 1842.81126.\n",
      "Epoch : 866, batch :  1, loss :   10.25877, regression loss :   38.80910, sparse loss :    7.08651, time : 1844.45507.\n",
      "Epoch : 867, batch :  1, loss :   10.24954, regression loss :   38.80335, sparse loss :    7.07690, time : 1846.25937.\n",
      "Epoch : 868, batch :  1, loss :   10.24036, regression loss :   38.79762, sparse loss :    7.06733, time : 1848.27838.\n",
      "Epoch : 869, batch :  1, loss :   10.23120, regression loss :   38.79192, sparse loss :    7.05778, time : 1849.99881.\n",
      "Epoch : 870, batch :  1, loss :   10.22207, regression loss :   38.78625, sparse loss :    7.04827, time : 1851.70964.\n",
      "Epoch : 871, batch :  1, loss :   10.21297, regression loss :   38.78060, sparse loss :    7.03879, time : 1853.45336.\n",
      "Epoch : 872, batch :  1, loss :   10.20390, regression loss :   38.77497, sparse loss :    7.02934, time : 1855.04504.\n",
      "Epoch : 873, batch :  1, loss :   10.19486, regression loss :   38.76938, sparse loss :    7.01991, time : 1856.68272.\n",
      "Epoch : 874, batch :  1, loss :   10.18585, regression loss :   38.76380, sparse loss :    7.01052, time : 1858.26390.\n",
      "Epoch : 875, batch :  1, loss :   10.17687, regression loss :   38.75825, sparse loss :    7.00116, time : 1860.07673.\n",
      "Epoch : 876, batch :  1, loss :   10.16792, regression loss :   38.75273, sparse loss :    6.99183, time : 1861.88178.\n",
      "Epoch : 877, batch :  1, loss :   10.15900, regression loss :   38.74723, sparse loss :    6.98253, time : 1863.57021.\n",
      "Epoch : 878, batch :  1, loss :   10.15011, regression loss :   38.74176, sparse loss :    6.97326, time : 1865.26270.\n",
      "Epoch : 879, batch :  1, loss :   10.14124, regression loss :   38.73631, sparse loss :    6.96401, time : 1866.84453.\n",
      "Epoch : 880, batch :  1, loss :   10.13241, regression loss :   38.73089, sparse loss :    6.95480, time : 1868.46128.\n",
      "Epoch : 881, batch :  1, loss :   10.12361, regression loss :   38.72549, sparse loss :    6.94562, time : 1870.00992.\n",
      "Epoch : 882, batch :  1, loss :   10.11483, regression loss :   38.72012, sparse loss :    6.93646, time : 1871.66787.\n",
      "Epoch : 883, batch :  1, loss :   10.10608, regression loss :   38.71477, sparse loss :    6.92734, time : 1873.23053.\n",
      "Epoch : 884, batch :  1, loss :   10.09737, regression loss :   38.70945, sparse loss :    6.91824, time : 1874.90377.\n",
      "Epoch : 885, batch :  1, loss :   10.08868, regression loss :   38.70415, sparse loss :    6.90918, time : 1876.66793.\n",
      "Epoch : 886, batch :  1, loss :   10.08001, regression loss :   38.69888, sparse loss :    6.90014, time : 1878.36272.\n",
      "Epoch : 887, batch :  1, loss :   10.07138, regression loss :   38.69363, sparse loss :    6.89113, time : 1880.06299.\n",
      "Epoch : 888, batch :  1, loss :   10.06278, regression loss :   38.68840, sparse loss :    6.88215, time : 1881.68653.\n",
      "Epoch : 889, batch :  1, loss :   10.05420, regression loss :   38.68320, sparse loss :    6.87320, time : 1883.22043.\n",
      "Epoch : 890, batch :  1, loss :   10.04565, regression loss :   38.67802, sparse loss :    6.86428, time : 1884.81637.\n",
      "Epoch : 891, batch :  1, loss :   10.03713, regression loss :   38.67287, sparse loss :    6.85539, time : 1886.37630.\n",
      "Epoch : 892, batch :  1, loss :   10.02864, regression loss :   38.66774, sparse loss :    6.84652, time : 1887.97056.\n",
      "Epoch : 893, batch :  1, loss :   10.02018, regression loss :   38.66264, sparse loss :    6.83768, time : 1889.62477.\n",
      "Epoch : 894, batch :  1, loss :   10.01174, regression loss :   38.65756, sparse loss :    6.82887, time : 1891.22089.\n",
      "Epoch : 895, batch :  1, loss :   10.00333, regression loss :   38.65250, sparse loss :    6.82009, time : 1892.86741.\n",
      "Epoch : 896, batch :  1, loss :    9.99495, regression loss :   38.64747, sparse loss :    6.81134, time : 1894.59478.\n",
      "Epoch : 897, batch :  1, loss :    9.98660, regression loss :   38.64246, sparse loss :    6.80261, time : 1896.36928.\n",
      "Epoch : 898, batch :  1, loss :    9.97827, regression loss :   38.63747, sparse loss :    6.79392, time : 1898.03750.\n",
      "Epoch : 899, batch :  1, loss :    9.96997, regression loss :   38.63251, sparse loss :    6.78525, time : 1899.71484.\n",
      "Epoch : 900, batch :  1, loss :    9.96170, regression loss :   38.62757, sparse loss :    6.77660, time : 1904.59832.\n",
      "Epoch : 901, batch :  1, loss :    9.95346, regression loss :   38.62266, sparse loss :    6.76799, time : 1906.96374.\n",
      "Epoch : 902, batch :  1, loss :    9.94524, regression loss :   38.61777, sparse loss :    6.75940, time : 1909.06816.\n",
      "Epoch : 903, batch :  1, loss :    9.93705, regression loss :   38.61290, sparse loss :    6.75084, time : 1911.19530.\n",
      "Epoch : 904, batch :  1, loss :    9.92888, regression loss :   38.60805, sparse loss :    6.74231, time : 1913.21043.\n",
      "Epoch : 905, batch :  1, loss :    9.92075, regression loss :   38.60323, sparse loss :    6.73380, time : 1915.48418.\n",
      "Epoch : 906, batch :  1, loss :    9.91264, regression loss :   38.59843, sparse loss :    6.72532, time : 1917.60898.\n",
      "Epoch : 907, batch :  1, loss :    9.90455, regression loss :   38.59366, sparse loss :    6.71687, time : 1919.95419.\n",
      "Epoch : 908, batch :  1, loss :    9.89649, regression loss :   38.58890, sparse loss :    6.70845, time : 1922.22700.\n",
      "Epoch : 909, batch :  1, loss :    9.88846, regression loss :   38.58417, sparse loss :    6.70005, time : 1924.34255.\n",
      "Epoch : 910, batch :  1, loss :    9.88046, regression loss :   38.57946, sparse loss :    6.69168, time : 1926.62319.\n",
      "Epoch : 911, batch :  1, loss :    9.87248, regression loss :   38.57478, sparse loss :    6.68333, time : 1929.07859.\n",
      "Epoch : 912, batch :  1, loss :    9.86453, regression loss :   38.57012, sparse loss :    6.67502, time : 1931.23052.\n",
      "Epoch : 913, batch :  1, loss :    9.85660, regression loss :   38.56547, sparse loss :    6.66672, time : 1933.34273.\n",
      "Epoch : 914, batch :  1, loss :    9.84870, regression loss :   38.56086, sparse loss :    6.65846, time : 1935.38679.\n",
      "Epoch : 915, batch :  1, loss :    9.84082, regression loss :   38.55626, sparse loss :    6.65022, time : 1937.53203.\n",
      "Epoch : 916, batch :  1, loss :    9.83297, regression loss :   38.55169, sparse loss :    6.64200, time : 1939.58295.\n",
      "Epoch : 917, batch :  1, loss :    9.82515, regression loss :   38.54713, sparse loss :    6.63382, time : 1941.91448.\n",
      "Epoch : 918, batch :  1, loss :    9.81735, regression loss :   38.54260, sparse loss :    6.62566, time : 1944.13543.\n",
      "Epoch : 919, batch :  1, loss :    9.80958, regression loss :   38.53810, sparse loss :    6.61752, time : 1946.30524.\n",
      "Epoch : 920, batch :  1, loss :    9.80183, regression loss :   38.53361, sparse loss :    6.60941, time : 1948.36294.\n",
      "Epoch : 921, batch :  1, loss :    9.79411, regression loss :   38.52914, sparse loss :    6.60133, time : 1950.69089.\n",
      "Epoch : 922, batch :  1, loss :    9.78641, regression loss :   38.52470, sparse loss :    6.59327, time : 1952.95824.\n",
      "Epoch : 923, batch :  1, loss :    9.77874, regression loss :   38.52028, sparse loss :    6.58524, time : 1955.09657.\n",
      "Epoch : 924, batch :  1, loss :    9.77110, regression loss :   38.51588, sparse loss :    6.57723, time : 1957.28284.\n",
      "Epoch : 925, batch :  1, loss :    9.76347, regression loss :   38.51150, sparse loss :    6.56925, time : 1959.33281.\n",
      "Epoch : 926, batch :  1, loss :    9.75588, regression loss :   38.50714, sparse loss :    6.56129, time : 1961.42209.\n",
      "Epoch : 927, batch :  1, loss :    9.74831, regression loss :   38.50280, sparse loss :    6.55336, time : 1963.56063.\n",
      "Epoch : 928, batch :  1, loss :    9.74076, regression loss :   38.49849, sparse loss :    6.54546, time : 1965.63985.\n",
      "Epoch : 929, batch :  1, loss :    9.73324, regression loss :   38.49419, sparse loss :    6.53758, time : 1967.96363.\n",
      "Epoch : 930, batch :  1, loss :    9.72574, regression loss :   38.48991, sparse loss :    6.52972, time : 1970.08373.\n",
      "Epoch : 931, batch :  1, loss :    9.71827, regression loss :   38.48566, sparse loss :    6.52189, time : 1972.27322.\n",
      "Epoch : 932, batch :  1, loss :    9.71082, regression loss :   38.48142, sparse loss :    6.51408, time : 1974.46920.\n",
      "Epoch : 933, batch :  1, loss :    9.70339, regression loss :   38.47721, sparse loss :    6.50630, time : 1976.62642.\n",
      "Epoch : 934, batch :  1, loss :    9.69599, regression loss :   38.47302, sparse loss :    6.49855, time : 1978.80864.\n",
      "Epoch : 935, batch :  1, loss :    9.68862, regression loss :   38.46884, sparse loss :    6.49081, time : 1981.09402.\n",
      "Epoch : 936, batch :  1, loss :    9.68126, regression loss :   38.46469, sparse loss :    6.48310, time : 1983.33074.\n",
      "Epoch : 937, batch :  1, loss :    9.67394, regression loss :   38.46056, sparse loss :    6.47542, time : 1985.61053.\n",
      "Epoch : 938, batch :  1, loss :    9.66663, regression loss :   38.45644, sparse loss :    6.46776, time : 1987.76568.\n",
      "Epoch : 939, batch :  1, loss :    9.65935, regression loss :   38.45235, sparse loss :    6.46013, time : 1990.03092.\n",
      "Epoch : 940, batch :  1, loss :    9.65209, regression loss :   38.44828, sparse loss :    6.45252, time : 1992.13257.\n",
      "Epoch : 941, batch :  1, loss :    9.64486, regression loss :   38.44422, sparse loss :    6.44493, time : 1994.25386.\n",
      "Epoch : 942, batch :  1, loss :    9.63765, regression loss :   38.44019, sparse loss :    6.43737, time : 1996.31322.\n",
      "Epoch : 943, batch :  1, loss :    9.63047, regression loss :   38.43617, sparse loss :    6.42983, time : 1998.63462.\n",
      "Epoch : 944, batch :  1, loss :    9.62330, regression loss :   38.43217, sparse loss :    6.42232, time : 2000.84994.\n",
      "Epoch : 945, batch :  1, loss :    9.61616, regression loss :   38.42819, sparse loss :    6.41483, time : 2003.00602.\n",
      "Epoch : 946, batch :  1, loss :    9.60905, regression loss :   38.42423, sparse loss :    6.40736, time : 2005.29411.\n",
      "Epoch : 947, batch :  1, loss :    9.60196, regression loss :   38.42030, sparse loss :    6.39992, time : 2007.42336.\n",
      "Epoch : 948, batch :  1, loss :    9.59489, regression loss :   38.41637, sparse loss :    6.39250, time : 2009.68436.\n",
      "Epoch : 949, batch :  1, loss :    9.58784, regression loss :   38.41247, sparse loss :    6.38510, time : 2011.91614.\n",
      "Epoch : 950, batch :  1, loss :    9.58082, regression loss :   38.40859, sparse loss :    6.37773, time : 2014.10724.\n",
      "Epoch : 951, batch :  1, loss :    9.57382, regression loss :   38.40472, sparse loss :    6.37038, time : 2016.34568.\n",
      "Epoch : 952, batch :  1, loss :    9.56684, regression loss :   38.40087, sparse loss :    6.36306, time : 2018.52874.\n",
      "Epoch : 953, batch :  1, loss :    9.55988, regression loss :   38.39704, sparse loss :    6.35575, time : 2020.72933.\n",
      "Epoch : 954, batch :  1, loss :    9.55295, regression loss :   38.39323, sparse loss :    6.34847, time : 2022.94948.\n",
      "Epoch : 955, batch :  1, loss :    9.54604, regression loss :   38.38944, sparse loss :    6.34122, time : 2025.06408.\n",
      "Epoch : 956, batch :  1, loss :    9.53915, regression loss :   38.38567, sparse loss :    6.33399, time : 2027.19228.\n",
      "Epoch : 957, batch :  1, loss :    9.53229, regression loss :   38.38191, sparse loss :    6.32677, time : 2029.28510.\n",
      "Epoch : 958, batch :  1, loss :    9.52544, regression loss :   38.37817, sparse loss :    6.31959, time : 2031.38283.\n",
      "Epoch : 959, batch :  1, loss :    9.51862, regression loss :   38.37445, sparse loss :    6.31242, time : 2033.47030.\n",
      "Epoch : 960, batch :  1, loss :    9.51183, regression loss :   38.37074, sparse loss :    6.30528, time : 2035.69009.\n",
      "Epoch : 961, batch :  1, loss :    9.50505, regression loss :   38.36705, sparse loss :    6.29816, time : 2037.75780.\n",
      "Epoch : 962, batch :  1, loss :    9.49830, regression loss :   38.36338, sparse loss :    6.29106, time : 2039.94826.\n",
      "Epoch : 963, batch :  1, loss :    9.49156, regression loss :   38.35973, sparse loss :    6.28399, time : 2041.99563.\n",
      "Epoch : 964, batch :  1, loss :    9.48485, regression loss :   38.35609, sparse loss :    6.27694, time : 2044.24793.\n",
      "Epoch : 965, batch :  1, loss :    9.47817, regression loss :   38.35247, sparse loss :    6.26991, time : 2046.34043.\n",
      "Epoch : 966, batch :  1, loss :    9.47150, regression loss :   38.34887, sparse loss :    6.26290, time : 2048.76305.\n",
      "Epoch : 967, batch :  1, loss :    9.46485, regression loss :   38.34528, sparse loss :    6.25592, time : 2050.83926.\n",
      "Epoch : 968, batch :  1, loss :    9.45823, regression loss :   38.34171, sparse loss :    6.24896, time : 2052.92584.\n",
      "Epoch : 969, batch :  1, loss :    9.45163, regression loss :   38.33816, sparse loss :    6.24202, time : 2055.05063.\n",
      "Epoch : 970, batch :  1, loss :    9.44505, regression loss :   38.33462, sparse loss :    6.23510, time : 2057.36888.\n",
      "Epoch : 971, batch :  1, loss :    9.43849, regression loss :   38.33110, sparse loss :    6.22820, time : 2059.46557.\n",
      "Epoch : 972, batch :  1, loss :    9.43195, regression loss :   38.32760, sparse loss :    6.22133, time : 2061.64732.\n",
      "Epoch : 973, batch :  1, loss :    9.42544, regression loss :   38.32411, sparse loss :    6.21447, time : 2063.97056.\n",
      "Epoch : 974, batch :  1, loss :    9.41894, regression loss :   38.32064, sparse loss :    6.20764, time : 2066.27700.\n",
      "Epoch : 975, batch :  1, loss :    9.41247, regression loss :   38.31718, sparse loss :    6.20083, time : 2068.62795.\n",
      "Epoch : 976, batch :  1, loss :    9.40601, regression loss :   38.31374, sparse loss :    6.19405, time : 2070.71965.\n",
      "Epoch : 977, batch :  1, loss :    9.39958, regression loss :   38.31031, sparse loss :    6.18728, time : 2073.14813.\n",
      "Epoch : 978, batch :  1, loss :    9.39317, regression loss :   38.30691, sparse loss :    6.18053, time : 2075.34869.\n",
      "Epoch : 979, batch :  1, loss :    9.38678, regression loss :   38.30350, sparse loss :    6.17381, time : 2077.39828.\n",
      "Epoch : 980, batch :  1, loss :    9.38041, regression loss :   38.30014, sparse loss :    6.16711, time : 2079.45001.\n",
      "Epoch : 981, batch :  1, loss :    9.37406, regression loss :   38.29675, sparse loss :    6.16043, time : 2081.83772.\n",
      "Epoch : 982, batch :  1, loss :    9.36774, regression loss :   38.29344, sparse loss :    6.15377, time : 2084.05207.\n",
      "Epoch : 983, batch :  1, loss :    9.36143, regression loss :   38.29005, sparse loss :    6.14714, time : 2086.23402.\n",
      "Epoch : 984, batch :  1, loss :    9.35514, regression loss :   38.28681, sparse loss :    6.14051, time : 2088.41608.\n",
      "Epoch : 985, batch :  1, loss :    9.34887, regression loss :   38.28339, sparse loss :    6.13393, time : 2090.51504.\n",
      "Epoch : 986, batch :  1, loss :    9.34263, regression loss :   38.28025, sparse loss :    6.12734, time : 2092.63857.\n",
      "Epoch : 987, batch :  1, loss :    9.33640, regression loss :   38.27683, sparse loss :    6.12080, time : 2094.90890.\n",
      "Epoch : 988, batch :  1, loss :    9.33020, regression loss :   38.27366, sparse loss :    6.11426, time : 2097.08465.\n",
      "Epoch : 989, batch :  1, loss :    9.32401, regression loss :   38.27042, sparse loss :    6.10774, time : 2099.20085.\n",
      "Epoch : 990, batch :  1, loss :    9.31784, regression loss :   38.26710, sparse loss :    6.10126, time : 2101.32601.\n",
      "Epoch : 991, batch :  1, loss :    9.31170, regression loss :   38.26399, sparse loss :    6.09478, time : 2103.55340.\n",
      "Epoch : 992, batch :  1, loss :    9.30557, regression loss :   38.26073, sparse loss :    6.08833, time : 2105.79862.\n",
      "Epoch : 993, batch :  1, loss :    9.29947, regression loss :   38.25751, sparse loss :    6.08191, time : 2107.98487.\n",
      "Epoch : 994, batch :  1, loss :    9.29338, regression loss :   38.25442, sparse loss :    6.07549, time : 2110.29753.\n",
      "Epoch : 995, batch :  1, loss :    9.28731, regression loss :   38.25119, sparse loss :    6.06911, time : 2112.61736.\n",
      "Epoch : 996, batch :  1, loss :    9.28127, regression loss :   38.24805, sparse loss :    6.06274, time : 2114.85102.\n",
      "Epoch : 997, batch :  1, loss :    9.27524, regression loss :   38.24497, sparse loss :    6.05638, time : 2117.10673.\n",
      "Epoch : 998, batch :  1, loss :    9.26923, regression loss :   38.24178, sparse loss :    6.05006, time : 2119.31085.\n",
      "Epoch : 999, batch :  1, loss :    9.26324, regression loss :   38.23870, sparse loss :    6.04375, time : 2121.50816.\n",
      "Epoch : 1000, batch :  1, loss :    9.25728, regression loss :   38.23564, sparse loss :    6.03746, time : 2126.31063.\n",
      "Epoch : 1001, batch :  1, loss :    9.25133, regression loss :   38.23250, sparse loss :    6.03120, time : 2128.68203.\n",
      "Epoch : 1002, batch :  1, loss :    9.24540, regression loss :   38.22947, sparse loss :    6.02495, time : 2130.89077.\n",
      "Epoch : 1003, batch :  1, loss :    9.23949, regression loss :   38.22643, sparse loss :    6.01872, time : 2133.10980.\n",
      "Epoch : 1004, batch :  1, loss :    9.23360, regression loss :   38.22334, sparse loss :    6.01251, time : 2135.21939.\n",
      "Epoch : 1005, batch :  1, loss :    9.22772, regression loss :   38.22036, sparse loss :    6.00632, time : 2137.32206.\n",
      "Epoch : 1006, batch :  1, loss :    9.22187, regression loss :   38.21734, sparse loss :    6.00015, time : 2139.53717.\n",
      "Epoch : 1007, batch :  1, loss :    9.21604, regression loss :   38.21430, sparse loss :    5.99401, time : 2141.74549.\n",
      "Epoch : 1008, batch :  1, loss :    9.21022, regression loss :   38.21136, sparse loss :    5.98787, time : 2143.95140.\n",
      "Epoch : 1009, batch :  1, loss :    9.20442, regression loss :   38.20836, sparse loss :    5.98176, time : 2146.13824.\n",
      "Epoch : 1010, batch :  1, loss :    9.19865, regression loss :   38.20538, sparse loss :    5.97568, time : 2148.23697.\n",
      "Epoch : 1011, batch :  1, loss :    9.19289, regression loss :   38.20247, sparse loss :    5.96960, time : 2150.33025.\n",
      "Epoch : 1012, batch :  1, loss :    9.18715, regression loss :   38.19950, sparse loss :    5.96355, time : 2152.64523.\n",
      "Epoch : 1013, batch :  1, loss :    9.18142, regression loss :   38.19658, sparse loss :    5.95752, time : 2154.77044.\n",
      "Epoch : 1014, batch :  1, loss :    9.17572, regression loss :   38.19369, sparse loss :    5.95150, time : 2157.14097.\n",
      "Epoch : 1015, batch :  1, loss :    9.17004, regression loss :   38.19076, sparse loss :    5.94551, time : 2159.42218.\n",
      "Epoch : 1016, batch :  1, loss :    9.16437, regression loss :   38.18788, sparse loss :    5.93954, time : 2161.62370.\n",
      "Epoch : 1017, batch :  1, loss :    9.15872, regression loss :   38.18502, sparse loss :    5.93358, time : 2163.67660.\n",
      "Epoch : 1018, batch :  1, loss :    9.15309, regression loss :   38.18212, sparse loss :    5.92764, time : 2165.73985.\n",
      "Epoch : 1019, batch :  1, loss :    9.14748, regression loss :   38.17929, sparse loss :    5.92172, time : 2168.09339.\n",
      "Epoch : 1020, batch :  1, loss :    9.14189, regression loss :   38.17645, sparse loss :    5.91582, time : 2170.29770.\n",
      "Epoch : 1021, batch :  1, loss :    9.13631, regression loss :   38.17361, sparse loss :    5.90995, time : 2172.44906.\n",
      "Epoch : 1022, batch :  1, loss :    9.13075, regression loss :   38.17081, sparse loss :    5.90408, time : 2174.54610.\n",
      "Epoch : 1023, batch :  1, loss :    9.12521, regression loss :   38.16799, sparse loss :    5.89824, time : 2176.63866.\n",
      "Epoch : 1024, batch :  1, loss :    9.11969, regression loss :   38.16520, sparse loss :    5.89242, time : 2178.75683.\n",
      "Epoch : 1025, batch :  1, loss :    9.11419, regression loss :   38.16243, sparse loss :    5.88661, time : 2180.84061.\n",
      "Epoch : 1026, batch :  1, loss :    9.10870, regression loss :   38.15964, sparse loss :    5.88082, time : 2182.97116.\n",
      "Epoch : 1027, batch :  1, loss :    9.10324, regression loss :   38.15689, sparse loss :    5.87505, time : 2185.07810.\n",
      "Epoch : 1028, batch :  1, loss :    9.09779, regression loss :   38.15415, sparse loss :    5.86930, time : 2187.34759.\n",
      "Epoch : 1029, batch :  1, loss :    9.09235, regression loss :   38.15140, sparse loss :    5.86357, time : 2189.58571.\n",
      "Epoch : 1030, batch :  1, loss :    9.08694, regression loss :   38.14869, sparse loss :    5.85785, time : 2191.69781.\n",
      "Epoch : 1031, batch :  1, loss :    9.08154, regression loss :   38.14597, sparse loss :    5.85216, time : 2193.86104.\n",
      "Epoch : 1032, batch :  1, loss :    9.07616, regression loss :   38.14327, sparse loss :    5.84648, time : 2195.91388.\n",
      "Epoch : 1033, batch :  1, loss :    9.07080, regression loss :   38.14058, sparse loss :    5.84082, time : 2197.99400.\n",
      "Epoch : 1034, batch :  1, loss :    9.06545, regression loss :   38.13789, sparse loss :    5.83518, time : 2200.11231.\n",
      "Epoch : 1035, batch :  1, loss :    9.06013, regression loss :   38.13523, sparse loss :    5.82956, time : 2202.43019.\n",
      "Epoch : 1036, batch :  1, loss :    9.05481, regression loss :   38.13257, sparse loss :    5.82395, time : 2204.58749.\n",
      "Epoch : 1037, batch :  1, loss :    9.04952, regression loss :   38.12992, sparse loss :    5.81837, time : 2206.66797.\n",
      "Epoch : 1038, batch :  1, loss :    9.04424, regression loss :   38.12730, sparse loss :    5.81279, time : 2208.76406.\n",
      "Epoch : 1039, batch :  1, loss :    9.03899, regression loss :   38.12466, sparse loss :    5.80724, time : 2210.87488.\n",
      "Epoch : 1040, batch :  1, loss :    9.03374, regression loss :   38.12206, sparse loss :    5.80171, time : 2212.95812.\n",
      "Epoch : 1041, batch :  1, loss :    9.02852, regression loss :   38.11945, sparse loss :    5.79619, time : 2215.06776.\n",
      "Epoch : 1042, batch :  1, loss :    9.02331, regression loss :   38.11686, sparse loss :    5.79069, time : 2217.18381.\n",
      "Epoch : 1043, batch :  1, loss :    9.01812, regression loss :   38.11428, sparse loss :    5.78521, time : 2219.36257.\n",
      "Epoch : 1044, batch :  1, loss :    9.01294, regression loss :   38.11170, sparse loss :    5.77975, time : 2221.64597.\n",
      "Epoch : 1045, batch :  1, loss :    9.00778, regression loss :   38.10915, sparse loss :    5.77430, time : 2223.77659.\n",
      "Epoch : 1046, batch :  1, loss :    9.00264, regression loss :   38.10660, sparse loss :    5.76887, time : 2226.00177.\n",
      "Epoch : 1047, batch :  1, loss :    8.99752, regression loss :   38.10406, sparse loss :    5.76346, time : 2228.42184.\n",
      "Epoch : 1048, batch :  1, loss :    8.99241, regression loss :   38.10153, sparse loss :    5.75806, time : 2230.72942.\n",
      "Epoch : 1049, batch :  1, loss :    8.98732, regression loss :   38.09901, sparse loss :    5.75268, time : 2233.09026.\n",
      "Epoch : 1050, batch :  1, loss :    8.98224, regression loss :   38.09651, sparse loss :    5.74732, time : 2235.28979.\n",
      "Epoch : 1051, batch :  1, loss :    8.97718, regression loss :   38.09401, sparse loss :    5.74198, time : 2237.63789.\n",
      "Epoch : 1052, batch :  1, loss :    8.97214, regression loss :   38.09153, sparse loss :    5.73665, time : 2239.76703.\n",
      "Epoch : 1053, batch :  1, loss :    8.96711, regression loss :   38.08905, sparse loss :    5.73134, time : 2241.86394.\n",
      "Epoch : 1054, batch :  1, loss :    8.96210, regression loss :   38.08658, sparse loss :    5.72605, time : 2243.98416.\n",
      "Epoch : 1055, batch :  1, loss :    8.95711, regression loss :   38.08413, sparse loss :    5.72077, time : 2246.15061.\n",
      "Epoch : 1056, batch :  1, loss :    8.95213, regression loss :   38.08168, sparse loss :    5.71551, time : 2248.28908.\n",
      "Epoch : 1057, batch :  1, loss :    8.94717, regression loss :   38.07925, sparse loss :    5.71027, time : 2250.39756.\n",
      "Epoch : 1058, batch :  1, loss :    8.94222, regression loss :   38.07682, sparse loss :    5.70505, time : 2252.58781.\n",
      "Epoch : 1059, batch :  1, loss :    8.93729, regression loss :   38.07441, sparse loss :    5.69984, time : 2254.89032.\n",
      "Epoch : 1060, batch :  1, loss :    8.93238, regression loss :   38.07200, sparse loss :    5.69464, time : 2257.09771.\n",
      "Epoch : 1061, batch :  1, loss :    8.92748, regression loss :   38.06961, sparse loss :    5.68947, time : 2259.21327.\n",
      "Epoch : 1062, batch :  1, loss :    8.92260, regression loss :   38.06722, sparse loss :    5.68431, time : 2261.38198.\n",
      "Epoch : 1063, batch :  1, loss :    8.91773, regression loss :   38.06485, sparse loss :    5.67917, time : 2263.49794.\n",
      "Epoch : 1064, batch :  1, loss :    8.91288, regression loss :   38.06248, sparse loss :    5.67404, time : 2265.80993.\n",
      "Epoch : 1065, batch :  1, loss :    8.90805, regression loss :   38.06013, sparse loss :    5.66893, time : 2267.97953.\n",
      "Epoch : 1066, batch :  1, loss :    8.90323, regression loss :   38.05778, sparse loss :    5.66384, time : 2270.29699.\n",
      "Epoch : 1067, batch :  1, loss :    8.89843, regression loss :   38.05545, sparse loss :    5.65876, time : 2272.60088.\n",
      "Epoch : 1068, batch :  1, loss :    8.89364, regression loss :   38.05311, sparse loss :    5.65370, time : 2274.85456.\n",
      "Epoch : 1069, batch :  1, loss :    8.88886, regression loss :   38.05082, sparse loss :    5.64865, time : 2277.02746.\n",
      "Epoch : 1070, batch :  1, loss :    8.88411, regression loss :   38.04848, sparse loss :    5.64362, time : 2279.17669.\n",
      "Epoch : 1071, batch :  1, loss :    8.87936, regression loss :   38.04623, sparse loss :    5.63860, time : 2281.26866.\n",
      "Epoch : 1072, batch :  1, loss :    8.87464, regression loss :   38.04388, sparse loss :    5.63361, time : 2283.39518.\n",
      "Epoch : 1073, batch :  1, loss :    8.86993, regression loss :   38.04169, sparse loss :    5.62862, time : 2285.77525.\n",
      "Epoch : 1074, batch :  1, loss :    8.86523, regression loss :   38.03931, sparse loss :    5.62367, time : 2287.98663.\n",
      "Epoch : 1075, batch :  1, loss :    8.86055, regression loss :   38.03718, sparse loss :    5.61870, time : 2290.24986.\n",
      "Epoch : 1076, batch :  1, loss :    8.85588, regression loss :   38.03483, sparse loss :    5.61378, time : 2292.39528.\n",
      "Epoch : 1077, batch :  1, loss :    8.85123, regression loss :   38.03263, sparse loss :    5.60885, time : 2294.47922.\n",
      "Epoch : 1078, batch :  1, loss :    8.84660, regression loss :   38.03043, sparse loss :    5.60395, time : 2296.59211.\n",
      "Epoch : 1079, batch :  1, loss :    8.84197, regression loss :   38.02813, sparse loss :    5.59907, time : 2298.73774.\n",
      "Epoch : 1080, batch :  1, loss :    8.83737, regression loss :   38.02601, sparse loss :    5.59419, time : 2301.02596.\n",
      "Epoch : 1081, batch :  1, loss :    8.83278, regression loss :   38.02376, sparse loss :    5.58933, time : 2303.12574.\n",
      "Epoch : 1082, batch :  1, loss :    8.82820, regression loss :   38.02156, sparse loss :    5.58450, time : 2305.27052.\n",
      "Epoch : 1083, batch :  1, loss :    8.82364, regression loss :   38.01944, sparse loss :    5.57966, time : 2307.39204.\n",
      "Epoch : 1084, batch :  1, loss :    8.81909, regression loss :   38.01719, sparse loss :    5.57486, time : 2309.48840.\n",
      "Epoch : 1085, batch :  1, loss :    8.81456, regression loss :   38.01507, sparse loss :    5.57006, time : 2311.77295.\n",
      "Epoch : 1086, batch :  1, loss :    8.81004, regression loss :   38.01293, sparse loss :    5.56528, time : 2313.94562.\n",
      "Epoch : 1087, batch :  1, loss :    8.80554, regression loss :   38.01073, sparse loss :    5.56052, time : 2316.03522.\n",
      "Epoch : 1088, batch :  1, loss :    8.80105, regression loss :   38.00865, sparse loss :    5.55576, time : 2318.34427.\n",
      "Epoch : 1089, batch :  1, loss :    8.79658, regression loss :   38.00650, sparse loss :    5.55103, time : 2320.51831.\n",
      "Epoch : 1090, batch :  1, loss :    8.79212, regression loss :   38.00436, sparse loss :    5.54631, time : 2322.69954.\n",
      "Epoch : 1091, batch :  1, loss :    8.78767, regression loss :   38.00230, sparse loss :    5.54160, time : 2324.85272.\n",
      "Epoch : 1092, batch :  1, loss :    8.78324, regression loss :   38.00016, sparse loss :    5.53692, time : 2326.97343.\n",
      "Epoch : 1093, batch :  1, loss :    8.77882, regression loss :   37.99808, sparse loss :    5.53224, time : 2329.29780.\n",
      "Epoch : 1094, batch :  1, loss :    8.77442, regression loss :   37.99602, sparse loss :    5.52758, time : 2331.50261.\n",
      "Epoch : 1095, batch :  1, loss :    8.77003, regression loss :   37.99391, sparse loss :    5.52294, time : 2333.83434.\n",
      "Epoch : 1096, batch :  1, loss :    8.76566, regression loss :   37.99188, sparse loss :    5.51830, time : 2336.01896.\n",
      "Epoch : 1097, batch :  1, loss :    8.76130, regression loss :   37.98982, sparse loss :    5.51369, time : 2338.28353.\n",
      "Epoch : 1098, batch :  1, loss :    8.75695, regression loss :   37.98775, sparse loss :    5.50909, time : 2340.51917.\n",
      "Epoch : 1099, batch :  1, loss :    8.75262, regression loss :   37.98575, sparse loss :    5.50450, time : 2342.93434.\n",
      "Epoch : 1100, batch :  1, loss :    8.74830, regression loss :   37.98369, sparse loss :    5.49993, time : 2347.86446.\n",
      "Epoch : 1101, batch :  1, loss :    8.74400, regression loss :   37.98168, sparse loss :    5.49537, time : 2349.47160.\n",
      "Epoch : 1102, batch :  1, loss :    8.73971, regression loss :   37.97969, sparse loss :    5.49082, time : 2351.21649.\n",
      "Epoch : 1103, batch :  1, loss :    8.73543, regression loss :   37.97766, sparse loss :    5.48630, time : 2352.78010.\n",
      "Epoch : 1104, batch :  1, loss :    8.73117, regression loss :   37.97568, sparse loss :    5.48178, time : 2354.39144.\n",
      "Epoch : 1105, batch :  1, loss :    8.72692, regression loss :   37.97369, sparse loss :    5.47728, time : 2355.91756.\n",
      "Epoch : 1106, batch :  1, loss :    8.72269, regression loss :   37.97170, sparse loss :    5.47280, time : 2357.57258.\n",
      "Epoch : 1107, batch :  1, loss :    8.71846, regression loss :   37.96976, sparse loss :    5.46832, time : 2359.15550.\n",
      "Epoch : 1108, batch :  1, loss :    8.71426, regression loss :   37.96778, sparse loss :    5.46387, time : 2360.80382.\n",
      "Epoch : 1109, batch :  1, loss :    8.71006, regression loss :   37.96584, sparse loss :    5.45942, time : 2362.38644.\n",
      "Epoch : 1110, batch :  1, loss :    8.70588, regression loss :   37.96390, sparse loss :    5.45499, time : 2363.90458.\n",
      "Epoch : 1111, batch :  1, loss :    8.70171, regression loss :   37.96195, sparse loss :    5.45058, time : 2365.52374.\n",
      "Epoch : 1112, batch :  1, loss :    8.69756, regression loss :   37.96004, sparse loss :    5.44617, time : 2367.15863.\n",
      "Epoch : 1113, batch :  1, loss :    8.69342, regression loss :   37.95811, sparse loss :    5.44179, time : 2368.81158.\n",
      "Epoch : 1114, batch :  1, loss :    8.68929, regression loss :   37.95621, sparse loss :    5.43741, time : 2370.52829.\n",
      "Epoch : 1115, batch :  1, loss :    8.68518, regression loss :   37.95431, sparse loss :    5.43305, time : 2372.07600.\n",
      "Epoch : 1116, batch :  1, loss :    8.68108, regression loss :   37.95241, sparse loss :    5.42871, time : 2373.73473.\n",
      "Epoch : 1117, batch :  1, loss :    8.67699, regression loss :   37.95054, sparse loss :    5.42437, time : 2375.29321.\n",
      "Epoch : 1118, batch :  1, loss :    8.67292, regression loss :   37.94865, sparse loss :    5.42006, time : 2376.97673.\n",
      "Epoch : 1119, batch :  1, loss :    8.66886, regression loss :   37.94679, sparse loss :    5.41575, time : 2378.54881.\n",
      "Epoch : 1120, batch :  1, loss :    8.66481, regression loss :   37.94493, sparse loss :    5.41146, time : 2380.09003.\n",
      "Epoch : 1121, batch :  1, loss :    8.66077, regression loss :   37.94307, sparse loss :    5.40718, time : 2381.68137.\n",
      "Epoch : 1122, batch :  1, loss :    8.65675, regression loss :   37.94123, sparse loss :    5.40292, time : 2383.36900.\n",
      "Epoch : 1123, batch :  1, loss :    8.65274, regression loss :   37.93939, sparse loss :    5.39867, time : 2385.16041.\n",
      "Epoch : 1124, batch :  1, loss :    8.64874, regression loss :   37.93757, sparse loss :    5.39443, time : 2386.87831.\n",
      "Epoch : 1125, batch :  1, loss :    8.64476, regression loss :   37.93574, sparse loss :    5.39021, time : 2388.46539.\n",
      "Epoch : 1126, batch :  1, loss :    8.64079, regression loss :   37.93393, sparse loss :    5.38600, time : 2390.03155.\n",
      "Epoch : 1127, batch :  1, loss :    8.63683, regression loss :   37.93213, sparse loss :    5.38180, time : 2391.77062.\n",
      "Epoch : 1128, batch :  1, loss :    8.63289, regression loss :   37.93032, sparse loss :    5.37762, time : 2393.46083.\n",
      "Epoch : 1129, batch :  1, loss :    8.62896, regression loss :   37.92854, sparse loss :    5.37345, time : 2395.16729.\n",
      "Epoch : 1130, batch :  1, loss :    8.62504, regression loss :   37.92675, sparse loss :    5.36929, time : 2396.85366.\n",
      "Epoch : 1131, batch :  1, loss :    8.62113, regression loss :   37.92499, sparse loss :    5.36515, time : 2398.44817.\n",
      "Epoch : 1132, batch :  1, loss :    8.61724, regression loss :   37.92321, sparse loss :    5.36102, time : 2400.07507.\n",
      "Epoch : 1133, batch :  1, loss :    8.61335, regression loss :   37.92146, sparse loss :    5.35690, time : 2401.83265.\n",
      "Epoch : 1134, batch :  1, loss :    8.60948, regression loss :   37.91970, sparse loss :    5.35279, time : 2403.52587.\n",
      "Epoch : 1135, batch :  1, loss :    8.60563, regression loss :   37.91796, sparse loss :    5.34870, time : 2405.18927.\n",
      "Epoch : 1136, batch :  1, loss :    8.60178, regression loss :   37.91622, sparse loss :    5.34462, time : 2406.78015.\n",
      "Epoch : 1137, batch :  1, loss :    8.59795, regression loss :   37.91449, sparse loss :    5.34056, time : 2408.36560.\n",
      "Epoch : 1138, batch :  1, loss :    8.59413, regression loss :   37.91277, sparse loss :    5.33650, time : 2410.25255.\n",
      "Epoch : 1139, batch :  1, loss :    8.59032, regression loss :   37.91105, sparse loss :    5.33246, time : 2411.91270.\n",
      "Epoch : 1140, batch :  1, loss :    8.58653, regression loss :   37.90935, sparse loss :    5.32844, time : 2413.47067.\n",
      "Epoch : 1141, batch :  1, loss :    8.58274, regression loss :   37.90763, sparse loss :    5.32442, time : 2415.14850.\n",
      "Epoch : 1142, batch :  1, loss :    8.57897, regression loss :   37.90595, sparse loss :    5.32042, time : 2416.82796.\n",
      "Epoch : 1143, batch :  1, loss :    8.57521, regression loss :   37.90425, sparse loss :    5.31643, time : 2418.55300.\n",
      "Epoch : 1144, batch :  1, loss :    8.57147, regression loss :   37.90259, sparse loss :    5.31245, time : 2420.14894.\n",
      "Epoch : 1145, batch :  1, loss :    8.56773, regression loss :   37.90089, sparse loss :    5.30849, time : 2421.85803.\n",
      "Epoch : 1146, batch :  1, loss :    8.56401, regression loss :   37.89926, sparse loss :    5.30454, time : 2423.54896.\n",
      "Epoch : 1147, batch :  1, loss :    8.56030, regression loss :   37.89755, sparse loss :    5.30061, time : 2425.12297.\n",
      "Epoch : 1148, batch :  1, loss :    8.55660, regression loss :   37.89597, sparse loss :    5.29667, time : 2426.80379.\n",
      "Epoch : 1149, batch :  1, loss :    8.55291, regression loss :   37.89422, sparse loss :    5.29277, time : 2428.39246.\n",
      "Epoch : 1150, batch :  1, loss :    8.54924, regression loss :   37.89270, sparse loss :    5.28885, time : 2430.03767.\n",
      "Epoch : 1151, batch :  1, loss :    8.54558, regression loss :   37.89096, sparse loss :    5.28498, time : 2431.68659.\n",
      "Epoch : 1152, batch :  1, loss :    8.54192, regression loss :   37.88940, sparse loss :    5.28109, time : 2433.43879.\n",
      "Epoch : 1153, batch :  1, loss :    8.53829, regression loss :   37.88778, sparse loss :    5.27723, time : 2435.14604.\n",
      "Epoch : 1154, batch :  1, loss :    8.53466, regression loss :   37.88611, sparse loss :    5.27338, time : 2436.78371.\n",
      "Epoch : 1155, batch :  1, loss :    8.53104, regression loss :   37.88459, sparse loss :    5.26954, time : 2438.43135.\n",
      "Epoch : 1156, batch :  1, loss :    8.52744, regression loss :   37.88292, sparse loss :    5.26572, time : 2440.08490.\n",
      "Epoch : 1157, batch :  1, loss :    8.52384, regression loss :   37.88134, sparse loss :    5.26190, time : 2441.85882.\n",
      "Epoch : 1158, batch :  1, loss :    8.52026, regression loss :   37.87979, sparse loss :    5.25809, time : 2443.65377.\n",
      "Epoch : 1159, batch :  1, loss :    8.51669, regression loss :   37.87815, sparse loss :    5.25431, time : 2445.34534.\n",
      "Epoch : 1160, batch :  1, loss :    8.51313, regression loss :   37.87663, sparse loss :    5.25052, time : 2446.87665.\n",
      "Epoch : 1161, batch :  1, loss :    8.50959, regression loss :   37.87505, sparse loss :    5.24676, time : 2448.51376.\n",
      "Epoch : 1162, batch :  1, loss :    8.50605, regression loss :   37.87345, sparse loss :    5.24301, time : 2450.06468.\n",
      "Epoch : 1163, batch :  1, loss :    8.50252, regression loss :   37.87195, sparse loss :    5.23926, time : 2451.72669.\n",
      "Epoch : 1164, batch :  1, loss :    8.49901, regression loss :   37.87035, sparse loss :    5.23553, time : 2453.42244.\n",
      "Epoch : 1165, batch :  1, loss :    8.49551, regression loss :   37.86882, sparse loss :    5.23181, time : 2455.05981.\n",
      "Epoch : 1166, batch :  1, loss :    8.49202, regression loss :   37.86731, sparse loss :    5.22810, time : 2456.70600.\n",
      "Epoch : 1167, batch :  1, loss :    8.48854, regression loss :   37.86573, sparse loss :    5.22441, time : 2458.30534.\n",
      "Epoch : 1168, batch :  1, loss :    8.48507, regression loss :   37.86424, sparse loss :    5.22072, time : 2459.96778.\n",
      "Epoch : 1169, batch :  1, loss :    8.48161, regression loss :   37.86272, sparse loss :    5.21705, time : 2461.55517.\n",
      "Epoch : 1170, batch :  1, loss :    8.47817, regression loss :   37.86117, sparse loss :    5.21339, time : 2463.18859.\n",
      "Epoch : 1171, batch :  1, loss :    8.47473, regression loss :   37.85970, sparse loss :    5.20974, time : 2464.68164.\n",
      "Epoch : 1172, batch :  1, loss :    8.47131, regression loss :   37.85817, sparse loss :    5.20610, time : 2466.17242.\n",
      "Epoch : 1173, batch :  1, loss :    8.46790, regression loss :   37.85667, sparse loss :    5.20248, time : 2467.76863.\n",
      "Epoch : 1174, batch :  1, loss :    8.46450, regression loss :   37.85520, sparse loss :    5.19886, time : 2469.38182.\n",
      "Epoch : 1175, batch :  1, loss :    8.46110, regression loss :   37.85369, sparse loss :    5.19526, time : 2471.06675.\n",
      "Epoch : 1176, batch :  1, loss :    8.45772, regression loss :   37.85223, sparse loss :    5.19167, time : 2472.70569.\n",
      "Epoch : 1177, batch :  1, loss :    8.45435, regression loss :   37.85075, sparse loss :    5.18809, time : 2474.35344.\n",
      "Epoch : 1178, batch :  1, loss :    8.45100, regression loss :   37.84926, sparse loss :    5.18452, time : 2476.01900.\n",
      "Epoch : 1179, batch :  1, loss :    8.44765, regression loss :   37.84782, sparse loss :    5.18096, time : 2477.68273.\n",
      "Epoch : 1180, batch :  1, loss :    8.44431, regression loss :   37.84634, sparse loss :    5.17742, time : 2479.37371.\n",
      "Epoch : 1181, batch :  1, loss :    8.44099, regression loss :   37.84489, sparse loss :    5.17388, time : 2481.04317.\n",
      "Epoch : 1182, batch :  1, loss :    8.43767, regression loss :   37.84345, sparse loss :    5.17036, time : 2482.83738.\n",
      "Epoch : 1183, batch :  1, loss :    8.43436, regression loss :   37.84199, sparse loss :    5.16685, time : 2484.56210.\n",
      "Epoch : 1184, batch :  1, loss :    8.43107, regression loss :   37.84057, sparse loss :    5.16335, time : 2486.19465.\n",
      "Epoch : 1185, batch :  1, loss :    8.42779, regression loss :   37.83913, sparse loss :    5.15986, time : 2487.98398.\n",
      "Epoch : 1186, batch :  1, loss :    8.42451, regression loss :   37.83770, sparse loss :    5.15638, time : 2489.71711.\n",
      "Epoch : 1187, batch :  1, loss :    8.42125, regression loss :   37.83628, sparse loss :    5.15291, time : 2491.43631.\n",
      "Epoch : 1188, batch :  1, loss :    8.41800, regression loss :   37.83485, sparse loss :    5.14946, time : 2493.05807.\n",
      "Epoch : 1189, batch :  1, loss :    8.41476, regression loss :   37.83345, sparse loss :    5.14601, time : 2494.72245.\n",
      "Epoch : 1190, batch :  1, loss :    8.41153, regression loss :   37.83203, sparse loss :    5.14258, time : 2496.54852.\n",
      "Epoch : 1191, batch :  1, loss :    8.40830, regression loss :   37.83062, sparse loss :    5.13916, time : 2498.12695.\n",
      "Epoch : 1192, batch :  1, loss :    8.40509, regression loss :   37.82923, sparse loss :    5.13574, time : 2499.72531.\n",
      "Epoch : 1193, batch :  1, loss :    8.40189, regression loss :   37.82783, sparse loss :    5.13234, time : 2501.32903.\n",
      "Epoch : 1194, batch :  1, loss :    8.39870, regression loss :   37.82645, sparse loss :    5.12895, time : 2503.00583.\n",
      "Epoch : 1195, batch :  1, loss :    8.39552, regression loss :   37.82505, sparse loss :    5.12557, time : 2504.53472.\n",
      "Epoch : 1196, batch :  1, loss :    8.39235, regression loss :   37.82368, sparse loss :    5.12221, time : 2506.25574.\n",
      "Epoch : 1197, batch :  1, loss :    8.38919, regression loss :   37.82230, sparse loss :    5.11885, time : 2507.95862.\n",
      "Epoch : 1198, batch :  1, loss :    8.38604, regression loss :   37.82092, sparse loss :    5.11550, time : 2509.74706.\n",
      "Epoch : 1199, batch :  1, loss :    8.38290, regression loss :   37.81956, sparse loss :    5.11216, time : 2511.53606.\n",
      "Epoch : 1200, batch :  1, loss :    8.37977, regression loss :   37.81819, sparse loss :    5.10884, time : 2515.68928.\n",
      "Epoch : 1201, batch :  1, loss :    8.37665, regression loss :   37.81684, sparse loss :    5.10552, time : 2517.37410.\n",
      "Epoch : 1202, batch :  1, loss :    8.37355, regression loss :   37.81548, sparse loss :    5.10222, time : 2518.96222.\n",
      "Epoch : 1203, batch :  1, loss :    8.37045, regression loss :   37.81413, sparse loss :    5.09892, time : 2520.52667.\n",
      "Epoch : 1204, batch :  1, loss :    8.36736, regression loss :   37.81279, sparse loss :    5.09564, time : 2522.18279.\n",
      "Epoch : 1205, batch :  1, loss :    8.36428, regression loss :   37.81144, sparse loss :    5.09237, time : 2523.85113.\n",
      "Epoch : 1206, batch :  1, loss :    8.36121, regression loss :   37.81011, sparse loss :    5.08911, time : 2525.43503.\n",
      "Epoch : 1207, batch :  1, loss :    8.35815, regression loss :   37.80877, sparse loss :    5.08586, time : 2527.00928.\n",
      "Epoch : 1208, batch :  1, loss :    8.35510, regression loss :   37.80745, sparse loss :    5.08261, time : 2528.67174.\n",
      "Epoch : 1209, batch :  1, loss :    8.35206, regression loss :   37.80611, sparse loss :    5.07938, time : 2530.47022.\n",
      "Epoch : 1210, batch :  1, loss :    8.34903, regression loss :   37.80480, sparse loss :    5.07616, time : 2532.11457.\n",
      "Epoch : 1211, batch :  1, loss :    8.34600, regression loss :   37.80347, sparse loss :    5.07295, time : 2533.66462.\n",
      "Epoch : 1212, batch :  1, loss :    8.34299, regression loss :   37.80217, sparse loss :    5.06975, time : 2535.46754.\n",
      "Epoch : 1213, batch :  1, loss :    8.33999, regression loss :   37.80085, sparse loss :    5.06656, time : 2537.17581.\n",
      "Epoch : 1214, batch :  1, loss :    8.33700, regression loss :   37.79956, sparse loss :    5.06338, time : 2538.84090.\n",
      "Epoch : 1215, batch :  1, loss :    8.33402, regression loss :   37.79824, sparse loss :    5.06021, time : 2540.52118.\n",
      "Epoch : 1216, batch :  1, loss :    8.33104, regression loss :   37.79696, sparse loss :    5.05705, time : 2542.18611.\n",
      "Epoch : 1217, batch :  1, loss :    8.32808, regression loss :   37.79564, sparse loss :    5.05391, time : 2544.00072.\n",
      "Epoch : 1218, batch :  1, loss :    8.32513, regression loss :   37.79439, sparse loss :    5.05076, time : 2545.65304.\n",
      "Epoch : 1219, batch :  1, loss :    8.32218, regression loss :   37.79305, sparse loss :    5.04764, time : 2547.31237.\n",
      "Epoch : 1220, batch :  1, loss :    8.31925, regression loss :   37.79184, sparse loss :    5.04452, time : 2549.03923.\n",
      "Epoch : 1221, batch :  1, loss :    8.31632, regression loss :   37.79047, sparse loss :    5.04142, time : 2550.89080.\n",
      "Epoch : 1222, batch :  1, loss :    8.31340, regression loss :   37.78929, sparse loss :    5.03831, time : 2552.74789.\n",
      "Epoch : 1223, batch :  1, loss :    8.31050, regression loss :   37.78792, sparse loss :    5.03523, time : 2554.41915.\n",
      "Epoch : 1224, batch :  1, loss :    8.30760, regression loss :   37.78672, sparse loss :    5.03214, time : 2556.05115.\n",
      "Epoch : 1225, batch :  1, loss :    8.30471, regression loss :   37.78544, sparse loss :    5.02908, time : 2557.59900.\n",
      "Epoch : 1226, batch :  1, loss :    8.30183, regression loss :   37.78414, sparse loss :    5.02602, time : 2559.20236.\n",
      "Epoch : 1227, batch :  1, loss :    8.29896, regression loss :   37.78294, sparse loss :    5.02296, time : 2560.91388.\n",
      "Epoch : 1228, batch :  1, loss :    8.29610, regression loss :   37.78162, sparse loss :    5.01993, time : 2562.48440.\n",
      "Epoch : 1229, batch :  1, loss :    8.29325, regression loss :   37.78042, sparse loss :    5.01690, time : 2564.04873.\n",
      "Epoch : 1230, batch :  1, loss :    8.29041, regression loss :   37.77916, sparse loss :    5.01388, time : 2565.62566.\n",
      "Epoch : 1231, batch :  1, loss :    8.28757, regression loss :   37.77788, sparse loss :    5.01087, time : 2567.34706.\n",
      "Epoch : 1232, batch :  1, loss :    8.28475, regression loss :   37.77669, sparse loss :    5.00787, time : 2569.03873.\n",
      "Epoch : 1233, batch :  1, loss :    8.28193, regression loss :   37.77540, sparse loss :    5.00488, time : 2570.60229.\n",
      "Epoch : 1234, batch :  1, loss :    8.27913, regression loss :   37.77420, sparse loss :    5.00190, time : 2572.26250.\n",
      "Epoch : 1235, batch :  1, loss :    8.27633, regression loss :   37.77297, sparse loss :    4.99893, time : 2573.82598.\n",
      "Epoch : 1236, batch :  1, loss :    8.27354, regression loss :   37.77171, sparse loss :    4.99597, time : 2575.39027.\n",
      "Epoch : 1237, batch :  1, loss :    8.27076, regression loss :   37.77052, sparse loss :    4.99301, time : 2576.96164.\n",
      "Epoch : 1238, batch :  1, loss :    8.26799, regression loss :   37.76926, sparse loss :    4.99007, time : 2578.51880.\n",
      "Epoch : 1239, batch :  1, loss :    8.26523, regression loss :   37.76806, sparse loss :    4.98714, time : 2580.03691.\n",
      "Epoch : 1240, batch :  1, loss :    8.26248, regression loss :   37.76685, sparse loss :    4.98421, time : 2581.70958.\n",
      "Epoch : 1241, batch :  1, loss :    8.25973, regression loss :   37.76561, sparse loss :    4.98130, time : 2583.37747.\n",
      "Epoch : 1242, batch :  1, loss :    8.25700, regression loss :   37.76443, sparse loss :    4.97839, time : 2585.05401.\n",
      "Epoch : 1243, batch :  1, loss :    8.25427, regression loss :   37.76319, sparse loss :    4.97550, time : 2586.69099.\n",
      "Epoch : 1244, batch :  1, loss :    8.25155, regression loss :   37.76200, sparse loss :    4.97261, time : 2588.37370.\n",
      "Epoch : 1245, batch :  1, loss :    8.24884, regression loss :   37.76080, sparse loss :    4.96974, time : 2589.93105.\n",
      "Epoch : 1246, batch :  1, loss :    8.24614, regression loss :   37.75958, sparse loss :    4.96687, time : 2591.48939.\n",
      "Epoch : 1247, batch :  1, loss :    8.24345, regression loss :   37.75841, sparse loss :    4.96401, time : 2593.23894.\n",
      "Epoch : 1248, batch :  1, loss :    8.24077, regression loss :   37.75719, sparse loss :    4.96116, time : 2595.02704.\n",
      "Epoch : 1249, batch :  1, loss :    8.23809, regression loss :   37.75602, sparse loss :    4.95832, time : 2596.62333.\n",
      "Epoch : 1250, batch :  1, loss :    8.23543, regression loss :   37.75483, sparse loss :    4.95549, time : 2598.27281.\n",
      "Epoch : 1251, batch :  1, loss :    8.23277, regression loss :   37.75363, sparse loss :    4.95267, time : 2599.97220.\n",
      "Epoch : 1252, batch :  1, loss :    8.23012, regression loss :   37.75247, sparse loss :    4.94986, time : 2601.67407.\n",
      "Epoch : 1253, batch :  1, loss :    8.22748, regression loss :   37.75126, sparse loss :    4.94706, time : 2603.48827.\n",
      "Epoch : 1254, batch :  1, loss :    8.22484, regression loss :   37.75010, sparse loss :    4.94426, time : 2605.22532.\n",
      "Epoch : 1255, batch :  1, loss :    8.22222, regression loss :   37.74892, sparse loss :    4.94148, time : 2606.95443.\n",
      "Epoch : 1256, batch :  1, loss :    8.21961, regression loss :   37.74775, sparse loss :    4.93870, time : 2608.80899.\n",
      "Epoch : 1257, batch :  1, loss :    8.21700, regression loss :   37.74659, sparse loss :    4.93593, time : 2610.44164.\n",
      "Epoch : 1258, batch :  1, loss :    8.21440, regression loss :   37.74540, sparse loss :    4.93318, time : 2612.00819.\n",
      "Epoch : 1259, batch :  1, loss :    8.21181, regression loss :   37.74426, sparse loss :    4.93043, time : 2613.74116.\n",
      "Epoch : 1260, batch :  1, loss :    8.20923, regression loss :   37.74308, sparse loss :    4.92769, time : 2615.28473.\n",
      "Epoch : 1261, batch :  1, loss :    8.20665, regression loss :   37.74193, sparse loss :    4.92495, time : 2616.94671.\n",
      "Epoch : 1262, batch :  1, loss :    8.20409, regression loss :   37.74077, sparse loss :    4.92223, time : 2618.72325.\n",
      "Epoch : 1263, batch :  1, loss :    8.20153, regression loss :   37.73962, sparse loss :    4.91952, time : 2620.47723.\n",
      "Epoch : 1264, batch :  1, loss :    8.19898, regression loss :   37.73846, sparse loss :    4.91681, time : 2622.18836.\n",
      "Epoch : 1265, batch :  1, loss :    8.19644, regression loss :   37.73731, sparse loss :    4.91412, time : 2623.79922.\n",
      "Epoch : 1266, batch :  1, loss :    8.19390, regression loss :   37.73617, sparse loss :    4.91143, time : 2625.36897.\n",
      "Epoch : 1267, batch :  1, loss :    8.19138, regression loss :   37.73502, sparse loss :    4.90875, time : 2627.15229.\n",
      "Epoch : 1268, batch :  1, loss :    8.18886, regression loss :   37.73389, sparse loss :    4.90608, time : 2628.80988.\n",
      "Epoch : 1269, batch :  1, loss :    8.18635, regression loss :   37.73274, sparse loss :    4.90342, time : 2630.61330.\n",
      "Epoch : 1270, batch :  1, loss :    8.18385, regression loss :   37.73162, sparse loss :    4.90077, time : 2632.40560.\n",
      "Epoch : 1271, batch :  1, loss :    8.18136, regression loss :   37.73046, sparse loss :    4.89812, time : 2634.01151.\n",
      "Epoch : 1272, batch :  1, loss :    8.17887, regression loss :   37.72936, sparse loss :    4.89549, time : 2635.73369.\n",
      "Epoch : 1273, batch :  1, loss :    8.17640, regression loss :   37.72820, sparse loss :    4.89286, time : 2637.43365.\n",
      "Epoch : 1274, batch :  1, loss :    8.17393, regression loss :   37.72711, sparse loss :    4.89024, time : 2639.05985.\n",
      "Epoch : 1275, batch :  1, loss :    8.17146, regression loss :   37.72594, sparse loss :    4.88763, time : 2640.65341.\n",
      "Epoch : 1276, batch :  1, loss :    8.16901, regression loss :   37.72487, sparse loss :    4.88503, time : 2642.24547.\n",
      "Epoch : 1277, batch :  1, loss :    8.16656, regression loss :   37.72369, sparse loss :    4.88244, time : 2643.78831.\n",
      "Epoch : 1278, batch :  1, loss :    8.16413, regression loss :   37.72263, sparse loss :    4.87985, time : 2645.52435.\n",
      "Epoch : 1279, batch :  1, loss :    8.16170, regression loss :   37.72146, sparse loss :    4.87728, time : 2647.09426.\n",
      "Epoch : 1280, batch :  1, loss :    8.15927, regression loss :   37.72040, sparse loss :    4.87470, time : 2648.67051.\n",
      "Epoch : 1281, batch :  1, loss :    8.15686, regression loss :   37.71926, sparse loss :    4.87215, time : 2650.44058.\n",
      "Epoch : 1282, batch :  1, loss :    8.15445, regression loss :   37.71816, sparse loss :    4.86960, time : 2652.08824.\n",
      "Epoch : 1283, batch :  1, loss :    8.15205, regression loss :   37.71706, sparse loss :    4.86705, time : 2653.65029.\n",
      "Epoch : 1284, batch :  1, loss :    8.14966, regression loss :   37.71594, sparse loss :    4.86452, time : 2655.29782.\n",
      "Epoch : 1285, batch :  1, loss :    8.14728, regression loss :   37.71486, sparse loss :    4.86199, time : 2656.99427.\n",
      "Epoch : 1286, batch :  1, loss :    8.14490, regression loss :   37.71374, sparse loss :    4.85947, time : 2658.55189.\n",
      "Epoch : 1287, batch :  1, loss :    8.14253, regression loss :   37.71267, sparse loss :    4.85696, time : 2660.15390.\n",
      "Epoch : 1288, batch :  1, loss :    8.14017, regression loss :   37.71156, sparse loss :    4.85446, time : 2661.73795.\n",
      "Epoch : 1289, batch :  1, loss :    8.13782, regression loss :   37.71047, sparse loss :    4.85196, time : 2663.36321.\n",
      "Epoch : 1290, batch :  1, loss :    8.13547, regression loss :   37.70939, sparse loss :    4.84948, time : 2664.96834.\n",
      "Epoch : 1291, batch :  1, loss :    8.13313, regression loss :   37.70829, sparse loss :    4.84700, time : 2666.62966.\n",
      "Epoch : 1292, batch :  1, loss :    8.13080, regression loss :   37.70722, sparse loss :    4.84453, time : 2668.32275.\n",
      "Epoch : 1293, batch :  1, loss :    8.12847, regression loss :   37.70613, sparse loss :    4.84207, time : 2669.89929.\n",
      "Epoch : 1294, batch :  1, loss :    8.12616, regression loss :   37.70506, sparse loss :    4.83961, time : 2671.46490.\n",
      "Epoch : 1295, batch :  1, loss :    8.12385, regression loss :   37.70397, sparse loss :    4.83717, time : 2673.12906.\n",
      "Epoch : 1296, batch :  1, loss :    8.12155, regression loss :   37.70291, sparse loss :    4.83473, time : 2674.71552.\n",
      "Epoch : 1297, batch :  1, loss :    8.11925, regression loss :   37.70183, sparse loss :    4.83230, time : 2676.40378.\n",
      "Epoch : 1298, batch :  1, loss :    8.11696, regression loss :   37.70076, sparse loss :    4.82988, time : 2678.06862.\n",
      "Epoch : 1299, batch :  1, loss :    8.11468, regression loss :   37.69970, sparse loss :    4.82746, time : 2679.71568.\n",
      "Epoch : 1300, batch :  1, loss :    8.11241, regression loss :   37.69863, sparse loss :    4.82506, time : 2684.46761.\n",
      "Epoch : 1301, batch :  1, loss :    8.11015, regression loss :   37.69758, sparse loss :    4.82266, time : 2686.77791.\n",
      "Epoch : 1302, batch :  1, loss :    8.10789, regression loss :   37.69651, sparse loss :    4.82026, time : 2689.03406.\n",
      "Epoch : 1303, batch :  1, loss :    8.10564, regression loss :   37.69546, sparse loss :    4.81788, time : 2691.19457.\n",
      "Epoch : 1304, batch :  1, loss :    8.10339, regression loss :   37.69440, sparse loss :    4.81550, time : 2693.43027.\n",
      "Epoch : 1305, batch :  1, loss :    8.10116, regression loss :   37.69335, sparse loss :    4.81314, time : 2695.80911.\n",
      "Epoch : 1306, batch :  1, loss :    8.09893, regression loss :   37.69229, sparse loss :    4.81078, time : 2698.16244.\n",
      "Epoch : 1307, batch :  1, loss :    8.09671, regression loss :   37.69126, sparse loss :    4.80842, time : 2700.84494.\n",
      "Epoch : 1308, batch :  1, loss :    8.09449, regression loss :   37.69020, sparse loss :    4.80608, time : 2703.07573.\n",
      "Epoch : 1309, batch :  1, loss :    8.09228, regression loss :   37.68917, sparse loss :    4.80374, time : 2705.39979.\n",
      "Epoch : 1310, batch :  1, loss :    8.09008, regression loss :   37.68812, sparse loss :    4.80141, time : 2707.89362.\n",
      "Epoch : 1311, batch :  1, loss :    8.08789, regression loss :   37.68709, sparse loss :    4.79908, time : 2710.25637.\n",
      "Epoch : 1312, batch :  1, loss :    8.08570, regression loss :   37.68605, sparse loss :    4.79677, time : 2712.52647.\n",
      "Epoch : 1313, batch :  1, loss :    8.08352, regression loss :   37.68502, sparse loss :    4.79446, time : 2714.73924.\n",
      "Epoch : 1314, batch :  1, loss :    8.08134, regression loss :   37.68399, sparse loss :    4.79216, time : 2716.91356.\n",
      "Epoch : 1315, batch :  1, loss :    8.07918, regression loss :   37.68297, sparse loss :    4.78987, time : 2719.10472.\n",
      "Epoch : 1316, batch :  1, loss :    8.07702, regression loss :   37.68195, sparse loss :    4.78758, time : 2721.40224.\n",
      "Epoch : 1317, batch :  1, loss :    8.07487, regression loss :   37.68092, sparse loss :    4.78530, time : 2723.79976.\n",
      "Epoch : 1318, batch :  1, loss :    8.07272, regression loss :   37.67990, sparse loss :    4.78303, time : 2726.17833.\n",
      "Epoch : 1319, batch :  1, loss :    8.07058, regression loss :   37.67889, sparse loss :    4.78077, time : 2728.56006.\n",
      "Epoch : 1320, batch :  1, loss :    8.06845, regression loss :   37.67787, sparse loss :    4.77851, time : 2730.73163.\n",
      "Epoch : 1321, batch :  1, loss :    8.06632, regression loss :   37.67686, sparse loss :    4.77626, time : 2732.98171.\n",
      "Epoch : 1322, batch :  1, loss :    8.06420, regression loss :   37.67585, sparse loss :    4.77402, time : 2735.14977.\n",
      "Epoch : 1323, batch :  1, loss :    8.06209, regression loss :   37.67485, sparse loss :    4.77179, time : 2737.41803.\n",
      "Epoch : 1324, batch :  1, loss :    8.05999, regression loss :   37.67385, sparse loss :    4.76956, time : 2739.69649.\n",
      "Epoch : 1325, batch :  1, loss :    8.05789, regression loss :   37.67285, sparse loss :    4.76734, time : 2741.98438.\n",
      "Epoch : 1326, batch :  1, loss :    8.05580, regression loss :   37.67185, sparse loss :    4.76512, time : 2744.18895.\n",
      "Epoch : 1327, batch :  1, loss :    8.05371, regression loss :   37.67085, sparse loss :    4.76292, time : 2746.45321.\n",
      "Epoch : 1328, batch :  1, loss :    8.05163, regression loss :   37.66987, sparse loss :    4.76072, time : 2748.64334.\n",
      "Epoch : 1329, batch :  1, loss :    8.04956, regression loss :   37.66886, sparse loss :    4.75853, time : 2750.86409.\n",
      "Epoch : 1330, batch :  1, loss :    8.04750, regression loss :   37.66791, sparse loss :    4.75634, time : 2753.01550.\n",
      "Epoch : 1331, batch :  1, loss :    8.04544, regression loss :   37.66689, sparse loss :    4.75416, time : 2755.25766.\n",
      "Epoch : 1332, batch :  1, loss :    8.04338, regression loss :   37.66594, sparse loss :    4.75199, time : 2757.88580.\n",
      "Epoch : 1333, batch :  1, loss :    8.04134, regression loss :   37.66494, sparse loss :    4.74983, time : 2760.21035.\n",
      "Epoch : 1334, batch :  1, loss :    8.03930, regression loss :   37.66397, sparse loss :    4.74767, time : 2762.66357.\n",
      "Epoch : 1335, batch :  1, loss :    8.03727, regression loss :   37.66301, sparse loss :    4.74552, time : 2764.86466.\n",
      "Epoch : 1336, batch :  1, loss :    8.03524, regression loss :   37.66203, sparse loss :    4.74338, time : 2767.11171.\n",
      "Epoch : 1337, batch :  1, loss :    8.03322, regression loss :   37.66108, sparse loss :    4.74124, time : 2769.39251.\n",
      "Epoch : 1338, batch :  1, loss :    8.03121, regression loss :   37.66011, sparse loss :    4.73911, time : 2771.71731.\n",
      "Epoch : 1339, batch :  1, loss :    8.02920, regression loss :   37.65915, sparse loss :    4.73699, time : 2773.99024.\n",
      "Epoch : 1340, batch :  1, loss :    8.02720, regression loss :   37.65820, sparse loss :    4.73487, time : 2776.25536.\n",
      "Epoch : 1341, batch :  1, loss :    8.02521, regression loss :   37.65724, sparse loss :    4.73276, time : 2778.49570.\n",
      "Epoch : 1342, batch :  1, loss :    8.02322, regression loss :   37.65629, sparse loss :    4.73065, time : 2780.93757.\n",
      "Epoch : 1343, batch :  1, loss :    8.02124, regression loss :   37.65535, sparse loss :    4.72856, time : 2783.14224.\n",
      "Epoch : 1344, batch :  1, loss :    8.01926, regression loss :   37.65440, sparse loss :    4.72647, time : 2785.37042.\n",
      "Epoch : 1345, batch :  1, loss :    8.01729, regression loss :   37.65347, sparse loss :    4.72439, time : 2787.76130.\n",
      "Epoch : 1346, batch :  1, loss :    8.01533, regression loss :   37.65253, sparse loss :    4.72231, time : 2790.07853.\n",
      "Epoch : 1347, batch :  1, loss :    8.01337, regression loss :   37.65159, sparse loss :    4.72024, time : 2792.32366.\n",
      "Epoch : 1348, batch :  1, loss :    8.01142, regression loss :   37.65067, sparse loss :    4.71817, time : 2794.60392.\n",
      "Epoch : 1349, batch :  1, loss :    8.00948, regression loss :   37.64974, sparse loss :    4.71612, time : 2797.15262.\n",
      "Epoch : 1350, batch :  1, loss :    8.00754, regression loss :   37.64881, sparse loss :    4.71407, time : 2799.50115.\n",
      "Epoch : 1351, batch :  1, loss :    8.00561, regression loss :   37.64791, sparse loss :    4.71202, time : 2801.68642.\n",
      "Epoch : 1352, batch :  1, loss :    8.00368, regression loss :   37.64698, sparse loss :    4.70999, time : 2804.08707.\n",
      "Epoch : 1353, batch :  1, loss :    8.00177, regression loss :   37.64608, sparse loss :    4.70795, time : 2806.37926.\n",
      "Epoch : 1354, batch :  1, loss :    7.99985, regression loss :   37.64516, sparse loss :    4.70593, time : 2808.72409.\n",
      "Epoch : 1355, batch :  1, loss :    7.99795, regression loss :   37.64426, sparse loss :    4.70391, time : 2811.01734.\n",
      "Epoch : 1356, batch :  1, loss :    7.99604, regression loss :   37.64337, sparse loss :    4.70190, time : 2813.28786.\n",
      "Epoch : 1357, batch :  1, loss :    7.99415, regression loss :   37.64245, sparse loss :    4.69989, time : 2815.51676.\n",
      "Epoch : 1358, batch :  1, loss :    7.99226, regression loss :   37.64158, sparse loss :    4.69789, time : 2817.81561.\n",
      "Epoch : 1359, batch :  1, loss :    7.99038, regression loss :   37.64067, sparse loss :    4.69590, time : 2820.18824.\n",
      "Epoch : 1360, batch :  1, loss :    7.98850, regression loss :   37.63979, sparse loss :    4.69391, time : 2822.48047.\n",
      "Epoch : 1361, batch :  1, loss :    7.98663, regression loss :   37.63892, sparse loss :    4.69193, time : 2824.83209.\n",
      "Epoch : 1362, batch :  1, loss :    7.98476, regression loss :   37.63802, sparse loss :    4.68996, time : 2827.13970.\n",
      "Epoch : 1363, batch :  1, loss :    7.98291, regression loss :   37.63716, sparse loss :    4.68799, time : 2829.32685.\n",
      "Epoch : 1364, batch :  1, loss :    7.98105, regression loss :   37.63628, sparse loss :    4.68603, time : 2831.71039.\n",
      "Epoch : 1365, batch :  1, loss :    7.97920, regression loss :   37.63542, sparse loss :    4.68407, time : 2833.99900.\n",
      "Epoch : 1366, batch :  1, loss :    7.97736, regression loss :   37.63456, sparse loss :    4.68212, time : 2836.27529.\n",
      "Epoch : 1367, batch :  1, loss :    7.97553, regression loss :   37.63369, sparse loss :    4.68018, time : 2838.54966.\n",
      "Epoch : 1368, batch :  1, loss :    7.97370, regression loss :   37.63285, sparse loss :    4.67824, time : 2840.72777.\n",
      "Epoch : 1369, batch :  1, loss :    7.97187, regression loss :   37.63198, sparse loss :    4.67631, time : 2843.13484.\n",
      "Epoch : 1370, batch :  1, loss :    7.97006, regression loss :   37.63115, sparse loss :    4.67438, time : 2845.52688.\n",
      "Epoch : 1371, batch :  1, loss :    7.96824, regression loss :   37.63029, sparse loss :    4.67246, time : 2848.07275.\n",
      "Epoch : 1372, batch :  1, loss :    7.96644, regression loss :   37.62947, sparse loss :    4.67054, time : 2850.40329.\n",
      "Epoch : 1373, batch :  1, loss :    7.96464, regression loss :   37.62862, sparse loss :    4.66864, time : 2852.83360.\n",
      "Epoch : 1374, batch :  1, loss :    7.96284, regression loss :   37.62780, sparse loss :    4.66673, time : 2855.11608.\n",
      "Epoch : 1375, batch :  1, loss :    7.96105, regression loss :   37.62697, sparse loss :    4.66484, time : 2857.36570.\n",
      "Epoch : 1376, batch :  1, loss :    7.95927, regression loss :   37.62616, sparse loss :    4.66295, time : 2859.54213.\n",
      "Epoch : 1377, batch :  1, loss :    7.95749, regression loss :   37.62533, sparse loss :    4.66106, time : 2861.72974.\n",
      "Epoch : 1378, batch :  1, loss :    7.95572, regression loss :   37.62453, sparse loss :    4.65918, time : 2863.97363.\n",
      "Epoch : 1379, batch :  1, loss :    7.95395, regression loss :   37.62371, sparse loss :    4.65731, time : 2866.16315.\n",
      "Epoch : 1380, batch :  1, loss :    7.95219, regression loss :   37.62292, sparse loss :    4.65544, time : 2868.45647.\n",
      "Epoch : 1381, batch :  1, loss :    7.95043, regression loss :   37.62210, sparse loss :    4.65358, time : 2870.72532.\n",
      "Epoch : 1382, batch :  1, loss :    7.94868, regression loss :   37.62134, sparse loss :    4.65172, time : 2872.89525.\n",
      "Epoch : 1383, batch :  1, loss :    7.94694, regression loss :   37.62049, sparse loss :    4.64988, time : 2875.35252.\n",
      "Epoch : 1384, batch :  1, loss :    7.94520, regression loss :   37.61979, sparse loss :    4.64802, time : 2877.65285.\n",
      "Epoch : 1385, batch :  1, loss :    7.94346, regression loss :   37.61892, sparse loss :    4.64619, time : 2880.23988.\n",
      "Epoch : 1386, batch :  1, loss :    7.94174, regression loss :   37.61821, sparse loss :    4.64435, time : 2882.61031.\n",
      "Epoch : 1387, batch :  1, loss :    7.94001, regression loss :   37.61742, sparse loss :    4.64252, time : 2884.98479.\n",
      "Epoch : 1388, batch :  1, loss :    7.93830, regression loss :   37.61660, sparse loss :    4.64071, time : 2887.38170.\n",
      "Epoch : 1389, batch :  1, loss :    7.93659, regression loss :   37.61595, sparse loss :    4.63888, time : 2889.62315.\n",
      "Epoch : 1390, batch :  1, loss :    7.93488, regression loss :   37.61506, sparse loss :    4.63708, time : 2891.94975.\n",
      "Epoch : 1391, batch :  1, loss :    7.93318, regression loss :   37.61443, sparse loss :    4.63526, time : 2894.58071.\n",
      "Epoch : 1392, batch :  1, loss :    7.93148, regression loss :   37.61361, sparse loss :    4.63347, time : 2896.85338.\n",
      "Epoch : 1393, batch :  1, loss :    7.92979, regression loss :   37.61287, sparse loss :    4.63167, time : 2899.13200.\n",
      "Epoch : 1394, batch :  1, loss :    7.92811, regression loss :   37.61219, sparse loss :    4.62988, time : 2901.80758.\n",
      "Epoch : 1395, batch :  1, loss :    7.92643, regression loss :   37.61138, sparse loss :    4.62810, time : 2904.51796.\n",
      "Epoch : 1396, batch :  1, loss :    7.92476, regression loss :   37.61070, sparse loss :    4.62632, time : 2906.89418.\n",
      "Epoch : 1397, batch :  1, loss :    7.92309, regression loss :   37.60998, sparse loss :    4.62455, time : 2909.23005.\n",
      "Epoch : 1398, batch :  1, loss :    7.92143, regression loss :   37.60922, sparse loss :    4.62278, time : 2911.54423.\n",
      "Epoch : 1399, batch :  1, loss :    7.91977, regression loss :   37.60856, sparse loss :    4.62101, time : 2914.27014.\n",
      "Epoch : 1400, batch :  1, loss :    7.91811, regression loss :   37.60782, sparse loss :    4.61926, time : 2919.29193.\n",
      "Epoch : 1401, batch :  1, loss :    7.91647, regression loss :   37.60712, sparse loss :    4.61751, time : 2920.91107.\n",
      "Epoch : 1402, batch :  1, loss :    7.91483, regression loss :   37.60645, sparse loss :    4.61576, time : 2922.58614.\n",
      "Epoch : 1403, batch :  1, loss :    7.91319, regression loss :   37.60572, sparse loss :    4.61402, time : 2924.29759.\n",
      "Epoch : 1404, batch :  1, loss :    7.91156, regression loss :   37.60505, sparse loss :    4.61228, time : 2925.97482.\n",
      "Epoch : 1405, batch :  1, loss :    7.90993, regression loss :   37.60438, sparse loss :    4.61055, time : 2927.75234.\n",
      "Epoch : 1406, batch :  1, loss :    7.90831, regression loss :   37.60367, sparse loss :    4.60882, time : 2929.47310.\n",
      "Epoch : 1407, batch :  1, loss :    7.90669, regression loss :   37.60303, sparse loss :    4.60710, time : 2931.17693.\n",
      "Epoch : 1408, batch :  1, loss :    7.90508, regression loss :   37.60235, sparse loss :    4.60538, time : 2932.91674.\n",
      "Epoch : 1409, batch :  1, loss :    7.90347, regression loss :   37.60169, sparse loss :    4.60367, time : 2934.66462.\n",
      "Epoch : 1410, batch :  1, loss :    7.90187, regression loss :   37.60105, sparse loss :    4.60197, time : 2936.42379.\n",
      "Epoch : 1411, batch :  1, loss :    7.90028, regression loss :   37.60037, sparse loss :    4.60027, time : 2938.14581.\n",
      "Epoch : 1412, batch :  1, loss :    7.89869, regression loss :   37.59974, sparse loss :    4.59857, time : 2939.81983.\n",
      "Epoch : 1413, batch :  1, loss :    7.89710, regression loss :   37.59911, sparse loss :    4.59688, time : 2941.55745.\n",
      "Epoch : 1414, batch :  1, loss :    7.89552, regression loss :   37.59846, sparse loss :    4.59520, time : 2943.19843.\n",
      "Epoch : 1415, batch :  1, loss :    7.89395, regression loss :   37.59785, sparse loss :    4.59351, time : 2944.81128.\n",
      "Epoch : 1416, batch :  1, loss :    7.89237, regression loss :   37.59721, sparse loss :    4.59184, time : 2946.44118.\n",
      "Epoch : 1417, batch :  1, loss :    7.89081, regression loss :   37.59659, sparse loss :    4.59017, time : 2948.06537.\n",
      "Epoch : 1418, batch :  1, loss :    7.88925, regression loss :   37.59599, sparse loss :    4.58850, time : 2949.86463.\n",
      "Epoch : 1419, batch :  1, loss :    7.88769, regression loss :   37.59537, sparse loss :    4.58684, time : 2951.54873.\n",
      "Epoch : 1420, batch :  1, loss :    7.88614, regression loss :   37.59478, sparse loss :    4.58518, time : 2953.19863.\n",
      "Epoch : 1421, batch :  1, loss :    7.88460, regression loss :   37.59418, sparse loss :    4.58353, time : 2954.84187.\n",
      "Epoch : 1422, batch :  1, loss :    7.88305, regression loss :   37.59358, sparse loss :    4.58189, time : 2956.43858.\n",
      "Epoch : 1423, batch :  1, loss :    7.88152, regression loss :   37.59301, sparse loss :    4.58024, time : 2958.03640.\n",
      "Epoch : 1424, batch :  1, loss :    7.87999, regression loss :   37.59242, sparse loss :    4.57861, time : 2959.81871.\n",
      "Epoch : 1425, batch :  1, loss :    7.87846, regression loss :   37.59185, sparse loss :    4.57697, time : 2961.49956.\n",
      "Epoch : 1426, batch :  1, loss :    7.87694, regression loss :   37.59128, sparse loss :    4.57535, time : 2963.22767.\n",
      "Epoch : 1427, batch :  1, loss :    7.87542, regression loss :   37.59071, sparse loss :    4.57372, time : 2964.86486.\n",
      "Epoch : 1428, batch :  1, loss :    7.87391, regression loss :   37.59016, sparse loss :    4.57211, time : 2966.56374.\n",
      "Epoch : 1429, batch :  1, loss :    7.87240, regression loss :   37.58960, sparse loss :    4.57049, time : 2968.28708.\n",
      "Epoch : 1430, batch :  1, loss :    7.87090, regression loss :   37.58906, sparse loss :    4.56888, time : 2969.92077.\n",
      "Epoch : 1431, batch :  1, loss :    7.86940, regression loss :   37.58851, sparse loss :    4.56728, time : 2971.60956.\n",
      "Epoch : 1432, batch :  1, loss :    7.86791, regression loss :   37.58799, sparse loss :    4.56568, time : 2973.35466.\n",
      "Epoch : 1433, batch :  1, loss :    7.86642, regression loss :   37.58744, sparse loss :    4.56409, time : 2974.99556.\n",
      "Epoch : 1434, batch :  1, loss :    7.86494, regression loss :   37.58693, sparse loss :    4.56250, time : 2976.74584.\n",
      "Epoch : 1435, batch :  1, loss :    7.86346, regression loss :   37.58641, sparse loss :    4.56091, time : 2978.35429.\n",
      "Epoch : 1436, batch :  1, loss :    7.86199, regression loss :   37.58589, sparse loss :    4.55933, time : 2980.08557.\n",
      "Epoch : 1437, batch :  1, loss :    7.86052, regression loss :   37.58539, sparse loss :    4.55775, time : 2981.71563.\n",
      "Epoch : 1438, batch :  1, loss :    7.85905, regression loss :   37.58487, sparse loss :    4.55618, time : 2983.35485.\n",
      "Epoch : 1439, batch :  1, loss :    7.85759, regression loss :   37.58440, sparse loss :    4.55462, time : 2984.98367.\n",
      "Epoch : 1440, batch :  1, loss :    7.85614, regression loss :   37.58387, sparse loss :    4.55306, time : 2986.70130.\n",
      "Epoch : 1441, batch :  1, loss :    7.85469, regression loss :   37.58343, sparse loss :    4.55149, time : 2988.29174.\n",
      "Epoch : 1442, batch :  1, loss :    7.85324, regression loss :   37.58290, sparse loss :    4.54995, time : 2990.05447.\n",
      "Epoch : 1443, batch :  1, loss :    7.85180, regression loss :   37.58247, sparse loss :    4.54839, time : 2991.77268.\n",
      "Epoch : 1444, batch :  1, loss :    7.85036, regression loss :   37.58195, sparse loss :    4.54685, time : 2993.45079.\n",
      "Epoch : 1445, batch :  1, loss :    7.84893, regression loss :   37.58154, sparse loss :    4.54531, time : 2995.16943.\n",
      "Epoch : 1446, batch :  1, loss :    7.84750, regression loss :   37.58102, sparse loss :    4.54378, time : 2996.83560.\n",
      "Epoch : 1447, batch :  1, loss :    7.84608, regression loss :   37.58065, sparse loss :    4.54224, time : 2998.59786.\n",
      "Epoch : 1448, batch :  1, loss :    7.84466, regression loss :   37.58010, sparse loss :    4.54072, time : 3000.27589.\n",
      "Epoch : 1449, batch :  1, loss :    7.84325, regression loss :   37.57977, sparse loss :    4.53919, time : 3001.88046.\n",
      "Epoch : 1450, batch :  1, loss :    7.84184, regression loss :   37.57920, sparse loss :    4.53768, time : 3003.57144.\n",
      "Epoch : 1451, batch :  1, loss :    7.84043, regression loss :   37.57891, sparse loss :    4.53616, time : 3005.33035.\n",
      "Epoch : 1452, batch :  1, loss :    7.83903, regression loss :   37.57835, sparse loss :    4.53466, time : 3006.99266.\n",
      "Epoch : 1453, batch :  1, loss :    7.83763, regression loss :   37.57804, sparse loss :    4.53314, time : 3008.63297.\n",
      "Epoch : 1454, batch :  1, loss :    7.83624, regression loss :   37.57753, sparse loss :    4.53165, time : 3010.35278.\n",
      "Epoch : 1455, batch :  1, loss :    7.83485, regression loss :   37.57717, sparse loss :    4.53015, time : 3012.06694.\n",
      "Epoch : 1456, batch :  1, loss :    7.83347, regression loss :   37.57676, sparse loss :    4.52866, time : 3013.69030.\n",
      "Epoch : 1457, batch :  1, loss :    7.83209, regression loss :   37.57633, sparse loss :    4.52718, time : 3015.31739.\n",
      "Epoch : 1458, batch :  1, loss :    7.83072, regression loss :   37.57598, sparse loss :    4.52569, time : 3017.01973.\n",
      "Epoch : 1459, batch :  1, loss :    7.82935, regression loss :   37.57554, sparse loss :    4.52421, time : 3018.74260.\n",
      "Epoch : 1460, batch :  1, loss :    7.82798, regression loss :   37.57519, sparse loss :    4.52273, time : 3020.57698.\n",
      "Epoch : 1461, batch :  1, loss :    7.82662, regression loss :   37.57479, sparse loss :    4.52127, time : 3022.27079.\n",
      "Epoch : 1462, batch :  1, loss :    7.82526, regression loss :   37.57441, sparse loss :    4.51980, time : 3023.96812.\n",
      "Epoch : 1463, batch :  1, loss :    7.82391, regression loss :   37.57407, sparse loss :    4.51833, time : 3025.55649.\n",
      "Epoch : 1464, batch :  1, loss :    7.82256, regression loss :   37.57366, sparse loss :    4.51688, time : 3027.44459.\n",
      "Epoch : 1465, batch :  1, loss :    7.82122, regression loss :   37.57334, sparse loss :    4.51542, time : 3029.19628.\n",
      "Epoch : 1466, batch :  1, loss :    7.81988, regression loss :   37.57296, sparse loss :    4.51398, time : 3030.92583.\n",
      "Epoch : 1467, batch :  1, loss :    7.81854, regression loss :   37.57260, sparse loss :    4.51253, time : 3032.70511.\n",
      "Epoch : 1468, batch :  1, loss :    7.81721, regression loss :   37.57230, sparse loss :    4.51109, time : 3034.52303.\n",
      "Epoch : 1469, batch :  1, loss :    7.81588, regression loss :   37.57190, sparse loss :    4.50966, time : 3036.27833.\n",
      "Epoch : 1470, batch :  1, loss :    7.81456, regression loss :   37.57162, sparse loss :    4.50822, time : 3038.20462.\n",
      "Epoch : 1471, batch :  1, loss :    7.81324, regression loss :   37.57126, sparse loss :    4.50679, time : 3039.90309.\n",
      "Epoch : 1472, batch :  1, loss :    7.81192, regression loss :   37.57093, sparse loss :    4.50537, time : 3041.65667.\n",
      "Epoch : 1473, batch :  1, loss :    7.81061, regression loss :   37.57064, sparse loss :    4.50394, time : 3043.29276.\n",
      "Epoch : 1474, batch :  1, loss :    7.80931, regression loss :   37.57027, sparse loss :    4.50253, time : 3044.95300.\n",
      "Epoch : 1475, batch :  1, loss :    7.80800, regression loss :   37.57001, sparse loss :    4.50112, time : 3046.70155.\n",
      "Epoch : 1476, batch :  1, loss :    7.80671, regression loss :   37.56967, sparse loss :    4.49971, time : 3048.51688.\n",
      "Epoch : 1477, batch :  1, loss :    7.80541, regression loss :   37.56938, sparse loss :    4.49831, time : 3050.20351.\n",
      "Epoch : 1478, batch :  1, loss :    7.80412, regression loss :   37.56909, sparse loss :    4.49690, time : 3051.74481.\n",
      "Epoch : 1479, batch :  1, loss :    7.80284, regression loss :   37.56876, sparse loss :    4.49551, time : 3053.44594.\n",
      "Epoch : 1480, batch :  1, loss :    7.80155, regression loss :   37.56851, sparse loss :    4.49411, time : 3055.24323.\n",
      "Epoch : 1481, batch :  1, loss :    7.80028, regression loss :   37.56819, sparse loss :    4.49273, time : 3056.95771.\n",
      "Epoch : 1482, batch :  1, loss :    7.79900, regression loss :   37.56794, sparse loss :    4.49134, time : 3058.60726.\n",
      "Epoch : 1483, batch :  1, loss :    7.79773, regression loss :   37.56764, sparse loss :    4.48997, time : 3060.19505.\n",
      "Epoch : 1484, batch :  1, loss :    7.79647, regression loss :   37.56738, sparse loss :    4.48859, time : 3061.87832.\n",
      "Epoch : 1485, batch :  1, loss :    7.79521, regression loss :   37.56712, sparse loss :    4.48722, time : 3063.61552.\n",
      "Epoch : 1486, batch :  1, loss :    7.79395, regression loss :   37.56684, sparse loss :    4.48585, time : 3065.21633.\n",
      "Epoch : 1487, batch :  1, loss :    7.79270, regression loss :   37.56660, sparse loss :    4.48448, time : 3066.78128.\n",
      "Epoch : 1488, batch :  1, loss :    7.79145, regression loss :   37.56633, sparse loss :    4.48313, time : 3068.55255.\n",
      "Epoch : 1489, batch :  1, loss :    7.79020, regression loss :   37.56609, sparse loss :    4.48177, time : 3070.28876.\n",
      "Epoch : 1490, batch :  1, loss :    7.78896, regression loss :   37.56583, sparse loss :    4.48042, time : 3071.89800.\n",
      "Epoch : 1491, batch :  1, loss :    7.78772, regression loss :   37.56560, sparse loss :    4.47907, time : 3073.63375.\n",
      "Epoch : 1492, batch :  1, loss :    7.78649, regression loss :   37.56535, sparse loss :    4.47772, time : 3075.37019.\n",
      "Epoch : 1493, batch :  1, loss :    7.78526, regression loss :   37.56512, sparse loss :    4.47638, time : 3077.08845.\n",
      "Epoch : 1494, batch :  1, loss :    7.78403, regression loss :   37.56489, sparse loss :    4.47505, time : 3078.78916.\n",
      "Epoch : 1495, batch :  1, loss :    7.78281, regression loss :   37.56467, sparse loss :    4.47371, time : 3080.56301.\n",
      "Epoch : 1496, batch :  1, loss :    7.78159, regression loss :   37.56443, sparse loss :    4.47238, time : 3082.28990.\n",
      "Epoch : 1497, batch :  1, loss :    7.78037, regression loss :   37.56423, sparse loss :    4.47106, time : 3083.97855.\n",
      "Epoch : 1498, batch :  1, loss :    7.77916, regression loss :   37.56399, sparse loss :    4.46974, time : 3085.69102.\n",
      "Epoch : 1499, batch :  1, loss :    7.77796, regression loss :   37.56382, sparse loss :    4.46842, time : 3087.40599.\n",
      "Epoch : 1500, batch :  1, loss :    7.77675, regression loss :   37.56356, sparse loss :    4.46711, time : 3091.64167.\n",
      "Epoch : 1501, batch :  1, loss :    7.77555, regression loss :   37.56341, sparse loss :    4.46579, time : 3093.23580.\n",
      "Epoch : 1502, batch :  1, loss :    7.77436, regression loss :   37.56317, sparse loss :    4.46449, time : 3094.81114.\n",
      "Epoch : 1503, batch :  1, loss :    7.77317, regression loss :   37.56298, sparse loss :    4.46319, time : 3096.49965.\n",
      "Epoch : 1504, batch :  1, loss :    7.77198, regression loss :   37.56282, sparse loss :    4.46189, time : 3098.09214.\n",
      "Epoch : 1505, batch :  1, loss :    7.77079, regression loss :   37.56255, sparse loss :    4.46060, time : 3099.70669.\n",
      "Epoch : 1506, batch :  1, loss :    7.76961, regression loss :   37.56248, sparse loss :    4.45930, time : 3101.30326.\n",
      "Epoch : 1507, batch :  1, loss :    7.76844, regression loss :   37.56217, sparse loss :    4.45802, time : 3102.91706.\n",
      "Epoch : 1508, batch :  1, loss :    7.76726, regression loss :   37.56210, sparse loss :    4.45673, time : 3104.53420.\n",
      "Epoch : 1509, batch :  1, loss :    7.76610, regression loss :   37.56186, sparse loss :    4.45546, time : 3106.23992.\n",
      "Epoch : 1510, batch :  1, loss :    7.76493, regression loss :   37.56171, sparse loss :    4.45418, time : 3107.86639.\n",
      "Epoch : 1511, batch :  1, loss :    7.76377, regression loss :   37.56154, sparse loss :    4.45290, time : 3109.49584.\n",
      "Epoch : 1512, batch :  1, loss :    7.76261, regression loss :   37.56137, sparse loss :    4.45164, time : 3111.15505.\n",
      "Epoch : 1513, batch :  1, loss :    7.76146, regression loss :   37.56120, sparse loss :    4.45037, time : 3112.74266.\n",
      "Epoch : 1514, batch :  1, loss :    7.76030, regression loss :   37.56106, sparse loss :    4.44911, time : 3114.46314.\n",
      "Epoch : 1515, batch :  1, loss :    7.75916, regression loss :   37.56088, sparse loss :    4.44786, time : 3116.14964.\n",
      "Epoch : 1516, batch :  1, loss :    7.75801, regression loss :   37.56074, sparse loss :    4.44660, time : 3117.84556.\n",
      "Epoch : 1517, batch :  1, loss :    7.75687, regression loss :   37.56060, sparse loss :    4.44535, time : 3119.56989.\n",
      "Epoch : 1518, batch :  1, loss :    7.75574, regression loss :   37.56041, sparse loss :    4.44411, time : 3121.16196.\n",
      "Epoch : 1519, batch :  1, loss :    7.75461, regression loss :   37.56033, sparse loss :    4.44286, time : 3122.76830.\n",
      "Epoch : 1520, batch :  1, loss :    7.75348, regression loss :   37.56012, sparse loss :    4.44163, time : 3124.49335.\n",
      "Epoch : 1521, batch :  1, loss :    7.75235, regression loss :   37.56004, sparse loss :    4.44038, time : 3126.10869.\n",
      "Epoch : 1522, batch :  1, loss :    7.75123, regression loss :   37.55987, sparse loss :    4.43916, time : 3127.78407.\n",
      "Epoch : 1523, batch :  1, loss :    7.75011, regression loss :   37.55975, sparse loss :    4.43793, time : 3129.51207.\n",
      "Epoch : 1524, batch :  1, loss :    7.74899, regression loss :   37.55962, sparse loss :    4.43670, time : 3131.22635.\n",
      "Epoch : 1525, batch :  1, loss :    7.74788, regression loss :   37.55949, sparse loss :    4.43548, time : 3132.95198.\n",
      "Epoch : 1526, batch :  1, loss :    7.74677, regression loss :   37.55938, sparse loss :    4.43426, time : 3134.56891.\n",
      "Epoch : 1527, batch :  1, loss :    7.74567, regression loss :   37.55923, sparse loss :    4.43305, time : 3136.26779.\n",
      "Epoch : 1528, batch :  1, loss :    7.74457, regression loss :   37.55915, sparse loss :    4.43184, time : 3137.94551.\n",
      "Epoch : 1529, batch :  1, loss :    7.74347, regression loss :   37.55899, sparse loss :    4.43064, time : 3139.80036.\n",
      "Epoch : 1530, batch :  1, loss :    7.74238, regression loss :   37.55892, sparse loss :    4.42943, time : 3141.49175.\n",
      "Epoch : 1531, batch :  1, loss :    7.74129, regression loss :   37.55876, sparse loss :    4.42824, time : 3143.09304.\n",
      "Epoch : 1532, batch :  1, loss :    7.74020, regression loss :   37.55870, sparse loss :    4.42703, time : 3144.76626.\n",
      "Epoch : 1533, batch :  1, loss :    7.73912, regression loss :   37.55855, sparse loss :    4.42585, time : 3146.35432.\n",
      "Epoch : 1534, batch :  1, loss :    7.73804, regression loss :   37.55849, sparse loss :    4.42465, time : 3148.04368.\n",
      "Epoch : 1535, batch :  1, loss :    7.73696, regression loss :   37.55835, sparse loss :    4.42347, time : 3149.66482.\n",
      "Epoch : 1536, batch :  1, loss :    7.73589, regression loss :   37.55829, sparse loss :    4.42229, time : 3151.31616.\n",
      "Epoch : 1537, batch :  1, loss :    7.73482, regression loss :   37.55816, sparse loss :    4.42111, time : 3152.90286.\n",
      "Epoch : 1538, batch :  1, loss :    7.73375, regression loss :   37.55810, sparse loss :    4.41993, time : 3154.55396.\n",
      "Epoch : 1539, batch :  1, loss :    7.73269, regression loss :   37.55798, sparse loss :    4.41877, time : 3156.10842.\n",
      "Epoch : 1540, batch :  1, loss :    7.73163, regression loss :   37.55791, sparse loss :    4.41760, time : 3157.73241.\n",
      "Epoch : 1541, batch :  1, loss :    7.73057, regression loss :   37.55782, sparse loss :    4.41643, time : 3159.27418.\n",
      "Epoch : 1542, batch :  1, loss :    7.72952, regression loss :   37.55771, sparse loss :    4.41527, time : 3160.85922.\n",
      "Epoch : 1543, batch :  1, loss :    7.72847, regression loss :   37.55768, sparse loss :    4.41411, time : 3162.48225.\n",
      "Epoch : 1544, batch :  1, loss :    7.72742, regression loss :   37.55753, sparse loss :    4.41296, time : 3164.09518.\n",
      "Epoch : 1545, batch :  1, loss :    7.72638, regression loss :   37.55754, sparse loss :    4.41181, time : 3165.69211.\n",
      "Epoch : 1546, batch :  1, loss :    7.72534, regression loss :   37.55737, sparse loss :    4.41067, time : 3167.37103.\n",
      "Epoch : 1547, batch :  1, loss :    7.72430, regression loss :   37.55739, sparse loss :    4.40951, time : 3168.96261.\n",
      "Epoch : 1548, batch :  1, loss :    7.72327, regression loss :   37.55724, sparse loss :    4.40838, time : 3170.58213.\n",
      "Epoch : 1549, batch :  1, loss :    7.72224, regression loss :   37.55723, sparse loss :    4.40724, time : 3172.21001.\n",
      "Epoch : 1550, batch :  1, loss :    7.72121, regression loss :   37.55712, sparse loss :    4.40611, time : 3173.76880.\n",
      "Epoch : 1551, batch :  1, loss :    7.72019, regression loss :   37.55707, sparse loss :    4.40498, time : 3175.36174.\n",
      "Epoch : 1552, batch :  1, loss :    7.71917, regression loss :   37.55700, sparse loss :    4.40385, time : 3176.97848.\n",
      "Epoch : 1553, batch :  1, loss :    7.71815, regression loss :   37.55695, sparse loss :    4.40273, time : 3178.54824.\n",
      "Epoch : 1554, batch :  1, loss :    7.71714, regression loss :   37.55688, sparse loss :    4.40161, time : 3180.11786.\n",
      "Epoch : 1555, batch :  1, loss :    7.71613, regression loss :   37.55684, sparse loss :    4.40049, time : 3181.68514.\n",
      "Epoch : 1556, batch :  1, loss :    7.71512, regression loss :   37.55675, sparse loss :    4.39938, time : 3183.28196.\n",
      "Epoch : 1557, batch :  1, loss :    7.71411, regression loss :   37.55674, sparse loss :    4.39827, time : 3184.88176.\n",
      "Epoch : 1558, batch :  1, loss :    7.71311, regression loss :   37.55664, sparse loss :    4.39717, time : 3186.54409.\n",
      "Epoch : 1559, batch :  1, loss :    7.71211, regression loss :   37.55663, sparse loss :    4.39606, time : 3188.08736.\n",
      "Epoch : 1560, batch :  1, loss :    7.71112, regression loss :   37.55656, sparse loss :    4.39496, time : 3189.92852.\n",
      "Epoch : 1561, batch :  1, loss :    7.71013, regression loss :   37.55651, sparse loss :    4.39386, time : 3191.60201.\n",
      "Epoch : 1562, batch :  1, loss :    7.70914, regression loss :   37.55649, sparse loss :    4.39277, time : 3193.48091.\n",
      "Epoch : 1563, batch :  1, loss :    7.70815, regression loss :   37.55641, sparse loss :    4.39168, time : 3195.25956.\n",
      "Epoch : 1564, batch :  1, loss :    7.70717, regression loss :   37.55641, sparse loss :    4.39059, time : 3196.96329.\n",
      "Epoch : 1565, batch :  1, loss :    7.70619, regression loss :   37.55633, sparse loss :    4.38951, time : 3198.71738.\n",
      "Epoch : 1566, batch :  1, loss :    7.70522, regression loss :   37.55633, sparse loss :    4.38843, time : 3200.31811.\n",
      "Epoch : 1567, batch :  1, loss :    7.70424, regression loss :   37.55627, sparse loss :    4.38735, time : 3201.97570.\n",
      "Epoch : 1568, batch :  1, loss :    7.70327, regression loss :   37.55623, sparse loss :    4.38628, time : 3203.66791.\n",
      "Epoch : 1569, batch :  1, loss :    7.70231, regression loss :   37.55621, sparse loss :    4.38521, time : 3205.32714.\n",
      "Epoch : 1570, batch :  1, loss :    7.70134, regression loss :   37.55616, sparse loss :    4.38414, time : 3207.02309.\n",
      "Epoch : 1571, batch :  1, loss :    7.70038, regression loss :   37.55616, sparse loss :    4.38307, time : 3208.60796.\n",
      "Epoch : 1572, batch :  1, loss :    7.69942, regression loss :   37.55610, sparse loss :    4.38202, time : 3210.31896.\n",
      "Epoch : 1573, batch :  1, loss :    7.69847, regression loss :   37.55609, sparse loss :    4.38096, time : 3211.94825.\n",
      "Epoch : 1574, batch :  1, loss :    7.69752, regression loss :   37.55605, sparse loss :    4.37990, time : 3213.71198.\n",
      "Epoch : 1575, batch :  1, loss :    7.69657, regression loss :   37.55603, sparse loss :    4.37885, time : 3215.41142.\n",
      "Epoch : 1576, batch :  1, loss :    7.69562, regression loss :   37.55601, sparse loss :    4.37780, time : 3217.14119.\n",
      "Epoch : 1577, batch :  1, loss :    7.69468, regression loss :   37.55598, sparse loss :    4.37676, time : 3218.88438.\n",
      "Epoch : 1578, batch :  1, loss :    7.69374, regression loss :   37.55597, sparse loss :    4.37572, time : 3220.51950.\n",
      "Epoch : 1579, batch :  1, loss :    7.69280, regression loss :   37.55593, sparse loss :    4.37468, time : 3222.11214.\n",
      "Epoch : 1580, batch :  1, loss :    7.69187, regression loss :   37.55594, sparse loss :    4.37364, time : 3223.64564.\n",
      "Epoch : 1581, batch :  1, loss :    7.69094, regression loss :   37.55589, sparse loss :    4.37261, time : 3225.31689.\n",
      "Epoch : 1582, batch :  1, loss :    7.69001, regression loss :   37.55592, sparse loss :    4.37158, time : 3227.00509.\n",
      "Epoch : 1583, batch :  1, loss :    7.68909, regression loss :   37.55585, sparse loss :    4.37056, time : 3228.72927.\n",
      "Epoch : 1584, batch :  1, loss :    7.68817, regression loss :   37.55589, sparse loss :    4.36953, time : 3230.35785.\n",
      "Epoch : 1585, batch :  1, loss :    7.68725, regression loss :   37.55582, sparse loss :    4.36852, time : 3232.08625.\n",
      "Epoch : 1586, batch :  1, loss :    7.68633, regression loss :   37.55587, sparse loss :    4.36749, time : 3233.88763.\n",
      "Epoch : 1587, batch :  1, loss :    7.68542, regression loss :   37.55580, sparse loss :    4.36649, time : 3235.65322.\n",
      "Epoch : 1588, batch :  1, loss :    7.68451, regression loss :   37.55585, sparse loss :    4.36547, time : 3237.33907.\n",
      "Epoch : 1589, batch :  1, loss :    7.68360, regression loss :   37.55578, sparse loss :    4.36447, time : 3239.16621.\n",
      "Epoch : 1590, batch :  1, loss :    7.68269, regression loss :   37.55584, sparse loss :    4.36346, time : 3240.88910.\n",
      "Epoch : 1591, batch :  1, loss :    7.68179, regression loss :   37.55575, sparse loss :    4.36246, time : 3242.59919.\n",
      "Epoch : 1592, batch :  1, loss :    7.68089, regression loss :   37.55585, sparse loss :    4.36145, time : 3244.30110.\n",
      "Epoch : 1593, batch :  1, loss :    7.68000, regression loss :   37.55573, sparse loss :    4.36047, time : 3244.77109.\n",
      "Epoch : 1594, batch :  1, loss :    7.67910, regression loss :   37.55583, sparse loss :    4.35947, time : 3246.41883.\n",
      "Epoch : 1595, batch :  1, loss :    7.67821, regression loss :   37.55576, sparse loss :    4.35849, time : 3248.04284.\n",
      "Epoch : 1596, batch :  1, loss :    7.67733, regression loss :   37.55578, sparse loss :    4.35750, time : 3249.78075.\n",
      "Epoch : 1597, batch :  1, loss :    7.67644, regression loss :   37.55580, sparse loss :    4.35651, time : 3251.49434.\n",
      "Epoch : 1598, batch :  1, loss :    7.67556, regression loss :   37.55576, sparse loss :    4.35554, time : 3253.20938.\n",
      "Epoch : 1599, batch :  1, loss :    7.67468, regression loss :   37.55579, sparse loss :    4.35456, time : 3254.94478.\n",
      "Epoch : 1600, batch :  1, loss :    7.67380, regression loss :   37.55579, sparse loss :    4.35358, time : 3259.14868.\n",
      "Epoch : 1601, batch :  1, loss :    7.67293, regression loss :   37.55578, sparse loss :    4.35261, time : 3260.75440.\n",
      "Epoch : 1602, batch :  1, loss :    7.67206, regression loss :   37.55579, sparse loss :    4.35164, time : 3262.49962.\n",
      "Epoch : 1603, batch :  1, loss :    7.67119, regression loss :   37.55580, sparse loss :    4.35068, time : 3264.19330.\n",
      "Epoch : 1604, batch :  1, loss :    7.67033, regression loss :   37.55579, sparse loss :    4.34972, time : 3265.88159.\n",
      "Epoch : 1605, batch :  1, loss :    7.66946, regression loss :   37.55579, sparse loss :    4.34876, time : 3267.43170.\n",
      "Epoch : 1606, batch :  1, loss :    7.66860, regression loss :   37.55583, sparse loss :    4.34780, time : 3269.13494.\n",
      "Epoch : 1607, batch :  1, loss :    7.66774, regression loss :   37.55579, sparse loss :    4.34685, time : 3270.88117.\n",
      "Epoch : 1608, batch :  1, loss :    7.66689, regression loss :   37.55585, sparse loss :    4.34589, time : 3272.70766.\n",
      "Epoch : 1609, batch :  1, loss :    7.66604, regression loss :   37.55581, sparse loss :    4.34495, time : 3274.45016.\n",
      "Epoch : 1610, batch :  1, loss :    7.66519, regression loss :   37.55585, sparse loss :    4.34401, time : 3276.12534.\n",
      "Epoch : 1611, batch :  1, loss :    7.66434, regression loss :   37.55585, sparse loss :    4.34306, time : 3277.84143.\n",
      "Epoch : 1612, batch :  1, loss :    7.66350, regression loss :   37.55586, sparse loss :    4.34213, time : 3279.51858.\n",
      "Epoch : 1613, batch :  1, loss :    7.66266, regression loss :   37.55586, sparse loss :    4.34119, time : 3281.29372.\n",
      "Epoch : 1614, batch :  1, loss :    7.66182, regression loss :   37.55589, sparse loss :    4.34026, time : 3282.99405.\n",
      "Epoch : 1615, batch :  1, loss :    7.66098, regression loss :   37.55590, sparse loss :    4.33933, time : 3284.66355.\n",
      "Epoch : 1616, batch :  1, loss :    7.66015, regression loss :   37.55588, sparse loss :    4.33840, time : 3286.32616.\n",
      "Epoch : 1617, batch :  1, loss :    7.65932, regression loss :   37.55595, sparse loss :    4.33747, time : 3288.04120.\n",
      "Epoch : 1618, batch :  1, loss :    7.65849, regression loss :   37.55588, sparse loss :    4.33656, time : 3289.72312.\n",
      "Epoch : 1619, batch :  1, loss :    7.65767, regression loss :   37.55598, sparse loss :    4.33563, time : 3291.53440.\n",
      "Epoch : 1620, batch :  1, loss :    7.65684, regression loss :   37.55592, sparse loss :    4.33472, time : 3293.24786.\n",
      "Epoch : 1621, batch :  1, loss :    7.65602, regression loss :   37.55599, sparse loss :    4.33381, time : 3294.77610.\n",
      "Epoch : 1622, batch :  1, loss :    7.65521, regression loss :   37.55596, sparse loss :    4.33290, time : 3296.35662.\n",
      "Epoch : 1623, batch :  1, loss :    7.65439, regression loss :   37.55601, sparse loss :    4.33199, time : 3297.92081.\n",
      "Epoch : 1624, batch :  1, loss :    7.65358, regression loss :   37.55598, sparse loss :    4.33109, time : 3299.51189.\n",
      "Epoch : 1625, batch :  1, loss :    7.65277, regression loss :   37.55604, sparse loss :    4.33018, time : 3301.15931.\n",
      "Epoch : 1626, batch :  1, loss :    7.65196, regression loss :   37.55602, sparse loss :    4.32929, time : 3302.78490.\n",
      "Epoch : 1627, batch :  1, loss :    7.65116, regression loss :   37.55607, sparse loss :    4.32839, time : 3304.41438.\n",
      "Epoch : 1628, batch :  1, loss :    7.65035, regression loss :   37.55605, sparse loss :    4.32750, time : 3306.09091.\n",
      "Epoch : 1629, batch :  1, loss :    7.64956, regression loss :   37.55609, sparse loss :    4.32661, time : 3307.68285.\n",
      "Epoch : 1630, batch :  1, loss :    7.64876, regression loss :   37.55609, sparse loss :    4.32572, time : 3309.35110.\n",
      "Epoch : 1631, batch :  1, loss :    7.64796, regression loss :   37.55612, sparse loss :    4.32484, time : 3311.02041.\n",
      "Epoch : 1632, batch :  1, loss :    7.64717, regression loss :   37.55611, sparse loss :    4.32396, time : 3312.71388.\n",
      "Epoch : 1633, batch :  1, loss :    7.64638, regression loss :   37.55616, sparse loss :    4.32307, time : 3314.42931.\n",
      "Epoch : 1634, batch :  1, loss :    7.64559, regression loss :   37.55614, sparse loss :    4.32220, time : 3316.09422.\n",
      "Epoch : 1635, batch :  1, loss :    7.64481, regression loss :   37.55619, sparse loss :    4.32132, time : 3317.81489.\n",
      "Epoch : 1636, batch :  1, loss :    7.64403, regression loss :   37.55616, sparse loss :    4.32046, time : 3319.50314.\n",
      "Epoch : 1637, batch :  1, loss :    7.64325, regression loss :   37.55623, sparse loss :    4.31958, time : 3321.19441.\n",
      "Epoch : 1638, batch :  1, loss :    7.64247, regression loss :   37.55618, sparse loss :    4.31873, time : 3322.89193.\n",
      "Epoch : 1639, batch :  1, loss :    7.64170, regression loss :   37.55626, sparse loss :    4.31786, time : 3324.46862.\n",
      "Epoch : 1640, batch :  1, loss :    7.64092, regression loss :   37.55620, sparse loss :    4.31701, time : 3326.19341.\n",
      "Epoch : 1641, batch :  1, loss :    7.64015, regression loss :   37.55629, sparse loss :    4.31614, time : 3327.85446.\n",
      "Epoch : 1642, batch :  1, loss :    7.63939, regression loss :   37.55625, sparse loss :    4.31529, time : 3329.54087.\n",
      "Epoch : 1643, batch :  1, loss :    7.63862, regression loss :   37.55629, sparse loss :    4.31444, time : 3331.24913.\n",
      "Epoch : 1644, batch :  1, loss :    7.63786, regression loss :   37.55631, sparse loss :    4.31359, time : 3333.02367.\n",
      "Epoch : 1645, batch :  1, loss :    7.63710, regression loss :   37.55629, sparse loss :    4.31274, time : 3334.74433.\n",
      "Epoch : 1646, batch :  1, loss :    7.63634, regression loss :   37.55635, sparse loss :    4.31190, time : 3336.38944.\n",
      "Epoch : 1647, batch :  1, loss :    7.63559, regression loss :   37.55632, sparse loss :    4.31106, time : 3338.00549.\n",
      "Epoch : 1648, batch :  1, loss :    7.63483, regression loss :   37.55636, sparse loss :    4.31022, time : 3339.73821.\n",
      "Epoch : 1649, batch :  1, loss :    7.63408, regression loss :   37.55635, sparse loss :    4.30939, time : 3341.43560.\n",
      "Epoch : 1650, batch :  1, loss :    7.63333, regression loss :   37.55638, sparse loss :    4.30855, time : 3343.18595.\n",
      "Epoch : 1651, batch :  1, loss :    7.63259, regression loss :   37.55638, sparse loss :    4.30772, time : 3345.07238.\n",
      "Epoch : 1652, batch :  1, loss :    7.63185, regression loss :   37.55640, sparse loss :    4.30690, time : 3346.72905.\n",
      "Epoch : 1653, batch :  1, loss :    7.63110, regression loss :   37.55641, sparse loss :    4.30607, time : 3348.44366.\n",
      "Epoch : 1654, batch :  1, loss :    7.63037, regression loss :   37.55642, sparse loss :    4.30525, time : 3350.03509.\n",
      "Epoch : 1655, batch :  1, loss :    7.62963, regression loss :   37.55642, sparse loss :    4.30443, time : 3351.77598.\n",
      "Epoch : 1656, batch :  1, loss :    7.62889, regression loss :   37.55643, sparse loss :    4.30361, time : 3353.46537.\n",
      "Epoch : 1657, batch :  1, loss :    7.62816, regression loss :   37.55645, sparse loss :    4.30280, time : 3355.20422.\n",
      "Epoch : 1658, batch :  1, loss :    7.62743, regression loss :   37.55644, sparse loss :    4.30199, time : 3356.81691.\n",
      "Epoch : 1659, batch :  1, loss :    7.62671, regression loss :   37.55647, sparse loss :    4.30118, time : 3358.39928.\n",
      "Epoch : 1660, batch :  1, loss :    7.62598, regression loss :   37.55646, sparse loss :    4.30037, time : 3360.03799.\n",
      "Epoch : 1661, batch :  1, loss :    7.62526, regression loss :   37.55648, sparse loss :    4.29957, time : 3361.67781.\n",
      "Epoch : 1662, batch :  1, loss :    7.62454, regression loss :   37.55647, sparse loss :    4.29877, time : 3363.37352.\n",
      "Epoch : 1663, batch :  1, loss :    7.62382, regression loss :   37.55649, sparse loss :    4.29797, time : 3364.99601.\n",
      "Epoch : 1664, batch :  1, loss :    7.62310, regression loss :   37.55649, sparse loss :    4.29717, time : 3366.56417.\n",
      "Epoch : 1665, batch :  1, loss :    7.62239, regression loss :   37.55649, sparse loss :    4.29638, time : 3368.26065.\n",
      "Epoch : 1666, batch :  1, loss :    7.62168, regression loss :   37.55650, sparse loss :    4.29559, time : 3369.95815.\n",
      "Epoch : 1667, batch :  1, loss :    7.62097, regression loss :   37.55650, sparse loss :    4.29480, time : 3371.52654.\n",
      "Epoch : 1668, batch :  1, loss :    7.62026, regression loss :   37.55649, sparse loss :    4.29402, time : 3373.13605.\n",
      "Epoch : 1669, batch :  1, loss :    7.61956, regression loss :   37.55653, sparse loss :    4.29323, time : 3374.88408.\n",
      "Epoch : 1670, batch :  1, loss :    7.61886, regression loss :   37.55645, sparse loss :    4.29246, time : 3376.61814.\n",
      "Epoch : 1671, batch :  1, loss :    7.61816, regression loss :   37.55658, sparse loss :    4.29166, time : 3378.36701.\n",
      "Epoch : 1672, batch :  1, loss :    7.61746, regression loss :   37.55642, sparse loss :    4.29091, time : 3379.98608.\n",
      "Epoch : 1673, batch :  1, loss :    7.61676, regression loss :   37.55657, sparse loss :    4.29012, time : 3381.89455.\n",
      "Epoch : 1674, batch :  1, loss :    7.61607, regression loss :   37.55646, sparse loss :    4.28936, time : 3383.59535.\n",
      "Epoch : 1675, batch :  1, loss :    7.61538, regression loss :   37.55650, sparse loss :    4.28859, time : 3385.24326.\n",
      "Epoch : 1676, batch :  1, loss :    7.61469, regression loss :   37.55653, sparse loss :    4.28782, time : 3386.88960.\n",
      "Epoch : 1677, batch :  1, loss :    7.61400, regression loss :   37.55643, sparse loss :    4.28707, time : 3388.72861.\n",
      "Epoch : 1678, batch :  1, loss :    7.61332, regression loss :   37.55655, sparse loss :    4.28629, time : 3390.47306.\n",
      "Epoch : 1679, batch :  1, loss :    7.61263, regression loss :   37.55643, sparse loss :    4.28555, time : 3392.21324.\n",
      "Epoch : 1680, batch :  1, loss :    7.61195, regression loss :   37.55649, sparse loss :    4.28478, time : 3393.82207.\n",
      "Epoch : 1681, batch :  1, loss :    7.61128, regression loss :   37.55648, sparse loss :    4.28403, time : 3395.49768.\n",
      "Epoch : 1682, batch :  1, loss :    7.61060, regression loss :   37.55643, sparse loss :    4.28329, time : 3397.09012.\n",
      "Epoch : 1683, batch :  1, loss :    7.60993, regression loss :   37.55649, sparse loss :    4.28253, time : 3398.71352.\n",
      "Epoch : 1684, batch :  1, loss :    7.60925, regression loss :   37.55640, sparse loss :    4.28179, time : 3400.31861.\n",
      "Epoch : 1685, batch :  1, loss :    7.60858, regression loss :   37.55645, sparse loss :    4.28104, time : 3401.97881.\n",
      "Epoch : 1686, batch :  1, loss :    7.60792, regression loss :   37.55642, sparse loss :    4.28030, time : 3403.68515.\n",
      "Epoch : 1687, batch :  1, loss :    7.60725, regression loss :   37.55639, sparse loss :    4.27957, time : 3405.39193.\n",
      "Epoch : 1688, batch :  1, loss :    7.60659, regression loss :   37.55642, sparse loss :    4.27883, time : 3406.95533.\n",
      "Epoch : 1689, batch :  1, loss :    7.60593, regression loss :   37.55635, sparse loss :    4.27810, time : 3408.67755.\n",
      "Epoch : 1690, batch :  1, loss :    7.60527, regression loss :   37.55639, sparse loss :    4.27736, time : 3410.35440.\n",
      "Epoch : 1691, batch :  1, loss :    7.60461, regression loss :   37.55634, sparse loss :    4.27664, time : 3411.89451.\n",
      "Epoch : 1692, batch :  1, loss :    7.60395, regression loss :   37.55634, sparse loss :    4.27591, time : 3413.57847.\n",
      "Epoch : 1693, batch :  1, loss :    7.60330, regression loss :   37.55633, sparse loss :    4.27519, time : 3415.42659.\n",
      "Epoch : 1694, batch :  1, loss :    7.60265, regression loss :   37.55628, sparse loss :    4.27447, time : 3417.08313.\n",
      "Epoch : 1695, batch :  1, loss :    7.60200, regression loss :   37.55631, sparse loss :    4.27374, time : 3418.79228.\n",
      "Epoch : 1696, batch :  1, loss :    7.60135, regression loss :   37.55624, sparse loss :    4.27303, time : 3420.55020.\n",
      "Epoch : 1697, batch :  1, loss :    7.60071, regression loss :   37.55626, sparse loss :    4.27231, time : 3422.14908.\n",
      "Epoch : 1698, batch :  1, loss :    7.60007, regression loss :   37.55622, sparse loss :    4.27161, time : 3423.74981.\n",
      "Epoch : 1699, batch :  1, loss :    7.59942, regression loss :   37.55620, sparse loss :    4.27090, time : 3425.35488.\n",
      "Epoch : 1700, batch :  1, loss :    7.59879, regression loss :   37.55619, sparse loss :    4.27019, time : 3430.40840.\n",
      "Epoch : 1701, batch :  1, loss :    7.59815, regression loss :   37.55614, sparse loss :    4.26948, time : 3432.90310.\n",
      "Epoch : 1702, batch :  1, loss :    7.59752, regression loss :   37.55615, sparse loss :    4.26878, time : 3435.32280.\n",
      "Epoch : 1703, batch :  1, loss :    7.59688, regression loss :   37.55610, sparse loss :    4.26808, time : 3437.72089.\n",
      "Epoch : 1704, batch :  1, loss :    7.59625, regression loss :   37.55609, sparse loss :    4.26738, time : 3440.15590.\n",
      "Epoch : 1705, batch :  1, loss :    7.59562, regression loss :   37.55606, sparse loss :    4.26669, time : 3442.59625.\n",
      "Epoch : 1706, batch :  1, loss :    7.59500, regression loss :   37.55604, sparse loss :    4.26599, time : 3445.04690.\n",
      "Epoch : 1707, batch :  1, loss :    7.59437, regression loss :   37.55600, sparse loss :    4.26530, time : 3447.44490.\n",
      "Epoch : 1708, batch :  1, loss :    7.59375, regression loss :   37.55598, sparse loss :    4.26461, time : 3449.77767.\n",
      "Epoch : 1709, batch :  1, loss :    7.59313, regression loss :   37.55594, sparse loss :    4.26393, time : 3452.68936.\n",
      "Epoch : 1710, batch :  1, loss :    7.59251, regression loss :   37.55593, sparse loss :    4.26324, time : 3455.24893.\n",
      "Epoch : 1711, batch :  1, loss :    7.59189, regression loss :   37.55589, sparse loss :    4.26256, time : 3457.79443.\n",
      "Epoch : 1712, batch :  1, loss :    7.59128, regression loss :   37.55587, sparse loss :    4.26188, time : 3460.36991.\n",
      "Epoch : 1713, batch :  1, loss :    7.59066, regression loss :   37.55582, sparse loss :    4.26120, time : 3462.84444.\n",
      "Epoch : 1714, batch :  1, loss :    7.59005, regression loss :   37.55582, sparse loss :    4.26052, time : 3465.22621.\n",
      "Epoch : 1715, batch :  1, loss :    7.58944, regression loss :   37.55574, sparse loss :    4.25985, time : 3467.83839.\n",
      "Epoch : 1716, batch :  1, loss :    7.58884, regression loss :   37.55576, sparse loss :    4.25918, time : 3470.51036.\n",
      "Epoch : 1717, batch :  1, loss :    7.58823, regression loss :   37.55567, sparse loss :    4.25851, time : 3473.05585.\n",
      "Epoch : 1718, batch :  1, loss :    7.58763, regression loss :   37.55570, sparse loss :    4.25784, time : 3475.53140.\n",
      "Epoch : 1719, batch :  1, loss :    7.58703, regression loss :   37.55560, sparse loss :    4.25718, time : 3477.86644.\n",
      "Epoch : 1720, batch :  1, loss :    7.58643, regression loss :   37.55563, sparse loss :    4.25652, time : 3480.29562.\n",
      "Epoch : 1721, batch :  1, loss :    7.58583, regression loss :   37.55555, sparse loss :    4.25586, time : 3482.90159.\n",
      "Epoch : 1722, batch :  1, loss :    7.58523, regression loss :   37.55554, sparse loss :    4.25520, time : 3485.40083.\n",
      "Epoch : 1723, batch :  1, loss :    7.58464, regression loss :   37.55550, sparse loss :    4.25454, time : 3487.84484.\n",
      "Epoch : 1724, batch :  1, loss :    7.58405, regression loss :   37.55545, sparse loss :    4.25389, time : 3490.28791.\n",
      "Epoch : 1725, batch :  1, loss :    7.58346, regression loss :   37.55544, sparse loss :    4.25324, time : 3492.76395.\n",
      "Epoch : 1726, batch :  1, loss :    7.58287, regression loss :   37.55537, sparse loss :    4.25259, time : 3495.15101.\n",
      "Epoch : 1727, batch :  1, loss :    7.58228, regression loss :   37.55536, sparse loss :    4.25194, time : 3497.48373.\n",
      "Epoch : 1728, batch :  1, loss :    7.58170, regression loss :   37.55530, sparse loss :    4.25130, time : 3499.96922.\n",
      "Epoch : 1729, batch :  1, loss :    7.58111, regression loss :   37.55526, sparse loss :    4.25065, time : 3502.30642.\n",
      "Epoch : 1730, batch :  1, loss :    7.58053, regression loss :   37.55524, sparse loss :    4.25001, time : 3504.75271.\n",
      "Epoch : 1731, batch :  1, loss :    7.57995, regression loss :   37.55518, sparse loss :    4.24937, time : 3507.17029.\n",
      "Epoch : 1732, batch :  1, loss :    7.57938, regression loss :   37.55515, sparse loss :    4.24873, time : 3509.60230.\n",
      "Epoch : 1733, batch :  1, loss :    7.57880, regression loss :   37.55512, sparse loss :    4.24810, time : 3512.06395.\n",
      "Epoch : 1734, batch :  1, loss :    7.57823, regression loss :   37.55504, sparse loss :    4.24747, time : 3514.51378.\n",
      "Epoch : 1735, batch :  1, loss :    7.57765, regression loss :   37.55508, sparse loss :    4.24683, time : 3516.98655.\n",
      "Epoch : 1736, batch :  1, loss :    7.57708, regression loss :   37.55491, sparse loss :    4.24622, time : 3519.28375.\n",
      "Epoch : 1737, batch :  1, loss :    7.57652, regression loss :   37.55507, sparse loss :    4.24557, time : 3521.64140.\n",
      "Epoch : 1738, batch :  1, loss :    7.57595, regression loss :   37.55477, sparse loss :    4.24497, time : 3524.43406.\n",
      "Epoch : 1739, batch :  1, loss :    7.57538, regression loss :   37.55499, sparse loss :    4.24432, time : 3527.07510.\n",
      "Epoch : 1740, batch :  1, loss :    7.57482, regression loss :   37.55476, sparse loss :    4.24372, time : 3529.53014.\n",
      "Epoch : 1741, batch :  1, loss :    7.57426, regression loss :   37.55478, sparse loss :    4.24309, time : 3532.06495.\n",
      "Epoch : 1742, batch :  1, loss :    7.57370, regression loss :   37.55480, sparse loss :    4.24247, time : 3534.49442.\n",
      "Epoch : 1743, batch :  1, loss :    7.57314, regression loss :   37.55461, sparse loss :    4.24187, time : 3536.89787.\n",
      "Epoch : 1744, batch :  1, loss :    7.57259, regression loss :   37.55470, sparse loss :    4.24124, time : 3539.39190.\n",
      "Epoch : 1745, batch :  1, loss :    7.57203, regression loss :   37.55465, sparse loss :    4.24063, time : 3541.77497.\n",
      "Epoch : 1746, batch :  1, loss :    7.57148, regression loss :   37.55450, sparse loss :    4.24004, time : 3544.26090.\n",
      "Epoch : 1747, batch :  1, loss :    7.57093, regression loss :   37.55455, sparse loss :    4.23942, time : 3546.63602.\n",
      "Epoch : 1748, batch :  1, loss :    7.57038, regression loss :   37.55455, sparse loss :    4.23881, time : 3549.17930.\n",
      "Epoch : 1749, batch :  1, loss :    7.56983, regression loss :   37.55437, sparse loss :    4.23822, time : 3552.03218.\n",
      "Epoch : 1750, batch :  1, loss :    7.56929, regression loss :   37.55440, sparse loss :    4.23761, time : 3554.54382.\n",
      "Epoch : 1751, batch :  1, loss :    7.56875, regression loss :   37.55441, sparse loss :    4.23701, time : 3556.91279.\n",
      "Epoch : 1752, batch :  1, loss :    7.56820, regression loss :   37.55427, sparse loss :    4.23642, time : 3559.39671.\n",
      "Epoch : 1753, batch :  1, loss :    7.56766, regression loss :   37.55426, sparse loss :    4.23582, time : 3561.84440.\n",
      "Epoch : 1754, batch :  1, loss :    7.56712, regression loss :   37.55427, sparse loss :    4.23522, time : 3564.28408.\n",
      "Epoch : 1755, batch :  1, loss :    7.56659, regression loss :   37.55417, sparse loss :    4.23463, time : 3566.77015.\n",
      "Epoch : 1756, batch :  1, loss :    7.56605, regression loss :   37.55414, sparse loss :    4.23404, time : 3569.16777.\n",
      "Epoch : 1757, batch :  1, loss :    7.56552, regression loss :   37.55411, sparse loss :    4.23345, time : 3571.84605.\n",
      "Epoch : 1758, batch :  1, loss :    7.56499, regression loss :   37.55406, sparse loss :    4.23287, time : 3574.36191.\n",
      "Epoch : 1759, batch :  1, loss :    7.56446, regression loss :   37.55403, sparse loss :    4.23228, time : 3576.82669.\n",
      "Epoch : 1760, batch :  1, loss :    7.56393, regression loss :   37.55395, sparse loss :    4.23170, time : 3579.32493.\n",
      "Epoch : 1761, batch :  1, loss :    7.56340, regression loss :   37.55393, sparse loss :    4.23112, time : 3581.85314.\n",
      "Epoch : 1762, batch :  1, loss :    7.56288, regression loss :   37.55389, sparse loss :    4.23054, time : 3584.34173.\n",
      "Epoch : 1763, batch :  1, loss :    7.56235, regression loss :   37.55386, sparse loss :    4.22996, time : 3586.81862.\n",
      "Epoch : 1764, batch :  1, loss :    7.56183, regression loss :   37.55375, sparse loss :    4.22940, time : 3589.18030.\n",
      "Epoch : 1765, batch :  1, loss :    7.56131, regression loss :   37.55381, sparse loss :    4.22881, time : 3591.74821.\n",
      "Epoch : 1766, batch :  1, loss :    7.56079, regression loss :   37.55371, sparse loss :    4.22825, time : 3594.22959.\n",
      "Epoch : 1767, batch :  1, loss :    7.56028, regression loss :   37.55365, sparse loss :    4.22768, time : 3596.65159.\n",
      "Epoch : 1768, batch :  1, loss :    7.55976, regression loss :   37.55365, sparse loss :    4.22711, time : 3599.08727.\n",
      "Epoch : 1769, batch :  1, loss :    7.55925, regression loss :   37.55360, sparse loss :    4.22654, time : 3601.56573.\n",
      "Epoch : 1770, batch :  1, loss :    7.55873, regression loss :   37.55353, sparse loss :    4.22598, time : 3604.00515.\n",
      "Epoch : 1771, batch :  1, loss :    7.55822, regression loss :   37.55350, sparse loss :    4.22542, time : 3606.46212.\n",
      "Epoch : 1772, batch :  1, loss :    7.55772, regression loss :   37.55348, sparse loss :    4.22485, time : 3608.92351.\n",
      "Epoch : 1773, batch :  1, loss :    7.55721, regression loss :   37.55341, sparse loss :    4.22430, time : 3611.65135.\n",
      "Epoch : 1774, batch :  1, loss :    7.55670, regression loss :   37.55339, sparse loss :    4.22374, time : 3614.13795.\n",
      "Epoch : 1775, batch :  1, loss :    7.55620, regression loss :   37.55333, sparse loss :    4.22318, time : 3616.55544.\n",
      "Epoch : 1776, batch :  1, loss :    7.55570, regression loss :   37.55330, sparse loss :    4.22263, time : 3619.17802.\n",
      "Epoch : 1777, batch :  1, loss :    7.55520, regression loss :   37.55327, sparse loss :    4.22208, time : 3621.77274.\n",
      "Epoch : 1778, batch :  1, loss :    7.55470, regression loss :   37.55319, sparse loss :    4.22153, time : 3624.22294.\n",
      "Epoch : 1779, batch :  1, loss :    7.55420, regression loss :   37.55319, sparse loss :    4.22098, time : 3626.77018.\n",
      "Epoch : 1780, batch :  1, loss :    7.55370, regression loss :   37.55314, sparse loss :    4.22043, time : 3629.23711.\n",
      "Epoch : 1781, batch :  1, loss :    7.55321, regression loss :   37.55307, sparse loss :    4.21989, time : 3631.73857.\n",
      "Epoch : 1782, batch :  1, loss :    7.55271, regression loss :   37.55307, sparse loss :    4.21934, time : 3634.13315.\n",
      "Epoch : 1783, batch :  1, loss :    7.55222, regression loss :   37.55302, sparse loss :    4.21880, time : 3636.81933.\n",
      "Epoch : 1784, batch :  1, loss :    7.55173, regression loss :   37.55297, sparse loss :    4.21826, time : 3639.26127.\n",
      "Epoch : 1785, batch :  1, loss :    7.55124, regression loss :   37.55294, sparse loss :    4.21772, time : 3641.71682.\n",
      "Epoch : 1786, batch :  1, loss :    7.55076, regression loss :   37.55290, sparse loss :    4.21719, time : 3644.19227.\n",
      "Epoch : 1787, batch :  1, loss :    7.55027, regression loss :   37.55286, sparse loss :    4.21665, time : 3646.83006.\n",
      "Epoch : 1788, batch :  1, loss :    7.54979, regression loss :   37.55281, sparse loss :    4.21612, time : 3649.35125.\n",
      "Epoch : 1789, batch :  1, loss :    7.54931, regression loss :   37.55278, sparse loss :    4.21559, time : 3651.86304.\n",
      "Epoch : 1790, batch :  1, loss :    7.54883, regression loss :   37.55274, sparse loss :    4.21506, time : 3654.51663.\n",
      "Epoch : 1791, batch :  1, loss :    7.54835, regression loss :   37.55269, sparse loss :    4.21453, time : 3657.03049.\n",
      "Epoch : 1792, batch :  1, loss :    7.54787, regression loss :   37.55266, sparse loss :    4.21400, time : 3659.69780.\n",
      "Epoch : 1793, batch :  1, loss :    7.54739, regression loss :   37.55263, sparse loss :    4.21348, time : 3662.20336.\n",
      "Epoch : 1794, batch :  1, loss :    7.54692, regression loss :   37.55258, sparse loss :    4.21296, time : 3664.67543.\n",
      "Epoch : 1795, batch :  1, loss :    7.54645, regression loss :   37.55254, sparse loss :    4.21244, time : 3667.16102.\n",
      "Epoch : 1796, batch :  1, loss :    7.54597, regression loss :   37.55251, sparse loss :    4.21191, time : 3669.94036.\n",
      "Epoch : 1797, batch :  1, loss :    7.54550, regression loss :   37.55247, sparse loss :    4.21140, time : 3672.50426.\n",
      "Epoch : 1798, batch :  1, loss :    7.54503, regression loss :   37.55243, sparse loss :    4.21088, time : 3674.88919.\n",
      "Epoch : 1799, batch :  1, loss :    7.54457, regression loss :   37.55239, sparse loss :    4.21037, time : 3677.53445.\n",
      "Epoch : 1800, batch :  1, loss :    7.54410, regression loss :   37.55236, sparse loss :    4.20985, time : 3682.76713.\n",
      "Epoch : 1801, batch :  1, loss :    7.54364, regression loss :   37.55232, sparse loss :    4.20934, time : 3684.53701.\n",
      "Epoch : 1802, batch :  1, loss :    7.54318, regression loss :   37.55228, sparse loss :    4.20883, time : 3686.30001.\n",
      "Epoch : 1803, batch :  1, loss :    7.54271, regression loss :   37.55225, sparse loss :    4.20832, time : 3687.86852.\n",
      "Epoch : 1804, batch :  1, loss :    7.54225, regression loss :   37.55220, sparse loss :    4.20782, time : 3689.50880.\n",
      "Epoch : 1805, batch :  1, loss :    7.54180, regression loss :   37.55218, sparse loss :    4.20731, time : 3691.14527.\n",
      "Epoch : 1806, batch :  1, loss :    7.54134, regression loss :   37.55213, sparse loss :    4.20681, time : 3692.73807.\n",
      "Epoch : 1807, batch :  1, loss :    7.54088, regression loss :   37.55210, sparse loss :    4.20630, time : 3694.49627.\n",
      "Epoch : 1808, batch :  1, loss :    7.54043, regression loss :   37.55206, sparse loss :    4.20580, time : 3696.30779.\n",
      "Epoch : 1809, batch :  1, loss :    7.53998, regression loss :   37.55203, sparse loss :    4.20530, time : 3697.97149.\n",
      "Epoch : 1810, batch :  1, loss :    7.53952, regression loss :   37.55199, sparse loss :    4.20481, time : 3699.63269.\n",
      "Epoch : 1811, batch :  1, loss :    7.53907, regression loss :   37.55195, sparse loss :    4.20431, time : 3701.36998.\n",
      "Epoch : 1812, batch :  1, loss :    7.53863, regression loss :   37.55192, sparse loss :    4.20382, time : 3703.15306.\n",
      "Epoch : 1813, batch :  1, loss :    7.53818, regression loss :   37.55188, sparse loss :    4.20332, time : 3704.82757.\n",
      "Epoch : 1814, batch :  1, loss :    7.53773, regression loss :   37.55185, sparse loss :    4.20283, time : 3706.57545.\n",
      "Epoch : 1815, batch :  1, loss :    7.53729, regression loss :   37.55182, sparse loss :    4.20234, time : 3708.22773.\n",
      "Epoch : 1816, batch :  1, loss :    7.53685, regression loss :   37.55177, sparse loss :    4.20186, time : 3710.10625.\n",
      "Epoch : 1817, batch :  1, loss :    7.53641, regression loss :   37.55175, sparse loss :    4.20137, time : 3711.79531.\n",
      "Epoch : 1818, batch :  1, loss :    7.53596, regression loss :   37.55169, sparse loss :    4.20088, time : 3713.57637.\n",
      "Epoch : 1819, batch :  1, loss :    7.53553, regression loss :   37.55168, sparse loss :    4.20040, time : 3715.24298.\n",
      "Epoch : 1820, batch :  1, loss :    7.53509, regression loss :   37.55163, sparse loss :    4.19992, time : 3717.04931.\n",
      "Epoch : 1821, batch :  1, loss :    7.53465, regression loss :   37.55161, sparse loss :    4.19944, time : 3718.86585.\n",
      "Epoch : 1822, batch :  1, loss :    7.53422, regression loss :   37.55156, sparse loss :    4.19896, time : 3720.56101.\n",
      "Epoch : 1823, batch :  1, loss :    7.53379, regression loss :   37.55155, sparse loss :    4.19848, time : 3722.19385.\n",
      "Epoch : 1824, batch :  1, loss :    7.53335, regression loss :   37.55149, sparse loss :    4.19801, time : 3723.94901.\n",
      "Epoch : 1825, batch :  1, loss :    7.53292, regression loss :   37.55149, sparse loss :    4.19753, time : 3725.60681.\n",
      "Epoch : 1826, batch :  1, loss :    7.53249, regression loss :   37.55141, sparse loss :    4.19706, time : 3727.38009.\n",
      "Epoch : 1827, batch :  1, loss :    7.53207, regression loss :   37.55143, sparse loss :    4.19658, time : 3729.05704.\n",
      "Epoch : 1828, batch :  1, loss :    7.53164, regression loss :   37.55134, sparse loss :    4.19612, time : 3730.67976.\n",
      "Epoch : 1829, batch :  1, loss :    7.53122, regression loss :   37.55136, sparse loss :    4.19564, time : 3732.38850.\n",
      "Epoch : 1830, batch :  1, loss :    7.53079, regression loss :   37.55128, sparse loss :    4.19518, time : 3734.05298.\n",
      "Epoch : 1831, batch :  1, loss :    7.53037, regression loss :   37.55129, sparse loss :    4.19471, time : 3735.79027.\n",
      "Epoch : 1832, batch :  1, loss :    7.52995, regression loss :   37.55122, sparse loss :    4.19425, time : 3737.44253.\n",
      "Epoch : 1833, batch :  1, loss :    7.52953, regression loss :   37.55122, sparse loss :    4.19379, time : 3739.22537.\n",
      "Epoch : 1834, batch :  1, loss :    7.52911, regression loss :   37.55116, sparse loss :    4.19333, time : 3740.98175.\n",
      "Epoch : 1835, batch :  1, loss :    7.52869, regression loss :   37.55115, sparse loss :    4.19286, time : 3742.60227.\n",
      "Epoch : 1836, batch :  1, loss :    7.52828, regression loss :   37.55110, sparse loss :    4.19241, time : 3744.28477.\n",
      "Epoch : 1837, batch :  1, loss :    7.52786, regression loss :   37.55107, sparse loss :    4.19195, time : 3745.90954.\n",
      "Epoch : 1838, batch :  1, loss :    7.52745, regression loss :   37.55105, sparse loss :    4.19149, time : 3747.63646.\n",
      "Epoch : 1839, batch :  1, loss :    7.52704, regression loss :   37.55099, sparse loss :    4.19104, time : 3749.25498.\n",
      "Epoch : 1840, batch :  1, loss :    7.52663, regression loss :   37.55100, sparse loss :    4.19059, time : 3751.02632.\n",
      "Epoch : 1841, batch :  1, loss :    7.52622, regression loss :   37.55093, sparse loss :    4.19014, time : 3752.77269.\n",
      "Epoch : 1842, batch :  1, loss :    7.52581, regression loss :   37.55094, sparse loss :    4.18969, time : 3754.52092.\n",
      "Epoch : 1843, batch :  1, loss :    7.52540, regression loss :   37.55087, sparse loss :    4.18924, time : 3756.19577.\n",
      "Epoch : 1844, batch :  1, loss :    7.52500, regression loss :   37.55086, sparse loss :    4.18879, time : 3757.81090.\n",
      "Epoch : 1845, batch :  1, loss :    7.52459, regression loss :   37.55082, sparse loss :    4.18835, time : 3759.52377.\n",
      "Epoch : 1846, batch :  1, loss :    7.52419, regression loss :   37.55080, sparse loss :    4.18790, time : 3761.13073.\n",
      "Epoch : 1847, batch :  1, loss :    7.52379, regression loss :   37.55076, sparse loss :    4.18746, time : 3762.85787.\n",
      "Epoch : 1848, batch :  1, loss :    7.52339, regression loss :   37.55073, sparse loss :    4.18702, time : 3764.60078.\n",
      "Epoch : 1849, batch :  1, loss :    7.52299, regression loss :   37.55071, sparse loss :    4.18658, time : 3766.33345.\n",
      "Epoch : 1850, batch :  1, loss :    7.52259, regression loss :   37.55067, sparse loss :    4.18614, time : 3767.97181.\n",
      "Epoch : 1851, batch :  1, loss :    7.52220, regression loss :   37.55065, sparse loss :    4.18570, time : 3769.69266.\n",
      "Epoch : 1852, batch :  1, loss :    7.52180, regression loss :   37.55060, sparse loss :    4.18527, time : 3771.46128.\n",
      "Epoch : 1853, batch :  1, loss :    7.52141, regression loss :   37.55059, sparse loss :    4.18483, time : 3773.12950.\n",
      "Epoch : 1854, batch :  1, loss :    7.52101, regression loss :   37.55054, sparse loss :    4.18440, time : 3774.73629.\n",
      "Epoch : 1855, batch :  1, loss :    7.52062, regression loss :   37.55053, sparse loss :    4.18397, time : 3776.40483.\n",
      "Epoch : 1856, batch :  1, loss :    7.52023, regression loss :   37.55048, sparse loss :    4.18354, time : 3778.03205.\n",
      "Epoch : 1857, batch :  1, loss :    7.51984, regression loss :   37.55048, sparse loss :    4.18311, time : 3779.75450.\n",
      "Epoch : 1858, batch :  1, loss :    7.51946, regression loss :   37.55041, sparse loss :    4.18268, time : 3781.50344.\n",
      "Epoch : 1859, batch :  1, loss :    7.51907, regression loss :   37.55042, sparse loss :    4.18225, time : 3783.25455.\n",
      "Epoch : 1860, batch :  1, loss :    7.51868, regression loss :   37.55037, sparse loss :    4.18183, time : 3785.00649.\n",
      "Epoch : 1861, batch :  1, loss :    7.51830, regression loss :   37.55035, sparse loss :    4.18141, time : 3786.67747.\n",
      "Epoch : 1862, batch :  1, loss :    7.51792, regression loss :   37.55031, sparse loss :    4.18098, time : 3788.40572.\n",
      "Epoch : 1863, batch :  1, loss :    7.51754, regression loss :   37.55029, sparse loss :    4.18056, time : 3790.03986.\n",
      "Epoch : 1864, batch :  1, loss :    7.51715, regression loss :   37.55026, sparse loss :    4.18014, time : 3791.73836.\n",
      "Epoch : 1865, batch :  1, loss :    7.51677, regression loss :   37.55023, sparse loss :    4.17972, time : 3793.38574.\n",
      "Epoch : 1866, batch :  1, loss :    7.51640, regression loss :   37.55021, sparse loss :    4.17931, time : 3795.07091.\n",
      "Epoch : 1867, batch :  1, loss :    7.51602, regression loss :   37.55017, sparse loss :    4.17889, time : 3796.72442.\n",
      "Epoch : 1868, batch :  1, loss :    7.51564, regression loss :   37.55015, sparse loss :    4.17848, time : 3798.40799.\n",
      "Epoch : 1869, batch :  1, loss :    7.51527, regression loss :   37.55013, sparse loss :    4.17806, time : 3800.09972.\n",
      "Epoch : 1870, batch :  1, loss :    7.51490, regression loss :   37.55009, sparse loss :    4.17765, time : 3801.72187.\n",
      "Epoch : 1871, batch :  1, loss :    7.51452, regression loss :   37.55006, sparse loss :    4.17724, time : 3803.54358.\n",
      "Epoch : 1872, batch :  1, loss :    7.51415, regression loss :   37.55005, sparse loss :    4.17683, time : 3805.19087.\n",
      "Epoch : 1873, batch :  1, loss :    7.51378, regression loss :   37.54999, sparse loss :    4.17643, time : 3806.90970.\n",
      "Epoch : 1874, batch :  1, loss :    7.51341, regression loss :   37.55001, sparse loss :    4.17601, time : 3808.60408.\n",
      "Epoch : 1875, batch :  1, loss :    7.51305, regression loss :   37.54995, sparse loss :    4.17561, time : 3810.19557.\n",
      "Epoch : 1876, batch :  1, loss :    7.51268, regression loss :   37.54992, sparse loss :    4.17521, time : 3811.90419.\n",
      "Epoch : 1877, batch :  1, loss :    7.51232, regression loss :   37.54992, sparse loss :    4.17480, time : 3813.64297.\n",
      "Epoch : 1878, batch :  1, loss :    7.51195, regression loss :   37.54986, sparse loss :    4.17441, time : 3815.28066.\n",
      "Epoch : 1879, batch :  1, loss :    7.51159, regression loss :   37.54985, sparse loss :    4.17400, time : 3817.01752.\n",
      "Epoch : 1880, batch :  1, loss :    7.51123, regression loss :   37.54983, sparse loss :    4.17360, time : 3818.67234.\n",
      "Epoch : 1881, batch :  1, loss :    7.51087, regression loss :   37.54978, sparse loss :    4.17321, time : 3820.37637.\n",
      "Epoch : 1882, batch :  1, loss :    7.51051, regression loss :   37.54978, sparse loss :    4.17281, time : 3822.10374.\n",
      "Epoch : 1883, batch :  1, loss :    7.51015, regression loss :   37.54973, sparse loss :    4.17242, time : 3823.76618.\n",
      "Epoch : 1884, batch :  1, loss :    7.50979, regression loss :   37.54971, sparse loss :    4.17202, time : 3825.43653.\n",
      "Epoch : 1885, batch :  1, loss :    7.50943, regression loss :   37.54969, sparse loss :    4.17163, time : 3827.23854.\n",
      "Epoch : 1886, batch :  1, loss :    7.50908, regression loss :   37.54966, sparse loss :    4.17124, time : 3828.94430.\n",
      "Epoch : 1887, batch :  1, loss :    7.50873, regression loss :   37.54963, sparse loss :    4.17085, time : 3830.69117.\n",
      "Epoch : 1888, batch :  1, loss :    7.50837, regression loss :   37.54961, sparse loss :    4.17046, time : 3832.45542.\n",
      "Epoch : 1889, batch :  1, loss :    7.50802, regression loss :   37.54960, sparse loss :    4.17007, time : 3834.20382.\n",
      "Epoch : 1890, batch :  1, loss :    7.50767, regression loss :   37.54955, sparse loss :    4.16968, time : 3836.09960.\n",
      "Epoch : 1891, batch :  1, loss :    7.50732, regression loss :   37.54954, sparse loss :    4.16930, time : 3837.78586.\n",
      "Epoch : 1892, batch :  1, loss :    7.50697, regression loss :   37.54951, sparse loss :    4.16891, time : 3839.47181.\n",
      "Epoch : 1893, batch :  1, loss :    7.50662, regression loss :   37.54948, sparse loss :    4.16853, time : 3841.04999.\n",
      "Epoch : 1894, batch :  1, loss :    7.50628, regression loss :   37.54946, sparse loss :    4.16815, time : 3842.74942.\n",
      "Epoch : 1895, batch :  1, loss :    7.50593, regression loss :   37.54943, sparse loss :    4.16777, time : 3844.36107.\n",
      "Epoch : 1896, batch :  1, loss :    7.50559, regression loss :   37.54941, sparse loss :    4.16739, time : 3845.96487.\n",
      "Epoch : 1897, batch :  1, loss :    7.50525, regression loss :   37.54938, sparse loss :    4.16701, time : 3847.54254.\n",
      "Epoch : 1898, batch :  1, loss :    7.50490, regression loss :   37.54936, sparse loss :    4.16663, time : 3849.25761.\n",
      "Epoch : 1899, batch :  1, loss :    7.50456, regression loss :   37.54934, sparse loss :    4.16626, time : 3851.00071.\n",
      "Epoch : 1900, batch :  1, loss :    7.50422, regression loss :   37.54931, sparse loss :    4.16588, time : 3855.24501.\n",
      "Epoch : 1901, batch :  1, loss :    7.50389, regression loss :   37.54929, sparse loss :    4.16551, time : 3856.82132.\n",
      "Epoch : 1902, batch :  1, loss :    7.50355, regression loss :   37.54926, sparse loss :    4.16514, time : 3858.54537.\n",
      "Epoch : 1903, batch :  1, loss :    7.50321, regression loss :   37.54924, sparse loss :    4.16476, time : 3860.28602.\n",
      "Epoch : 1904, batch :  1, loss :    7.50288, regression loss :   37.54921, sparse loss :    4.16440, time : 3861.95696.\n",
      "Epoch : 1905, batch :  1, loss :    7.50254, regression loss :   37.54920, sparse loss :    4.16402, time : 3863.68755.\n",
      "Epoch : 1906, batch :  1, loss :    7.50221, regression loss :   37.54916, sparse loss :    4.16366, time : 3865.43820.\n",
      "Epoch : 1907, batch :  1, loss :    7.50188, regression loss :   37.54915, sparse loss :    4.16329, time : 3867.20897.\n",
      "Epoch : 1908, batch :  1, loss :    7.50155, regression loss :   37.54912, sparse loss :    4.16293, time : 3868.93065.\n",
      "Epoch : 1909, batch :  1, loss :    7.50121, regression loss :   37.54909, sparse loss :    4.16256, time : 3870.63822.\n",
      "Epoch : 1910, batch :  1, loss :    7.50089, regression loss :   37.54908, sparse loss :    4.16220, time : 3872.26272.\n",
      "Epoch : 1911, batch :  1, loss :    7.50056, regression loss :   37.54905, sparse loss :    4.16184, time : 3873.88556.\n",
      "Epoch : 1912, batch :  1, loss :    7.50023, regression loss :   37.54903, sparse loss :    4.16148, time : 3875.70618.\n",
      "Epoch : 1913, batch :  1, loss :    7.49990, regression loss :   37.54901, sparse loss :    4.16112, time : 3877.40128.\n",
      "Epoch : 1914, batch :  1, loss :    7.49958, regression loss :   37.54898, sparse loss :    4.16076, time : 3879.10632.\n",
      "Epoch : 1915, batch :  1, loss :    7.49926, regression loss :   37.54896, sparse loss :    4.16040, time : 3880.73772.\n",
      "Epoch : 1916, batch :  1, loss :    7.49893, regression loss :   37.54894, sparse loss :    4.16004, time : 3882.30098.\n",
      "Epoch : 1917, batch :  1, loss :    7.49861, regression loss :   37.54891, sparse loss :    4.15969, time : 3883.86382.\n",
      "Epoch : 1918, batch :  1, loss :    7.49829, regression loss :   37.54890, sparse loss :    4.15933, time : 3885.45716.\n",
      "Epoch : 1919, batch :  1, loss :    7.49797, regression loss :   37.54886, sparse loss :    4.15898, time : 3887.28604.\n",
      "Epoch : 1920, batch :  1, loss :    7.49765, regression loss :   37.54886, sparse loss :    4.15863, time : 3888.93331.\n",
      "Epoch : 1921, batch :  1, loss :    7.49733, regression loss :   37.54880, sparse loss :    4.15828, time : 3890.58434.\n",
      "Epoch : 1922, batch :  1, loss :    7.49702, regression loss :   37.54884, sparse loss :    4.15793, time : 3892.17546.\n",
      "Epoch : 1923, batch :  1, loss :    7.49670, regression loss :   37.54872, sparse loss :    4.15759, time : 3893.90573.\n",
      "Epoch : 1924, batch :  1, loss :    7.49639, regression loss :   37.54885, sparse loss :    4.15722, time : 3895.67810.\n",
      "Epoch : 1925, batch :  1, loss :    7.49607, regression loss :   37.54862, sparse loss :    4.15690, time : 3897.35871.\n",
      "Epoch : 1926, batch :  1, loss :    7.49576, regression loss :   37.54881, sparse loss :    4.15653, time : 3898.98919.\n",
      "Epoch : 1927, batch :  1, loss :    7.49545, regression loss :   37.54866, sparse loss :    4.15620, time : 3900.79538.\n",
      "Epoch : 1928, batch :  1, loss :    7.49514, regression loss :   37.54861, sparse loss :    4.15586, time : 3902.49341.\n",
      "Epoch : 1929, batch :  1, loss :    7.49483, regression loss :   37.54877, sparse loss :    4.15550, time : 3904.23145.\n",
      "Epoch : 1930, batch :  1, loss :    7.49452, regression loss :   37.54853, sparse loss :    4.15518, time : 3905.84883.\n",
      "Epoch : 1931, batch :  1, loss :    7.49421, regression loss :   37.54861, sparse loss :    4.15483, time : 3907.57278.\n",
      "Epoch : 1932, batch :  1, loss :    7.49390, regression loss :   37.54868, sparse loss :    4.15448, time : 3909.25413.\n",
      "Epoch : 1933, batch :  1, loss :    7.49360, regression loss :   37.54848, sparse loss :    4.15416, time : 3911.00670.\n",
      "Epoch : 1934, batch :  1, loss :    7.49329, regression loss :   37.54857, sparse loss :    4.15381, time : 3912.66674.\n",
      "Epoch : 1935, batch :  1, loss :    7.49299, regression loss :   37.54857, sparse loss :    4.15348, time : 3914.29697.\n",
      "Epoch : 1936, batch :  1, loss :    7.49268, regression loss :   37.54843, sparse loss :    4.15315, time : 3916.10224.\n",
      "Epoch : 1937, batch :  1, loss :    7.49238, regression loss :   37.54852, sparse loss :    4.15281, time : 3917.88852.\n",
      "Epoch : 1938, batch :  1, loss :    7.49208, regression loss :   37.54849, sparse loss :    4.15248, time : 3919.62666.\n",
      "Epoch : 1939, batch :  1, loss :    7.49178, regression loss :   37.54839, sparse loss :    4.15215, time : 3921.34258.\n",
      "Epoch : 1940, batch :  1, loss :    7.49148, regression loss :   37.54846, sparse loss :    4.15181, time : 3922.97335.\n",
      "Epoch : 1941, batch :  1, loss :    7.49118, regression loss :   37.54840, sparse loss :    4.15149, time : 3924.68240.\n",
      "Epoch : 1942, batch :  1, loss :    7.49088, regression loss :   37.54836, sparse loss :    4.15116, time : 3926.28977.\n",
      "Epoch : 1943, batch :  1, loss :    7.49059, regression loss :   37.54838, sparse loss :    4.15083, time : 3927.97011.\n",
      "Epoch : 1944, batch :  1, loss :    7.49029, regression loss :   37.54833, sparse loss :    4.15051, time : 3929.66045.\n",
      "Epoch : 1945, batch :  1, loss :    7.49000, regression loss :   37.54832, sparse loss :    4.15018, time : 3931.35486.\n",
      "Epoch : 1946, batch :  1, loss :    7.48970, regression loss :   37.54830, sparse loss :    4.14986, time : 3933.10212.\n",
      "Epoch : 1947, batch :  1, loss :    7.48941, regression loss :   37.54829, sparse loss :    4.14953, time : 3934.95427.\n",
      "Epoch : 1948, batch :  1, loss :    7.48912, regression loss :   37.54825, sparse loss :    4.14921, time : 3936.58113.\n",
      "Epoch : 1949, batch :  1, loss :    7.48883, regression loss :   37.54823, sparse loss :    4.14889, time : 3938.20239.\n",
      "Epoch : 1950, batch :  1, loss :    7.48854, regression loss :   37.54824, sparse loss :    4.14857, time : 3939.96850.\n",
      "Epoch : 1951, batch :  1, loss :    7.48825, regression loss :   37.54818, sparse loss :    4.14825, time : 3941.59933.\n",
      "Epoch : 1952, batch :  1, loss :    7.48796, regression loss :   37.54819, sparse loss :    4.14793, time : 3943.37608.\n",
      "Epoch : 1953, batch :  1, loss :    7.48767, regression loss :   37.54818, sparse loss :    4.14761, time : 3945.08356.\n",
      "Epoch : 1954, batch :  1, loss :    7.48738, regression loss :   37.54812, sparse loss :    4.14730, time : 3946.73896.\n",
      "Epoch : 1955, batch :  1, loss :    7.48710, regression loss :   37.54815, sparse loss :    4.14698, time : 3948.37864.\n",
      "Epoch : 1956, batch :  1, loss :    7.48681, regression loss :   37.54810, sparse loss :    4.14667, time : 3950.01049.\n",
      "Epoch : 1957, batch :  1, loss :    7.48653, regression loss :   37.54807, sparse loss :    4.14636, time : 3951.87342.\n",
      "Epoch : 1958, batch :  1, loss :    7.48625, regression loss :   37.54810, sparse loss :    4.14604, time : 3953.69079.\n",
      "Epoch : 1959, batch :  1, loss :    7.48597, regression loss :   37.54804, sparse loss :    4.14574, time : 3955.34580.\n",
      "Epoch : 1960, batch :  1, loss :    7.48569, regression loss :   37.54804, sparse loss :    4.14542, time : 3957.04280.\n",
      "Epoch : 1961, batch :  1, loss :    7.48540, regression loss :   37.54803, sparse loss :    4.14511, time : 3958.69776.\n",
      "Epoch : 1962, batch :  1, loss :    7.48513, regression loss :   37.54798, sparse loss :    4.14481, time : 3960.31945.\n",
      "Epoch : 1963, batch :  1, loss :    7.48485, regression loss :   37.54800, sparse loss :    4.14450, time : 3961.90683.\n",
      "Epoch : 1964, batch :  1, loss :    7.48457, regression loss :   37.54795, sparse loss :    4.14419, time : 3963.65865.\n",
      "Epoch : 1965, batch :  1, loss :    7.48429, regression loss :   37.54794, sparse loss :    4.14389, time : 3965.38588.\n",
      "Epoch : 1966, batch :  1, loss :    7.48402, regression loss :   37.54793, sparse loss :    4.14358, time : 3967.05769.\n",
      "Epoch : 1967, batch :  1, loss :    7.48374, regression loss :   37.54790, sparse loss :    4.14328, time : 3968.77425.\n",
      "Epoch : 1968, batch :  1, loss :    7.48347, regression loss :   37.54790, sparse loss :    4.14298, time : 3970.51286.\n",
      "Epoch : 1969, batch :  1, loss :    7.48319, regression loss :   37.54786, sparse loss :    4.14268, time : 3972.16309.\n",
      "Epoch : 1970, batch :  1, loss :    7.48292, regression loss :   37.54787, sparse loss :    4.14237, time : 3973.90787.\n",
      "Epoch : 1971, batch :  1, loss :    7.48265, regression loss :   37.54782, sparse loss :    4.14208, time : 3975.52311.\n",
      "Epoch : 1972, batch :  1, loss :    7.48238, regression loss :   37.54783, sparse loss :    4.14177, time : 3977.24330.\n",
      "Epoch : 1973, batch :  1, loss :    7.48211, regression loss :   37.54779, sparse loss :    4.14148, time : 3979.02729.\n",
      "Epoch : 1974, batch :  1, loss :    7.48184, regression loss :   37.54778, sparse loss :    4.14118, time : 3980.73934.\n",
      "Epoch : 1975, batch :  1, loss :    7.48157, regression loss :   37.54779, sparse loss :    4.14088, time : 3982.40367.\n",
      "Epoch : 1976, batch :  1, loss :    7.48130, regression loss :   37.54770, sparse loss :    4.14059, time : 3984.09214.\n",
      "Epoch : 1977, batch :  1, loss :    7.48104, regression loss :   37.54778, sparse loss :    4.14029, time : 3985.69942.\n",
      "Epoch : 1978, batch :  1, loss :    7.48077, regression loss :   37.54767, sparse loss :    4.14001, time : 3987.44855.\n",
      "Epoch : 1979, batch :  1, loss :    7.48051, regression loss :   37.54771, sparse loss :    4.13971, time : 3989.17509.\n",
      "Epoch : 1980, batch :  1, loss :    7.48024, regression loss :   37.54771, sparse loss :    4.13942, time : 3990.83387.\n",
      "Epoch : 1981, batch :  1, loss :    7.47998, regression loss :   37.54760, sparse loss :    4.13913, time : 3992.45259.\n",
      "Epoch : 1982, batch :  1, loss :    7.47972, regression loss :   37.54771, sparse loss :    4.13883, time : 3994.14327.\n",
      "Epoch : 1983, batch :  1, loss :    7.47946, regression loss :   37.54759, sparse loss :    4.13855, time : 3995.82076.\n",
      "Epoch : 1984, batch :  1, loss :    7.47920, regression loss :   37.54762, sparse loss :    4.13826, time : 3997.52205.\n",
      "Epoch : 1985, batch :  1, loss :    7.47894, regression loss :   37.54763, sparse loss :    4.13797, time : 3999.25923.\n",
      "Epoch : 1986, batch :  1, loss :    7.47868, regression loss :   37.54752, sparse loss :    4.13770, time : 4001.01661.\n",
      "Epoch : 1987, batch :  1, loss :    7.47842, regression loss :   37.54763, sparse loss :    4.13740, time : 4002.72976.\n",
      "Epoch : 1988, batch :  1, loss :    7.47816, regression loss :   37.54750, sparse loss :    4.13713, time : 4004.44130.\n",
      "Epoch : 1989, batch :  1, loss :    7.47791, regression loss :   37.54753, sparse loss :    4.13684, time : 4006.08163.\n",
      "Epoch : 1990, batch :  1, loss :    7.47765, regression loss :   37.54756, sparse loss :    4.13655, time : 4007.66840.\n",
      "Epoch : 1991, batch :  1, loss :    7.47740, regression loss :   37.54741, sparse loss :    4.13628, time : 4009.25593.\n",
      "Epoch : 1992, batch :  1, loss :    7.47714, regression loss :   37.54759, sparse loss :    4.13598, time : 4011.01480.\n",
      "Epoch : 1993, batch :  1, loss :    7.47689, regression loss :   37.54736, sparse loss :    4.13573, time : 4012.64647.\n",
      "Epoch : 1994, batch :  1, loss :    7.47664, regression loss :   37.54752, sparse loss :    4.13543, time : 4014.28833.\n",
      "Epoch : 1995, batch :  1, loss :    7.47638, regression loss :   37.54740, sparse loss :    4.13516, time : 4015.93370.\n",
      "Epoch : 1996, batch :  1, loss :    7.47613, regression loss :   37.54741, sparse loss :    4.13488, time : 4017.61219.\n",
      "Epoch : 1997, batch :  1, loss :    7.47588, regression loss :   37.54741, sparse loss :    4.13460, time : 4019.29081.\n",
      "Epoch : 1998, batch :  1, loss :    7.47563, regression loss :   37.54737, sparse loss :    4.13433, time : 4020.93689.\n",
      "Epoch : 1999, batch :  1, loss :    7.47539, regression loss :   37.54737, sparse loss :    4.13405, time : 4022.60293.\n"
     ]
    }
   ],
   "source": [
    "model = sparse_model(mat).to('cuda')\n",
    "model = torch.nn.DataParallel(model, device_ids=[0,1,2,3])#必须从零开始(这里0表示第1块卡，1表示第2块卡.)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    [model.module.sparse_weight],\n",
    "    lr=5e-3,\n",
    "    betas=(0.9, 0.99),\n",
    "    weight_decay=0,\n",
    "    eps=1e-15,\n",
    "    amsgrad=False,\n",
    ")\n",
    "\n",
    "num_epochs = 2000\n",
    "lambda_1 = 0.9\n",
    "loss_record = torch.zeros((num_epochs, 3))\n",
    "start_optim = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    running_loss = 0.\n",
    "    for batch_idx, (rate, x_dot) in enumerate(data_loader):\n",
    "        rate = rate.to('cuda')\n",
    "        x_dot = x_dot.to('cuda')\n",
    "        reduced_x_dot = model(rate) # rate * F.sigmoid(sparse_weight) @ mat\n",
    "        weight = rate.pow(2).sum(0) / rate.pow(2).sum()\n",
    "\n",
    "        regression_loss = ((reduced_x_dot - x_dot) / x_dot.abs().max(dim=1)[0].unsqueeze(1)).norm(p=2, dim=1).mean() * 100\n",
    "        sparse_loss = (F.sigmoid(model.module.sparse_weight)).mean() * 100\n",
    "        loss = (1 - lambda_1) * regression_loss + lambda_1 * sparse_loss\n",
    "\n",
    "        loss_record[epoch, 0] += regression_loss.item()\n",
    "        loss_record[epoch, 1] += sparse_loss.item()\n",
    "        loss_record[epoch, 2] += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch : {epoch:3}, batch : {batch_idx+1:2}, \"\n",
    "              f\"loss : {loss.item():>10.5f}, \"\n",
    "              f\"regression loss : {regression_loss.item():>10.5f}, \"\n",
    "              f\"sparse loss : {sparse_loss.item():>10.5f}, \"\n",
    "              f\"time : {time.time()-start_optim:>10.5f}.\")\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        np.save('./data/sparse_weight_9.npy', F.sigmoid(model.module.sparse_weight).data.cpu().numpy())\n",
    "        np.save('./data/loss_record_9.npy', loss_record)\n",
    "        plot_weight(F.sigmoid(model.module.sparse_weight).data.cpu().numpy())\n",
    "        plot_record(loss_record, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
